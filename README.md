# cleansumstats

**Convert GWAS sumstat files into a common format with a common reference for positions, rsids and effect alleles.**.

[![Build Status](https://travis-ci.com/nf-core/cleansumstats.svg?branch=master)](https://travis-ci.com/nf-core/cleansumstats)
[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg)](https://www.nextflow.io/)

## Introduction
The clean sumstats pipeline takes a genomic sumstat file as input(normally output from  GWAS), together with specifiers for chr, pos and available stats.

Briefly, the pipeline first detects genome build, then map to a dbsnp build to keep only entries with rsids and ref/alt allele information. Secondly, using information of which allele is A1, the direction of the statistic is assesed. Lastly, an output file is assembled, which then can be directly compared to other similar studies.

The pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible.

## Quick Start
To run a quick test using provided example and test data

```bash
# i. Make sure singularity is installed, see [singularity installation](docs/singularity-installation.md)
singularity --version

# ii. Download our container image containing all software and code needed
# A downloadable image will be provided at dockerhub

# iii. Run the singularity image using example data
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --input /cleansumstats/tests/example_data/sumstat_1/sumstat_1_raw_meta.txt \
  --outdir ./out_example

```

## Add full size reference data
In the cleaning all positions are compared to a reference to confirm or add missing annotation.

### dbsnp reference
The preparation of the dbsnp reference only has to be done once, and can be reused for all sumstats that needs cleaning.

```bash
# i. Download the dbsnp reference: size 15GB (and the readme, etc for future reference)
mkdir -p source_data/dbsnp
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/README.txt
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz.md5
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz.tbi
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz

# ii. If you are on a HPC Start your interactive session (cpus around 40)
srun --mem=400g --ntasks 1 --cpus-per-task 60 --time=3:00:00 --account ibp_pipeline_cleansumstats --pty /bin/bash
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --generateDbSNPreference \
  --input source_data/dbsnp/All_20180418.vcf.gz \
  --outdir ./out_dbsnp
```

### 1000 genomes project reference
```bash
# i. Download
mkdir -p source_data/1kgp
wget -P source_data/1kgp http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz
wget -P source_data/1kgp http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz.tbi

./scripts/singularity-run.sh nextflow run /cleansumstats \
  --generateDbSNPreference \
  --input source_data/1kgp/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz \
  --outdir ./out_1kgp
```

## Prepare meta data files
After the reference data paths have been set in the nextflow.config file, the pipeline can be run with only one argument, pointing to only one file. This file is called the meta file, and contains paths to other important files, such as the actual sumstats, README, article pdf, etc,. This file has to be filled in manually, and a template can be generated by running this command.

```bash
# Generate metadata template

```

## Run the cleaning pipeline


## Using images

### Pre-requisites

Docker and singularity has to be installed to create an image executable at a HPC
- docker-install-instructions(todo)
- [singularity installation](docs/singularity-installation.md)

### build images

If you are a developer or don't want to download the image from dockerhub, it is easy to build a docker image and singularity images using the following code.

```bash
# Build docker image (tied to your system)
./scripts/docker-build.sh

# Build singularity image (movable to other systems)
./scripts/singularity-build.sh
```

The general idea of first use docker and then singularity is to facilitate how docker uses layers to speed up build speed, and as a consequence of that, development is also sped up. From the created docker image, it is easy to create a singularity image, which often required by HPCs, as they usually are incompatible with the Docker deamon. The created singulariy image goes to the 'tmp/' folder.

### Use the docker or singulariy image to run the internal test suit

As an extra failsafe when updating the code, we have added unit and e2e tests, which can directly be run using the docker image. They should all return ok.

```bash
# using docker
./scripts/docker-run.sh /cleansumstats/tests/run-tests.sh

# using singularity
./scripts/singularity-run.sh /cleansumstats/tests/run-tests.sh
```

## More documentation
- See [usage docs](docs/usage.md) for all of the available options when running the pipeline.
- See [Output and how to interpret the results](docs/output.md) for the output structure and how to interpret the results.
- See [Developer instructions](docs/developers.md) only for developers

## Credits

cleansumstats was originally written by Jesper R. GÃ¥din
