# cleansumstats

**Convert GWAS sumstat files into a common format with a common reference for positions, rsids and effect alleles.**.

[![Build Status](https://travis-ci.com/nf-core/cleansumstats.svg?branch=master)](https://travis-ci.com/nf-core/cleansumstats)
[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg)](https://www.nextflow.io/)

## Introduction
The clean sumstats pipeline takes a genomic sumstat file as input(normally output from  GWAS), together with specifiers for chr, pos and available stats.

Briefly, the pipeline first detects genome build, then map to a dbsnp build to keep only entries with rsids and ref/alt allele information. Secondly, using information of which allele is A1, the direction of the statistic is assesed. Lastly, an output file is assembled, which then can be directly compared to other similar studies.

The pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible.

## Quick Start
To run a quick test using provided example and test data

```bash
# i. Make sure singularity is installed, see [singularity installation](docs/singularity-installation.md)
singularity --version

# ii. Download our container image containing all software and code needed
# A downloadable image will be provided at dockerhub

# iii. Run the singularity image using example data
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --input /cleansumstats/tests/example_data/sumstat_1/sumstat_1_raw_meta.txt \
  --outdir ./out_example

```

## Quick Start IBP
To run the IBP GDK specific fully configurated and operational pipeline just run

```
# i. Start interactive node and prepare output folder
srun --mem=40g --ntasks 1 --cpus-per-task 6 --time=1:00:00 --account ibp_pipeline_cleansumstats --pty /bin/bash
mkdir output

# ii. Run the singularity image using example data
./cleansumstats.sh tests/example_data/sumstat_1/sumstat_1_raw_meta.txt  output
```

## Add full size reference data
In the cleaning all positions are compared to a reference to confirm or add missing annotation.

### dbsnp reference
The preparation of the dbsnp reference only has to be done once, and can be reused for all sumstats that needs cleaning.

```bash
# i. Download the dbsnp reference: size 15GB (and the readme, etc for future reference)
mkdir -p source_data/dbsnp
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/README.txt
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz.md5
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz.tbi
wget -P source_data/dbsnp ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz

# ii. If you are on a HPC Start your interactive session (below SLURM settings took about 1h to run)
srun --mem=400g --ntasks 1 --cpus-per-task 60 --time=3:00:00 --account ibp_pipeline_cleansumstats --pty /bin/bash
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --generateDbSNPreference \
  --input source_data/dbsnp/All_20180418.vcf.gz \
  --outdir ./out_dbsnp \
  --libdirdbsnp ./out_dbsnp
```

### 1000 genomes project reference
```bash
# i. Download
mkdir -p source_data/1kgp
wget -P source_data/1kgp http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz
wget -P source_data/1kgp http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz.tbi

# ii. If you are on a HPC Start your interactive session (below SLURM settings took about 5min to run)
srun --mem=80g --ntasks 1 --cpus-per-task 5 --time=1:00:00 --account ibp_pipeline_cleansumstats --pty /bin/bash
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --generate1KgAfSNPreference \
  --input source_data/1kgp/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz \
  --outdir ./out_1kgp \
  --kg1000AFGRCh38 ./out_1kgp
```

## Prepare meta data files
After the reference data paths have been set in the nextflow.config file, the pipeline can be run with only one argument, pointing to only one file. This file is called the meta file, and contains paths to other important files, such as the actual sumstats, README, article pdf, etc,. This file has to be filled in manually, and a template can be generated by running this command.

```bash
# Generate metadata template

```

## Run a fully operational cleaning pipeline (Replace example data with your own data to clean)
This will take longer time compared to the quick-start run as we now use the full >600 million rows dbsnp reference to map our variants to.

```
# ii. If you are on a HPC Start your interactive session (below SLURM settings took about 10min to run)
srun --mem=40g --ntasks 1 --cpus-per-task 6 --time=1:00:00 --account ibp_pipeline_cleansumstats --pty /bin/bash
./scripts/singularity-run.sh nextflow run /cleansumstats \
  --input /cleansumstats/tests/example_data/sumstat_1/sumstat_1_raw_meta.txt \
  --outdir ./out_clean \
  --libdirdbsnp ./out_dbsnp \
  --kg1000AFGRCh38 ./out_1kgp
```


## More documentation
- See [usage docs](docs/usage.md) for all of the available options when running the pipeline.
- See [Output and how to interpret the results](docs/output.md) for the output structure and how to interpret the results.
- See [Developer instructions](docs/developers.md) only for developers

## Credits

cleansumstats was originally written by Jesper R. GÃ¥din
