#!/usr/bin/env nextflow
/*
========================================================================================
                         nf-core/cleansumstats
========================================================================================
 nf-core/cleansumstats Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/nf-core/cleansumstats
----------------------------------------------------------------------------------------
*/

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run nf-core/cleansumstats --infile 'gwas-sumstats.gz' -profile singularity

    Mandatory arguments:
      --infile                      Path to tab-separated input data (must be surrounded with quotes)
      -profile                      Configuration profile to use. Can use multiple (comma separated)
                                    Available: conda, docker, singularity, awsbatch, test and more.

    References:                     If not specified in the configuration file or you wish to overwrite any of the references.
      --dbsnp38                     Path to dbsnp GRCh38 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp37                     Path to dbsnp GRCh37 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp36                     Path to dbsnp GRCh36 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp35                     Path to dbsnp GRCh35 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    AWSBatch options:
      --awsqueue                    The AWSBatch JobQueue that needs to be set when running on AWSBatch
      --awsregion                   The AWS Region for your AWS Batch job to run on
    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// Check if genome exists in the config file
//if (params.genomes && params.genome && !params.genomes.containsKey(params.genome)) {
//    exit 1, "The provided genome '${params.genome}' is not available in the iGenomes file. Currently the available genomes are ${params.genomes.keySet().join(", ")}"
//}


// Set channels
if (params.dbsnp38) { ch_dbsnp38 = file(params.dbsnp38, checkIfExists: true) }
if (params.dbsnp37) { ch_dbsnp37 = file(params.dbsnp37, checkIfExists: true) } 
if (params.dbsnp36) { ch_dbsnp36 = file(params.dbsnp36, checkIfExists: true) } 
if (params.dbsnp35) { ch_dbsnp35 = file(params.dbsnp35, checkIfExists: true) } 
ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)

// Stage config files
ch_multiqc_config = file(params.multiqc_config, checkIfExists: true)
ch_output_docs = file("$baseDir/docs/output.md", checkIfExists: true)

//example from nf-core how to use fasta
//params.fasta = params.genome ? params.genomes[ params.genome ].fasta ?: false : false
//if (params.fasta) { ch_fasta = file(params.fasta, checkIfExists: true) }

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}

if ( workflow.profile == 'awsbatch') {
  // AWSBatch sanity checking
  if (!params.awsqueue || !params.awsregion) exit 1, "Specify correct --awsqueue and --awsregion parameters on AWSBatch!"
  // Check outdir paths to be S3 buckets if running on AWSBatch
  // related: https://github.com/nextflow-io/nextflow/issues/813
  if (!params.outdir.startsWith('s3:')) exit 1, "Outdir not on S3 - specify S3 Bucket to run on AWSBatch!"
  // Prevent trace files to be stored on S3 since S3 does not support rolling files.
  if (workflow.tracedir.startsWith('s3:')) exit 1, "Specify a local tracedir or run without trace! S3 cannot be used for tracefiles."
}



// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38 
if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37 
if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36 
if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35 
summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
if (params.email || params.email_on_fail) {
  summary['E-mail Address']    = params.email
  summary['E-mail on failure'] = params.email_on_fail
  summary['MultiQC maxsize']   = params.maxMultiqcEmailFileSize
}
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'nf-core-cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}



if (params.generateMetafile){
  ch_metatemplate = file("${baseDir}/assets/meta_data_readMe_v4.txt")
  ch_sumstat_file = Channel
                   .fromPath(params.input, type: 'file')
                   .map { file -> tuple(file.baseName, file) }

  process create_meta_data_template {
  
      publishDir "${params.outdir}", mode: 'copy', overwrite: false
  
      input:
      tuple basefilename, sfilename from ch_sumstat_file
  
      output:
      file("${basefilename}.meta") into ch_metafile_template_out
  
      script:
      """
      cat ${ch_metatemplate} > ${basefilename}.meta
      """
  }
}else {


  process get_software_versions {
      publishDir "${params.outdir}/pipeline_info", mode: 'copy',
          saveAs: { filename ->
              if (filename.indexOf(".csv") > 0) filename
              else null
          }
  
      output:
      file 'software_versions_mqc.yaml' into software_versions_yaml
      file "software_versions.csv" into ch_software_versions

  
      script:
      """
      echo $workflow.manifest.version > v_pipeline.txt
      echo $workflow.nextflow.version > v_nextflow.txt
      #sstools-version > v_sumstattools.txt
      scrape_software_versions.py &> software_versions_mqc.yaml
      """
  }
  
  
  //to_stream_sumstat_dir = Channel
  //                .fromPath(params.input, type: 'dir')
  //                .map { dir -> tuple(dir.baseName, dir) }

  //use metafile filename as datasetID through the pipeline
  ch_mfile_check = Channel
                  .fromPath(params.input, type: 'file')
                  .map { file -> tuple(file.baseName, file) }
  
  //to_stream_sumstat_dir.into { ch_to_stream_sumstat_dir1; ch_to_stream_sumstat_dir2 }


  process check_and_force_basic_formats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile from ch_mfile_check
  
      output:
      tuple datasetID, mfile into ch_mfile_ok
      tuple datasetID, file("${datasetID}_sfile") into ch_sfile_ok
      tuple datasetID, env(spath) into ch_input_sfile
      tuple datasetID, env(rpath) into ch_input_readme
      tuple datasetID, env(pmid) into ch_pmid
      tuple datasetID, file("*.log") into inputchecker_log_files
      tuple datasetID, file("${datasetID}_header") into extra_stuff1
      tuple datasetID, file("${datasetID}_sfile_1000") into extra_stuff2
      tuple datasetID, file("${datasetID}_sfile_1000_formatted") into extra_stuff3
      tuple datasetID, env(pmid), env(pdfpath), file("${datasetID}_pdf_suppfiles.txt") into ch_input_pdf_stuff
      tuple datasetID, file("${datasetID}_one_line_summary_of_metadata.txt") into ch_one_line_metafile
  
      script:
      """
      # Check if the datasetID folder is already present, if so just increment a number to get the new outname
      #  this is because a metafile can have the same name as another even though the content might be different. 

      # Check if field for variable exists and if the file specified exists
      spath="\$(check_meta_file_references.sh "path_sumStats" ${mfile})"
      rpath="\$(check_meta_file_references.sh "path_readMe" ${mfile})"
      pdfpath="\$(check_meta_file_references.sh "path_pdf" ${mfile})"
      check_meta_file_references.sh "path_pdfSupp" ${mfile} > ${datasetID}_pdf_suppfiles.txt

      Px="\$(grep "^study_PMID=" $mfile)"
      pmid="\$(echo "\${Px#*=}")"

      # Check library if this has been processed before
      # TODO after the script producing the 00inventory.txt has been created 
      
      # Make complete metafile check
      echo "\$(head -n 1 < <(zcat \$spath))" > ${datasetID}_header
      check_meta_data_format.sh $mfile ${datasetID}_header ${datasetID}_mfile_format.log
      
      # Make a one line metafile to use for the inventory file and test if dataset is already in library
      cat ${baseDir}/assets/columns_for_one_line_summary.txt | while read -r varx; do 
        Px="\$(grep "^\${varx}=" $mfile)"
        var="\$(echo "\${Px#*=}")"
        printf "\t%s" "\${var}" >> one_line_summary
      done
      printf "\n" >> one_line_summary
      cat one_line_summary | sed -e 's/^[\t]//' > ${datasetID}_one_line_summary_of_metadata.txt

      # Sumstat file check on first 1000 lines
      echo "\$(head -n 1000 < <(zcat \$spath))" | gzip -c > ${datasetID}_sfile_1000
      check_and_format_sfile.sh ${datasetID}_sfile_1000 ${datasetID}_sfile_1000_formatted ${datasetID}_sfile_1000_format.log
      
      # Make second sumstat file check on all lines
      check_and_format_sfile.sh \$spath ${datasetID}_sfile ${datasetID}_sfile_format.log

      """
  }

  process add_sorted_index_sumstat {
  
  
      input:
      tuple datasetID, sfile from ch_sfile_ok

      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      output:
      tuple datasetID, file("${datasetID}_sfile") into ch_sfile_on_stream
  
      script:
      """
      cat $sfile | sstools-raw add-index | LC_ALL=C sort -k 1 - > ${datasetID}_sfile
      """
  }
  
  ch_mfile_ok.into { ch_mfile_ok1; ch_mfile_ok2; ch_mfile_ok3; ch_mfile_ok4; ch_mfile_ok5}
  ch_sfile_on_stream.into { ch_sfile_on_stream1; ch_sfile_on_stream2; ch_sfile_on_stream3; ch_sfile_on_stream4; ch_sfile_on_stream5 }
  ch_mfile_and_stream=ch_mfile_ok1.join(ch_sfile_on_stream1)
  ch_mfile_and_stream.into { ch_check_gb; ch_liftover; ch_stats_inference }
  
  
  whichbuild = ['GRCh35', 'GRCh36', 'GRCh37', 'GRCh38']
  
  process genome_build_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile from ch_check_gb
      each build from whichbuild
  
      output:
      tuple datasetID, file("${datasetID}*.res") into ch_genome_build_stats
  
      script:
      """
  
      colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
      colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
  
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" | awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' > gb_lift
      LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
      format_chrpos_for_dbsnp.sh ${build} gb_lift_sorted ${ch_dbsnp35} ${ch_dbsnp36} ${ch_dbsnp37} ${ch_dbsnp38} > ${build}.map
      sort -u -k1,1 ${build}.map | wc -l | awk -vOFS="\t" -vbuild=${build} '{print \$1,build}' > ${datasetID}.${build}.res
  
      """
  }
  
  
  
  ch_genome_build_stats_grouped = ch_genome_build_stats.groupTuple(by:0,size:4)
  
  process infer_genome_build {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, file(ujoins) from ch_genome_build_stats_grouped
  
      
      output:
      tuple datasetID, env(GRChmax) into ch_known_genome_build
      tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build
  
      script:
      """
      for gbuild in ${ujoins}
      do
          cat \$gbuild >> ${datasetID}.stats
      done
      GRChmax="\$(cat ${datasetID}.stats | sort -r -k1,1 | head -n1 | awk '{print \$2}')"
      """
  
  }
  
  ch_liftover_2=ch_liftover.join(ch_known_genome_build)
  
  process prep_dbsnp_mapling_by_sorting_chrpos {
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile, gbmax from ch_liftover_2
  
      output:
      tuple datasetID, mfile, file("gb_lift_sorted"), gbmax into ch_liftover_3
  
      script:
      """
      
      colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
      colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
  
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" | awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' > gb_lift
      LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
      """
  
  }
  
  process liftover_and_map_to_dbsnp38 {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, fsorted, gbmax from ch_liftover_3
      
      output:
      tuple datasetID, mfile, file("gb_liftgr38") into ch_liftover_4
  
      script:
      """
      format_chrpos_for_dbsnp.sh ${gbmax} ${fsorted} ${ch_dbsnp35} ${ch_dbsnp36} ${ch_dbsnp37} ${ch_dbsnp38} > gb_liftgr38
      """
  }
  
  process sort_new_dbsnp38map {
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, mapped from ch_liftover_4
      
      output:
      tuple datasetID, mfile, file("gb_liftgr38_sorted") into ch_liftover_5
  
      script:
      """
      LC_ALL=C sort -k1,1 $mapped > gb_liftgr38_sorted
      """
  
  }
  
  process liftover_and_map_to_rsids_and_alleles {
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, gb_liftgr38_sorted from ch_liftover_5
      
      output:
      tuple datasetID, val("GRCh37"), mfile, file("gb_ready_liftgr37") into ch_mapped_GRCh37
      tuple datasetID, file("gb_ready_liftgr38") into ch_mapped_GRCh38
  
      script:
      """
      LC_ALL=C join -1 1 -2 1 $gb_liftgr38_sorted ${ch_dbsnp38} | awk -vFS="[[:space:]]" -vOFS="\t" '{print \$2,\$6,\$7,\$8,\$9}' > gb_ready_liftgr37
      awk -vFS="[[:space:]]" -vOFS="\t" '{print \$2,\$1,\$3,\$4,\$5}' $gb_liftgr38_sorted > gb_ready_liftgr38
      """
  }
  
  //ch_mapped_data_mix=ch_mapped_GRCh38.mix(ch_mapped_GRCh37)
  
  process split_multiallelics_and_resort_index {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, liftgrs from ch_mapped_GRCh37
      
      output:
      tuple datasetID, build, mfile, file("${datasetID}_${build}_mapped") into ch_allele_correction
  
      script:
      """
      split_multiallelics_to_rows.sh $liftgrs > liftgrs2
      echo -e "0\tCHRPOS\tRSID\tA1\tA2" > ${datasetID}_${build}_mapped
      LC_ALL=C sort -k1,1 liftgrs2 >> ${datasetID}_${build}_mapped
      """
  }
  
  ch_allele_correction_combine=ch_allele_correction.combine(ch_sfile_on_stream2, by: 0)
  ch_allele_correction_combine.into{ ch_allele_correction_combine1; ch_allele_correction_combine2 }
  
  process does_exist_A2 {
  
      input:
      tuple datasetID, mfile from ch_mfile_ok2
      
      output:
      tuple datasetID, env(A2exists) into ch_present_A2
  
      script:
      """
      A2exists=\$(doesA2exist.sh $mfile)
      """
  }
  
  //Create filter for when A2 exists or not
  ch_present_A2_br=ch_present_A2.branch { key, value -> 
                  A2exists: value == "true"
                  A2missing: value == "false"
                  }
  
  //split the channels based on filter
  ch_present_A2_br2=ch_present_A2_br.A2exists
  ch_present_A2_br3=ch_present_A2_br.A2missing
  
  //combine each channel with the matching datasetID
  ch_A2_exists=ch_allele_correction_combine1.combine(ch_present_A2_br2, by: 0)
  ch_A2_missing=ch_allele_correction_combine2.combine(ch_present_A2_br3, by: 0)
  
  process allele_correction_A1_A2 {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, mapped, sfile, A2exists from ch_A2_exists
      
      output:
      tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_exists2
  
      script:
      """
      echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
  
      colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
      colA2=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "altallele")
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}|\${colA2}" -n"0,A1,A2" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 1.3 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${mapped} | sstools-eallele correction -f - >> ${build}_acorrected
      """
  }
  
  
  process allele_correction_A1 {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, mapped, sfile, A2missing from ch_A2_missing
      
      output:
      tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_missing2
      file("${build}_mapped2") into placeholder4
  
      script:
      """
      multiallelic_filter.sh $mapped > ${build}_mapped2
      echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
  
      colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}" -n"0,A1" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${build}_mapped2 | sstools-eallele correction -f - -a >> ${build}_acorrected 
  
      """
  }
  
  
  process filter_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile from ch_stats_inference
      
      output:
      tuple datasetID, file("st_filtered_remains") into ch_stats_filtered_remain
      tuple datasetID, file("st_filtered_removed")  into ch_stats_filtered_removed
  
      script:
      """
      filter_stat_values.sh $mfile $sfile > st_filtered_remains 2> st_filtered_removed
      """
  }
  
  ch_stats_filtered_remain
    .into { ch_stats_filtered_remain1; ch_stats_filtered_remain2}

  ch_stats_filtered_remain1
    .combine(ch_mfile_ok5, by: 0)
    .set{ ch_stats_filtered_remain3 }

  process infer_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, st_filtered, mfile from ch_stats_filtered_remain3
      
      output:
      tuple datasetID, mfile, file("st_inferred_stats") into ch_stats_selection
      file("st_which_to_do") into out_st_which_to_do
  
      script:
      """
      check_stat_inference.sh $mfile > st_which_to_do
  
      if [ -s st_which_to_do ]; then
        if grep -q "Z_fr_OR_P" st_which_to_do; then
  
          Px="\$(grep "^col_P=" $mfile)"
          P="\$(echo "\${Px#*=}")"
  
          echo -e "QNORM" > prepared_qnorm_vals
          cat $st_filtered | sstools-utils ad-hoc-do -f - -k "\${P}" -n"\${P}" | awk 'NR>1{print \$1/2}' | /home/projects/ip_10000/IBP_pipelines/cleansumstats_v1.0a/cleansumstats_images/2020-04-11-ubuntu-1804_stat_r_in_c.simg stat_r_in_c qnorm >> prepared_qnorm_vals
          cut -f 1 $st_filtered | paste - prepared_qnorm_vals > prepared_qnorm_vals2
          LC_ALL=C join -1 1 -2 1 -t "\$(printf '\t')" $st_filtered prepared_qnorm_vals2 > st_filtered2
  
          nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
          nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
          cat st_filtered2 | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
  
        else
          nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
          nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
          cat $st_filtered | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
        fi
      else
        touch st_inferred_stats
      fi
      """
  }
  
  ch_stats_selection
    .combine(ch_sfile_on_stream3, by: 0)
    .set{ ch_stats_selection2 }
  
  process select_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, inferred, sfile from ch_stats_selection2
      
      output:
      tuple datasetID, file("st_stats_for_output") into ch_stats_for_output
  
      script:
      """
      select_stats_for_output.sh $mfile $sfile $inferred > st_stats_for_output 
      """
  }


  ch_stats_filtered_remain2
    .combine(ch_stats_filtered_removed, by: 0)
    .set{ ch_combined_desc_material }

  process describe_rows_formatted_or_filtered {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, filtered_stats_remain, filtered_stats_removed from ch_combined_desc_material
      
      output:
      tuple datasetID, file("desc_*") into ch_desc_end
  
      script:
      """
      # prepare process specific descriptive statistics
      echo "\$(wc -l $filtered_stats_remain)" | awk '{print \$1}' > desc_st_filt_remain.txt
      cat $filtered_stats_removed | awk '{ seen[\$2] += 1 } END { for (i in seen) print i, seen[i] }' > desc_st_filt_removed.txt
      
      """
  }

  
  //mix the A1_A2_both and A1_solo channels
  ch_A2_exists2
    .mix(ch_A2_missing2)
    .into{ ch_allele_corrected_mix1; ch_allele_corrected_mix2 }
  
  ch_allele_corrected_mix1
    .combine(ch_stats_for_output, by: 0)
    .combine(ch_mapped_GRCh38, by: 0)
    .set{ ch_allele_corrected_and_outstats }
  
  process final_assembly {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, acorrected, stats, grch38 from ch_allele_corrected_and_outstats
      
      output:
      tuple datasetID, build, file("${datasetID}_${build}_cleaned"), file("${datasetID}_GRCh38") into ch_cleaned_file
  
      script:
      """
      #
      apply_modifier_on_stats.sh $acorrected $stats > ${datasetID}_${build}_cleaned
      # match the GRCh38 build and publish it as separate file
      echo -e "0\tCHR\tPOS" > ${datasetID}_GRCh38
      awk -vOFS="\t" 'NR>1{split(\$2,out,":"); print \$1, out[1], out[2]}' $grch38 | LC_ALL=C sort -k 1 - > sorted_GRCh38
      LC_ALL=C join -t "\$(printf '\t')" -1 1 -2 1 -o 1.1 1.2 1.3 sorted_GRCh38 ${datasetID}_${build}_cleaned > ${datasetID}_GRCh38
      """
  }

  ch_cleaned_file
    .combine(ch_input_sfile, by: 0)
    .combine(ch_sfile_on_stream5, by: 0)
    .set{ ch_to_write_to_filelibrary2 }

  process gzip_outfiles {
      input:
      tuple datasetID, build, sclean, scleanGRCh38, inputsfile, inputformatted from ch_to_write_to_filelibrary2

      output:
      tuple datasetID, build, path("sclean.gz"), path("scleanGRCh38.gz"), inputsfile, path("raw_formatted_rowindexed.gz") into ch_to_write_to_filelibrary3

      script:
      """
      # Store data in library
      gzip -c ${sclean} > sclean.gz
      gzip -c ${scleanGRCh38} > scleanGRCh38.gz
      gzip -c ${inputformatted} > raw_formatted_rowindexed.gz
      """
  }


  ch_to_write_to_filelibrary3.combine(ch_mfile_ok3, by: 0)
   .combine(ch_input_readme, by: 0)
   .combine(ch_input_pdf_stuff, by: 0)
   .combine(ch_one_line_metafile, by: 0)
   .combine(ch_software_versions)
   .set{ ch_to_write_to_filelibrary7 }

  process put_in_library {
  
      publishDir "${params.libdirpdfs}", mode: 'copy', overwrite: false, pattern: 'pmid_*'
      publishDir "${params.libdirsumstats}", mode: 'copyNoFollow', overwrite: true, pattern: 'sumstat_*'

      input:
      tuple datasetID, build, sclean, scleanGRCh38, inputsfile, inputformatted, mfile, readme, pmid, pdfpath, pdfsuppdir, onelinemeta, softv from ch_to_write_to_filelibrary7
      
      output:
      path("sumstat_*") into ch_end2
      path("pmid_*") into ch_end3
      tuple datasetID, env(libfolder), mfile, onelinemeta into ch_update_library_info_file
  
      script:
      """
      
      # copy the pdf and supplemental material if missing in pdf library
      cp ${pdfpath} pmid_${pmid}.pdf
      if [ -d "${params.libdirpdfs}/pmid_${pmid}_supp" ] ;then
        :
      else 
        mkdir pmid_${pmid}_supp
        i=1
        cat ${pdfsuppdir} | while read -r supp; do extension="\${supp##*.}"; cp -r \$supp pmid_${pmid}_supp/pmid_${pmid}_supp_\${i}.\${extension}; i=\$((i+1)); done
      fi
      
      # Scan for available ID
      libfolder=\$(assign_folder_id.sh ${params.libdirsumstats})
      mkdir -p "\${libfolder}"

      # Store data in library
      cp ${sclean} \${libfolder}/\${libfolder}_cleaned_${build}.gz
      cp ${scleanGRCh38} \${libfolder}/\${libfolder}_cleaned_GRCh38.gz
      cp ${inputformatted} \${libfolder}/\${libfolder}_raw_formatted_rowindexed.gz
      cp $inputsfile \${libfolder}/\${libfolder}_raw.gz
      cp $readme \${libfolder}/\${libfolder}_README.txt
      cp $onelinemeta \${libfolder}/\${libfolder}_one_line_summary_of_metadata.txt
      cp $softv \${libfolder}/\${libfolder}_software_versions.csv
      cat ${mfile} > \${libfolder}/\${libfolder}_meta.txt
      
      # Add link to the pdf and supplemental material
      ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf \${libfolder}/pmid_${pmid}.pdf
      mkdir -p \${libfolder}/pmid_${pmid}_supp
      ln -s ${params.libdirpdfs}/pmid_${pmid}_supp/* \${libfolder}/pmid_${pmid}_supp/.
      """
  }


  process update_inventory_file {
      publishDir "${params.libdir}", mode: 'copy', overwrite: true

      input:
      tuple datasetID, libfolder, mfile, onelinemeta from ch_update_library_info_file

      output:
      path("00_inventory.txt") into ch_00_inventory_end

      script:
      """
      # Make header if the file does not exist
      if [ -f ${params.libdir}/00_inventory.txt ] ; then 
        cat ${params.libdir}/00_inventory.txt ${onelinemeta} > 00_inventory.txt
      else
        cat ${baseDir}/assets/columns_for_one_line_summary.txt | while read -r varx; do 
          printf "\t%s" "\${varx}" >> one_line_summary_header
        done
        printf "\n" >> one_line_summary_header
        cat one_line_summary_header | sed -e 's/^[\t]//' > 00_inventory_new.txt
        cat 00_inventory_new.txt ${onelinemeta} > 00_inventory.txt
      fi
      
      """
  }

}

/*
 * Completion e-mail notification
 */
workflow.onComplete {

    // Set up the e-mail variables
    def subject = "[nf-core/cleansumstats] Successful: $workflow.runName"
    if (!workflow.success) {
      subject = "[nf-core/cleansumstats] FAILED: $workflow.runName"
    }
    def email_fields = [:]
    email_fields['version'] = workflow.manifest.version
    email_fields['runName'] = custom_runName ?: workflow.runName
    email_fields['success'] = workflow.success
    email_fields['dateComplete'] = workflow.complete
    email_fields['duration'] = workflow.duration
    email_fields['exitStatus'] = workflow.exitStatus
    email_fields['errorMessage'] = (workflow.errorMessage ?: 'None')
    email_fields['errorReport'] = (workflow.errorReport ?: 'None')
    email_fields['commandLine'] = workflow.commandLine
    email_fields['projectDir'] = workflow.projectDir
    email_fields['summary'] = summary
    email_fields['summary']['Date Started'] = workflow.start
    email_fields['summary']['Date Completed'] = workflow.complete
    email_fields['summary']['Pipeline script file path'] = workflow.scriptFile
    email_fields['summary']['Pipeline script hash ID'] = workflow.scriptId
    if (workflow.repository) email_fields['summary']['Pipeline repository Git URL'] = workflow.repository
    if (workflow.commitId) email_fields['summary']['Pipeline repository Git Commit'] = workflow.commitId
    if (workflow.revision) email_fields['summary']['Pipeline Git branch/tag'] = workflow.revision
    if (workflow.container) email_fields['summary']['Docker image'] = workflow.container
    email_fields['summary']['Nextflow Version'] = workflow.nextflow.version
    email_fields['summary']['Nextflow Build'] = workflow.nextflow.build
    email_fields['summary']['Nextflow Compile Timestamp'] = workflow.nextflow.timestamp

    // Check if we are only sending emails on failure
    email_address = params.email
    if (!params.email && params.email_on_fail && !workflow.success) {
        email_address = params.email_on_fail
    }

    // Render the TXT template
    def engine = new groovy.text.GStringTemplateEngine()
    def tf = new File("$baseDir/assets/email_template.txt")
    def txt_template = engine.createTemplate(tf).make(email_fields)
    def email_txt = txt_template.toString()

    // Render the HTML template
    def hf = new File("$baseDir/assets/email_template.html")
    def html_template = engine.createTemplate(hf).make(email_fields)
    def email_html = html_template.toString()

    // Render the sendmail template
    def smail_fields = [ email: email_address, subject: subject, email_txt: email_txt, email_html: email_html, baseDir: "$baseDir", mqcFile: mqc_report, mqcMaxSize: params.maxMultiqcEmailFileSize.toBytes() ]
    def sf = new File("$baseDir/assets/sendmail_template.txt")
    def sendmail_template = engine.createTemplate(sf).make(smail_fields)
    def sendmail_html = sendmail_template.toString()

    // Send the HTML e-mail
    if (email_address) {
        try {
          if ( params.plaintext_email ){ throw GroovyException('Send plaintext e-mail, not HTML') }
          // Try to send HTML e-mail using sendmail
          [ 'sendmail', '-t' ].execute() << sendmail_html
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (sendmail)"
        } catch (all) {
          // Catch failures and try with plaintext
          [ 'mail', '-s', subject, email_address ].execute() << email_txt
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (mail)"
        }
    }

    // Write summary e-mail HTML to a file
    def output_d = new File( "${params.outdir}/pipeline_info/" )
    if (!output_d.exists()) {
      output_d.mkdirs()
    }
    def output_hf = new File( output_d, "pipeline_report.html" )
    output_hf.withWriter { w -> w << email_html }
    def output_tf = new File( output_d, "pipeline_report.txt" )
    output_tf.withWriter { w -> w << email_txt }

    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_red = params.monochrome_logs ? '' : "\033[0;31m";

    if (workflow.stats.ignoredCount > 0 && workflow.success) {
      log.info "${c_purple}Warning, pipeline completed, but with errored process(es) ${c_reset}"
      log.info "${c_red}Number of ignored errored process(es) : ${workflow.stats.ignoredCount} ${c_reset}"
      log.info "${c_green}Number of successfully ran process(es) : ${workflow.stats.succeedCount} ${c_reset}"
    }

    if (workflow.success) {
        log.info "${c_purple}[nf-core/cleansumstats]${c_green} Pipeline completed successfully${c_reset}"
    } else {
        checkHostname()
        log.info "${c_purple}[nf-core/cleansumstats]${c_red} Pipeline completed with errors${c_reset}"
    }

}


def nfcoreHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
    ${c_blue}        ___     __   __   __   ___     ${c_green}/,-._.--~\'${c_reset}
    ${c_blue}  |\\ | |__  __ /  ` /  \\ |__) |__         ${c_yellow}}  {${c_reset}
    ${c_blue}  | \\| |       \\__, \\__/ |  \\ |___     ${c_green}\\`-._,-`-,${c_reset}
                                            ${c_green}`._,._,\'${c_reset}
    ${c_purple}  nf-core/cleansumstats v${workflow.manifest.version}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
