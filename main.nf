#!/usr/bin/env nextflow
/*
========================================================================================
                         nf-core/cleansumstats
========================================================================================
 nf-core/cleansumstats Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/nf-core/cleansumstats
----------------------------------------------------------------------------------------
*/

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run nf-core/cleansumstats --infile 'gwas-sumstats.gz' -profile singularity

    Mandatory arguments:
      --infile                      Path to tab-separated input data (must be surrounded with quotes)
      -profile                      Configuration profile to use. Can use multiple (comma separated)
                                    Available: conda, docker, singularity, awsbatch, test and more.

    References:                     If not specified in the configuration file or you wish to overwrite any of the references.
      --dbsnp38                     Path to dbsnp GRCh38 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp37                     Path to dbsnp GRCh37 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp36                     Path to dbsnp GRCh36 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp35                     Path to dbsnp GRCh35 reference. Has to be sorted on chr:pos as first column using LC_ALL=C.

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    AWSBatch options:
      --awsqueue                    The AWSBatch JobQueue that needs to be set when running on AWSBatch
      --awsregion                   The AWS Region for your AWS Batch job to run on
    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// Check if genome exists in the config file
//if (params.genomes && params.genome && !params.genomes.containsKey(params.genome)) {
//    exit 1, "The provided genome '${params.genome}' is not available in the iGenomes file. Currently the available genomes are ${params.genomes.keySet().join(", ")}"
//}


// Set channels
if (params.dbsnp38) { ch_dbsnp38 = file(params.dbsnp38, checkIfExists: true) }
if (params.dbsnp37) { ch_dbsnp37 = file(params.dbsnp37, checkIfExists: true) } 
if (params.dbsnp36) { ch_dbsnp36 = file(params.dbsnp36, checkIfExists: true) } 
if (params.dbsnp35) { ch_dbsnp35 = file(params.dbsnp35, checkIfExists: true) } 
if (params.dbsnpRSID) { ch_dbsnpRSID = file(params.dbsnpRSID, checkIfExists: true) } 
ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)

// Stage config files
ch_multiqc_config = file(params.multiqc_config, checkIfExists: true)
ch_output_docs = file("$baseDir/docs/output.md", checkIfExists: true)

//example from nf-core how to use fasta
//params.fasta = params.genome ? params.genomes[ params.genome ].fasta ?: false : false
//if (params.fasta) { ch_fasta = file(params.fasta, checkIfExists: true) }

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}

if ( workflow.profile == 'awsbatch') {
  // AWSBatch sanity checking
  if (!params.awsqueue || !params.awsregion) exit 1, "Specify correct --awsqueue and --awsregion parameters on AWSBatch!"
  // Check outdir paths to be S3 buckets if running on AWSBatch
  // related: https://github.com/nextflow-io/nextflow/issues/813
  if (!params.outdir.startsWith('s3:')) exit 1, "Outdir not on S3 - specify S3 Bucket to run on AWSBatch!"
  // Prevent trace files to be stored on S3 since S3 does not support rolling files.
  if (workflow.tracedir.startsWith('s3:')) exit 1, "Specify a local tracedir or run without trace! S3 cannot be used for tracefiles."
}



// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38 
if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37 
if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36 
if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35 
if (params.dbsnpRSID) summary['dbsnpRSID'] = params.dbsnpRSID 

summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
if (params.email || params.email_on_fail) {
  summary['E-mail Address']    = params.email
  summary['E-mail on failure'] = params.email_on_fail
  summary['MultiQC maxsize']   = params.maxMultiqcEmailFileSize
}
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'nf-core-cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}



if (params.generateMetafile){
  ch_metatemplate = file("${baseDir}/assets/meta_data_readMe_v4.txt")
  ch_sumstat_file = Channel
                   .fromPath(params.input, type: 'file')
                   .map { file -> tuple(file.baseName, file) }

  process create_meta_data_template {
  
      publishDir "${params.outdir}", mode: 'copy', overwrite: false
  
      input:
      tuple basefilename, sfilename from ch_sumstat_file
  
      output:
      file("${basefilename}.meta") into ch_metafile_template_out
  
      script:
      """
      cat ${ch_metatemplate} > ${basefilename}.meta
      """
  }
}else if(params.generateDbSNPreference){

  process create_dnsnp_database {

      publishDir "${params.outdir}", mode: 'copy', overwrite: false

      output:
      file("tmp.db") into ch_db

      script:
     // #1-22
     // ##X
     // ##Y
     // ##MT
     // #
     // ##Download from web
     // #wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.*
     // #
     // ##cp All_20180418.vcf.gz All_20180418_GRCh38.bed
     // #
     // #
     // #module load tools
     // #module load bcftools/1.9
     // #
     // #date 
     // ##Wed Feb 26 10:11:02 CET 2020
     // ##bcftools view All_20180418.vcf.gz -Ov | grep -v "#" | awk '{print "chr"$1, $2, $2, $3, $4, $5}' > All_20180418_GRCh38.bed
     // #date 
     // ##Wed Feb 26 10:48:33 CET 2020
     // #
     // ##Add extra chrpos column
     // #bcftools view All_20180418.vcf.gz -Ov | grep -v "#" | awk '{print "chr"$1, $2, $2,  $1":"$2, $3, $4, $5}' > All_20180418_GRCh38.bed
     // #
     // #module load tools
     // #module load singularity/3.5.2
     // #
     // ##map to GRCh37
     // #./images/liftover-lates.img bed data/hg38ToHg19.over.chain.gz All_20180418_GRCh38.bed All_20180418_GRCh37.bed
     // #awk '{tmp=$1; sub(/[cC][hH][rR]/, "", tmp); print $1, $2, $3, tmp":"$2, $4, $5, $6, $7}' All_20180418_GRCh37.bed > All_20180418_liftcoord_GRCh37_GRCh38.bed
     // #
     // #./images/liftover-lates.img bed data/hg19ToHg18.over.chain.gz All_20180418_liftcoord_GRCh37_GRCh38.bed All_20180418_liftcoord_GRCh36.bed
     // #./images/liftover-lates.img bed data/hg19ToHg17.over.chain.gz All_20180418_liftcoord_GRCh37_GRCh38.bed All_20180418_liftcoord_GRCh35.bed
     // #
     // #awk '{tmp=$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"$2, $4, $5, $6, $7, $8}' All_20180418_liftcoord_GRCh36.bed > All_20180418_GRCh36_GRCh37_GRCh38.bed
     // #awk '{tmp=$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"$2, $4, $5, $6, $7, $8}' All_20180418_liftcoord_GRCh35.bed > All_20180418_GRCh35_GRCh37_GRCh38.bed
     // #awk '{print $4, $5, $6, $7, $8}' All_20180418_liftcoord_GRCh37_GRCh38.bed > All_20180418_GRCh37_GRCh38.bed
     // #awk '{print $5, $4, $6, $7, $8}' All_20180418_liftcoord_GRCh37_GRCh38.bed > All_20180418_GRCh38_GRCh37.bed
     // #
     // #
     // #LC_ALL=C sort -k 1 --parallel 8 All_20180418_GRCh37_GRCh38.bed > All_20180418_GRCh37_GRCh38.sorted.bed
     // #LC_ALL=C sort -k 1 --parallel 8 All_20180418_GRCh38_GRCh37.bed > All_20180418_GRCh38_GRCh37.sorted.bed
     // #LC_ALL=C sort -k 1 --parallel 8 All_20180418_GRCh36_GRCh37_GRCh38.bed > All_20180418_GRCh36_GRCh37_GRCh38.sorted.bed
     // #LC_ALL=C sort -k 1 --parallel 8 All_20180418_GRCh35_GRCh37_GRCh38.bed > All_20180418_GRCh35_GRCh37_GRCh38.sorted.bed
     // #
     // #
     // ## Make version sorted on RSID to get correct coordinates
     // # awk '{print $3, $1, $2, $4, $5}' All_20180418_GRCh37_GRCh38.sorted.bed | LC_ALL=C sort -k 1 --parallel 8 > All_20180418_RSID_GRCh37_GRCh38.sorted.bed
     // #
     // #nextflow run ../../../repos/nf-core-cleansumstats --input 'raw_formatted_row-indexed/sumstat_*' --outdir raw_formatted_row-indexed_cleaned \
     // #--dbsnp38 ../../../data/dbsnp151/All_20180418_GRCh38_GRCh37.sorted.bed \
     // #--dbsnp37 ../../../data/dbsnp151/All_20180418_GRCh37_GRCh38.sorted.bed \
     // #--dbsnp36 ../../../data/dbsnp151/All_20180418_GRCh36_GRCh37_GRCh38.sorted.bed \
     // #--dbsnp35 ../../../data/dbsnp151/All_20180418_GRCh35_GRCh37_GRCh38.sorted.bed \
     // #-resume
     // #
     // #
     // #
     // ##remove temp files
     // #rm All_20180418_GRCh38.chrpos.bed
     // #rm All_20180418_GRCh37.chrpos.bed
     // #rm All_20180418_GRCh36.chrpos.bed
     // #rm All_20180418_GRCh35.chrpos.bed
     // #rm All_20180418_GRCh37.bed
     // #rm All_20180418_GRCh36.bed
     // #rm All_20180418_GRCh35.bed
     // #
      """
      cat "hek" > tmp.db

      """
  }

}else {


  process get_software_versions {
      publishDir "${params.outdir}/pipeline_info", mode: 'copy',
          saveAs: { filename ->
              if (filename.indexOf(".csv") > 0) filename
              else null
          }
  
      output:
      file 'software_versions_mqc.yaml' into software_versions_yaml
      file "software_versions.csv" into ch_software_versions

  
      script:
      """
      echo $workflow.manifest.version > v_pipeline.txt
      echo $workflow.nextflow.version > v_nextflow.txt
      #sstools-version > v_sumstattools.txt
      scrape_software_versions.py &> software_versions_mqc.yaml
      """
  }
  
  
  //to_stream_sumstat_dir = Channel
  //                .fromPath(params.input, type: 'dir')
  //                .map { dir -> tuple(dir.baseName, dir) }

  //use metafile filename as datasetID through the pipeline
  ch_mfile_check = Channel
                  .fromPath(params.input, type: 'file')
                  .map { file -> tuple(file.baseName, file) }
  
  //to_stream_sumstat_dir.into { ch_to_stream_sumstat_dir1; ch_to_stream_sumstat_dir2 }


  process check_and_force_basic_formats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile from ch_mfile_check
  
      output:
      tuple datasetID, file("mfile_unix_safe") into ch_mfile_ok
      tuple datasetID, file("${datasetID}_sfile") into ch_sfile_ok
      tuple datasetID, env(spath) into ch_input_sfile
      tuple datasetID, env(rpath) into ch_input_readme
      tuple datasetID, env(pmid) into ch_pmid
      tuple datasetID, file("*.log") into inputchecker_log_files
      tuple datasetID, file("${datasetID}_header") into extra_stuff1
      tuple datasetID, file("${datasetID}_sfile_1000") into extra_stuff2
      tuple datasetID, file("${datasetID}_sfile_1000_formatted") into extra_stuff3
      tuple datasetID, env(pmid), env(pdfpath), file("${datasetID}_pdf_suppfiles.txt") into ch_input_pdf_stuff
      tuple datasetID, file("${datasetID}_one_line_summary_of_metadata.txt") into ch_one_line_metafile
      file("mfile_sent_in") 
      tuple datasetID, file("desc_force_tab_sep_BA.txt") into ch_desc_prep_force_tab_sep_BA
      tuple datasetID, env(sumstatID) into ch_preassigned_sumstat_id
  
      script:
      """


      metaDir="\$(dirname ${mfile})"
      cat ${mfile} > mfile_sent_in
      # Clean meta file from windows return characters
      awk '{ sub("\\r\$", ""); print }' ${mfile} > mfile_unix_safe
      
      # Check if the datasetID folder is already present, if so just increment a number to get the new outname
      #  this is because a metafile can have the same name as another even though the content might be different. 

      # Check if field for variable exists and if the file specified exists
      spath="\$(check_meta_file_references.sh "path_sumStats" mfile_unix_safe \$metaDir)"
      rpath="\$(check_meta_file_references.sh "path_readMe" mfile_unix_safe \$metaDir)"
      pdfpath="\$(check_meta_file_references.sh "path_pdf" mfile_unix_safe \$metaDir)"
      check_meta_file_references.sh "path_pdfSupp" mfile_unix_safe \${metaDir} > ${datasetID}_pdf_suppfiles.txt

      Px="\$(grep "^study_PMID=" mfile_unix_safe)"
      pmid="\$(echo "\${Px#*=}")"

      # Check library if this has been processed before
      # TODO after the script producing the 00inventory.txt has been created 
      
      # Make complete metafile check
      echo "\$(head -n 1 < <(zcat \$spath))" > ${datasetID}_header
      check_meta_data_format.sh mfile_unix_safe ${datasetID}_header ${datasetID}_mfile_format.log

      #Check if new sumstatname should be assigned
      if grep -Pq "^cleansumstats_ID=" mfile_unix_safe
      then
        SIDx="\$(grep "^cleansumstats_ID=" mfile_unix_safe)"
        sumstatID="\$(echo "\${SIDx#*=}")"
      else
        sumstatID="missing"
      fi
      
      # Make a one line metafile to use for the inventory file and test if dataset is already in library
      cat ${baseDir}/assets/columns_for_one_line_summary.txt | while read -r varx; do 
        Px="\$(grep "^\${varx}=" mfile_unix_safe)"
        var="\$(echo "\${Px#*=}")"
        printf "\t%s" "\${var}" >> one_line_summary
      done
      printf "\\n" >> one_line_summary
      cat one_line_summary | sed -e 's/^[\t]//' > ${datasetID}_one_line_summary_of_metadata.txt

      # Sumstat file check on first 1000 lines
      echo "\$(head -n 1000 < <(zcat \$spath))" | gzip -c > ${datasetID}_sfile_1000
      check_and_format_sfile.sh ${datasetID}_sfile_1000 ${datasetID}_sfile_1000_formatted ${datasetID}_sfile_1000_format.log
      
      # Make second sumstat file check on all lines
      check_and_format_sfile.sh \$spath ${datasetID}_sfile ${datasetID}_sfile_format.log
      #check_and_format_sfile.sh ${datasetID}_sfile_1000 ${datasetID}_sfile ${datasetID}_sfile_format.log
      
      #process before and after stats
      rowsBefore="\$(zcat \$spath | wc -l )"
      rowsAfter="\$(wc -l ${datasetID}_sfile | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tForce tab separation" > desc_force_tab_sep_BA.txt

      """
  }

  process add_sorted_index_sumstat {
  
  
      input:
      tuple datasetID, sfile from ch_sfile_ok

      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      output:
      tuple datasetID, file("prep_sfile_added_rowindex") into ch_sfile_on_stream
      tuple datasetID, file("desc_add_sorted_rowindex_BA.txt") into ch_desc_prep_add_sorted_rowindex_BA
  
      script:
      """
      cat $sfile | sstools-raw add-index | LC_ALL=C sort -k 1 - > prep_sfile_added_rowindex
      
      #process before and after stats
      rowsBefore="\$(wc -l $sfile | awk '{print \$1}')"
      rowsAfter="\$(wc -l prep_sfile_added_rowindex | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tAdd sorted rowindex, which maps back to the unfiltered file" > desc_add_sorted_rowindex_BA.txt
      """
  }
  
  ch_mfile_ok.into { ch_mfile_ok1; ch_mfile_ok2; ch_mfile_ok3; ch_mfile_ok4; ch_mfile_ok5; ; ch_mfile_ok6}
  ch_sfile_on_stream.into { ch_sfile_on_stream1; ch_sfile_on_stream2; ch_sfile_on_stream3; ch_sfile_on_stream4; ch_sfile_on_stream5 }
  ch_mfile_and_stream=ch_mfile_ok1.join(ch_sfile_on_stream1)
  ch_mfile_and_stream.into { ch_check_gb; ch_liftover; ch_liftover1; ch_liftover2; ch_stats_inference }
  
  process does_chrpos_exist {
  
      input:
      tuple datasetID, mfile from ch_mfile_ok6
      
      output:
      tuple datasetID, env(CHRPOSexists) into ch_present_CHRPOS
  
      script:
      """
      CHRPOSexists=\$(does_CHR_and_POS_exist.sh $mfile)
      """
  }

  //Create filter for when CHR and POS exists or not
  ch_present_CHRPOS_br=ch_present_CHRPOS.branch { key, value -> 
                 CHRPOSexists: value == "true"
                 CHRPOSmissing: value == "false"
                  }
  
  //split the channels based on filter
  ch_present_CHRPOS_br2=ch_present_CHRPOS_br.CHRPOSexists
  ch_present_CHRPOS_br3=ch_present_CHRPOS_br.CHRPOSmissing
  
  //combine each channel with the matching datasetID
  ch_CHRPOS_exists=ch_liftover1.combine(ch_present_CHRPOS_br2, by: 0)
  ch_CHRPOS_missing=ch_liftover2.combine(ch_present_CHRPOS_br3, by: 0)

  // LIFTOVER BRANCH 1

  process prep_dbsnp_mapping_by_sorting_rsid_version {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile, chrposExists from ch_CHRPOS_missing
  
      output:
      tuple datasetID, mfile, file("gb_lift_sorted") into ch_liftover_33
      tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos_rsid
      file("gb_*")

      script:
      """
  
      Sx="\$(grep "^col_SNP=" $mfile)"
      colSNP="\$(echo "\${Sx#*=}")"
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colSNP}" -n"0,RSID" | awk -vFS="\t" -vOFS="\t" '{print \$2,\$1}' > gb_lift
      LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
      #
      #process before and after stats
      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lift_sorted | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
  
      """
  }

  process liftover_GRCh37_and_GRCh38_and_map_to_dbsnp_rsid_version {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, fsorted from ch_liftover_33
      
      output:
      tuple datasetID, mfile, file("gb_lifted_and_mapped_to_GRCh37_and_GRCh38") into ch_liftover_49
      tuple datasetID, file("desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA_rsid
      tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build_rsid
  
      script:
      """
      LC_ALL=C join -1 1 -2 1 ${fsorted} ${ch_dbsnpRSID} | awk -vFS="[[:space:]]" -vOFS="\t" '{print \$4,\$3,\$2,\$1,\$5,\$6}'  > gb_lifted_and_mapped_to_GRCh37_and_GRCh38
      
      # Process before and after stats
      rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lifted_and_mapped_to_GRCh37_and_GRCh38 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh37 and GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA

      # Make an empty stats file as we are not trying to infer genome build
      echo "No inference of build going from RSID" > ${datasetID}.stats
      """
  }
  
  
  // LIFTOVER BRANCH 2

  whichbuild = ['GRCh35', 'GRCh36', 'GRCh37', 'GRCh38']
  
  process genome_build_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile, chrposexist from ch_CHRPOS_exists
      each build from whichbuild
  
      output:
      tuple datasetID, file("${datasetID}*.res") into ch_genome_build_stats
      file("gb_*")
  
      script:
      """
  
      colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
      colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
      echo "\${colCHR}" > gb_ad-hoc-do_funx_CHR
      echo "\${colPOS}" > gb_ad-hoc-do_funx_POS

      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" > gb_extract_and_format_chr_and_pos_to_detect_build
      awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' gb_extract_and_format_chr_and_pos_to_detect_build > gb_ready_to_join_to_detect_build
      LC_ALL=C sort -k1,1 gb_ready_to_join_to_detect_build > gb_ready_to_join_to_detect_build_sorted
      format_chrpos_for_dbsnp.sh ${build} gb_ready_to_join_to_detect_build_sorted ${ch_dbsnp35} ${ch_dbsnp36} ${ch_dbsnp37} ${ch_dbsnp38} > ${build}.map
      sort -u -k1,1 ${build}.map | wc -l | awk -vOFS="\t" -vbuild=${build} '{print \$1,build}' > ${datasetID}.${build}.res
  
      """
  }
  
  
  
  ch_genome_build_stats_grouped = ch_genome_build_stats.groupTuple(by:0,size:4)
  
  process infer_genome_build {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, file(ujoins) from ch_genome_build_stats_grouped
  
      
      output:
      tuple datasetID, env(GRChmax) into ch_known_genome_build
      tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build_chrpos
  
      script:
      """
      for gbuild in ${ujoins}
      do
          cat \$gbuild >> ${datasetID}.stats
      done
      GRChmax="\$(cat ${datasetID}.stats | sort -nr -k1,1 | head -n1 | awk '{print \$2}')"
      """
  
  }
  
  ch_liftover_2=ch_liftover.join(ch_known_genome_build)
  
  process prep_dbsnp_mapping_by_sorting_chrpos_version {
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile, gbmax from ch_liftover_2
  
      output:
      tuple datasetID, mfile, file("gb_lift_sorted"), gbmax into ch_liftover_3
      tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos
  
      script:
      """
      
      colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
      colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
  
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" | awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' > gb_lift
      LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
      
      #process before and after stats
      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lift_sorted | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
      """
  
  }
  
  process liftover_GRCh37_and_GRCh38_and_map_to_dbsnp_chrpos_version {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, fsorted, gbmax from ch_liftover_3
      
      output:
      tuple datasetID, mfile, file("gb_lifted_and_mapped_to_GRCh37_and_GRCh38") into ch_liftover_44
      tuple datasetID, file("desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA_chrpos
  
      script:
      """
      format_chrpos_for_dbsnp.sh ${gbmax} ${fsorted} ${ch_dbsnp35} ${ch_dbsnp36} ${ch_dbsnp37} ${ch_dbsnp38} > gb_lifted_and_mapped_to_GRCh37_and_GRCh38
      
      #process before and after stats
      rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lifted_and_mapped_to_GRCh37_and_GRCh38 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh37 and GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA
      """
  }

  //mix the chrpos and rsid channels
  ch_liftover_49
    .mix(ch_liftover_44)
    .set{ ch_liftover_mix_X }

  ch_stats_genome_build_rsid
    .mix(ch_stats_genome_build_chrpos)
    .set{ ch_stats_genome_build }

  ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA_rsid
    .mix(ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA_chrpos)
    .set{ ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA }

  ch_desc_prep_for_dbsnp_mapping_BA_chrpos_rsid
    .mix(ch_desc_prep_for_dbsnp_mapping_BA_chrpos)
    .set{ ch_desc_prep_for_dbsnp_mapping_BA }

  

  process remove_duplicated_chr_position_allele_rows {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, liftedandmapped from ch_liftover_mix_X
      
      output:
      tuple datasetID, mfile, file("gb_unique_rows5") into ch_liftover_4
      file("gb_unique_rows")
      file("gb_unique_rows2")
      file("gb_unique_rows3")
      file("gb_unique_rows4")
      file("gb_duplicated_rows_removed_GRCh37")
      file("gb_duplicated_rows_removed_GRCh37_hard")
      //file("gb_duplicated_rows_removed_GRCh37_allele_switch_in_db")
      file("gb_duplicated_rows_removed_GRCh38")
      file("gb_duplicated_rows_removed_GRCh38_hard")
      //file("gb_duplicated_rows_removed_GRCh38_allele_switch_in_db")
      file("gb_multiallelic_rows_removed")
      tuple datasetID, file("desc_removed_duplicated_rows_GRCh37_BA") into ch_desc_removed_duplicated_rows_GRCh37_BA
      tuple datasetID, file("desc_removed_duplicated_rows_GRCh37_HARD_BA") into ch_desc_removed_duplicated_rows_GRCh37_HARD_BA
      //tuple datasetID, file("desc_removed_duplicated_rows_GRCh37_AF_BA") into ch_desc_removed_duplicated_rows_GRCh37_AF_BA
      tuple datasetID, file("desc_removed_duplicated_rows_GRCh38_BA") into ch_desc_removed_duplicated_rows_GRCh38_BA
      tuple datasetID, file("desc_removed_duplicated_rows_GRCh38_HARD_BA") into ch_desc_removed_duplicated_rows_GRCh38_HARD_BA
      //tuple datasetID, file("desc_removed_duplicated_rows_GRCh38_AF_BA") into ch_desc_removed_duplicated_rows_GRCh38_AF_BA
      tuple datasetID, file("desc_removed_multiallelic_rows_BA") into ch_desc_removed_multiallelic_rows_BA

      script:
      """
      #remove all but the first ecnountered row where chr:pos REF and ALT for GRCh37
      touch gb_duplicated_rows_removed_GRCh37
      awk 'BEGIN{r0="initrowhere"} {var=\$2"-"\$5"-"\$6; if(r0!=var){print \$0}else{print \$0 > "gb_duplicated_rows_removed_GRCh37"}; r0=var}' $liftedandmapped > gb_unique_rows
      #awk 'BEGIN{pos0="init"; ref="init"; alt="init"} {pos=\$2; ref=\$5; alt=\$6; if(pos0==pos && ((ref0==ref && alt0==alt) || (ref0==alt && alt0==ref))){print \$0 > "gb_duplicated_rows_removed_GRCh37_allele_switch_in_db"}else{print \$0}; pos0=\$2; ref0=\$5; alt0=\$6}' gb_unique_rows > gb_unique_rows2
      
      #remove all but the first ecnountered row where chr:pos REF and ALT for GRCh38
      touch gb_duplicated_rows_removed_GRCh38
      awk 'BEGIN{r0="initrowhere"} {var=\$1"-"\$5"-"\$6; if(r0!=var){print \$0}else{print \$0 > "gb_duplicated_rows_removed_GRCh38"}; r0=var}' gb_unique_rows > gb_unique_rows2

      #Filter only on position duplicates (A very hard filter but is good enough for alpha release)
      touch gb_duplicated_rows_removed_GRCh37_hard
      touch gb_duplicated_rows_removed_GRCh38_hard
      awk 'BEGIN{r0="initrowhere"} {var=\$2; if(r0!=var){print \$0}else{print \$0 > "gb_duplicated_rows_removed_GRCh37_hard"}; r0=var}' gb_unique_rows2 > gb_unique_rows3
      awk 'BEGIN{r0="initrowhere"} {var=\$1; if(r0!=var){print \$0}else{print \$0 > "gb_duplicated_rows_removed_GRCh38_hard"}; r0=var}' gb_unique_rows3 > gb_unique_rows4
      #awk 'BEGIN{pos0="init"; ref="init"; alt="init"} {pos=\$1; ref=\$5; alt=\$6; if(pos0==pos && ((ref0==ref && alt0==alt) || (ref0==alt && alt0==ref))){print \$0 > "gb_duplicated_rows_removed_GRCh38_allele_switch_in_db"}else{print \$0}; pos0=\$2; ref0=\$5; alt0=\$6}' gb_unique_rows3 > gb_unique_rows4
      
      touch gb_multiallelic_rows_removed
      awk -vFS="\t" -vOFS="\t" '{if(\$6 ~ /,/){print > "gb_multiallelic_rows_removed"}else{print \$0} }' gb_unique_rows4 > gb_unique_rows5
      
      #process before and after stats
      rowsBefore="\$(wc -l ${liftedandmapped} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved duplicated rows in respect to chr:pos and dbsnp REF/ALT, GRCh37" > desc_removed_duplicated_rows_GRCh37_BA

      rowsBefore="\$(wc -l gb_unique_rows | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows2 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved duplicated rows in respect to chr:pos and dbsnp REF/ALT, GRCh38" > desc_removed_duplicated_rows_GRCh38_BA

      rowsBefore="\$(wc -l gb_unique_rows2 | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows3 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved duplicated rows in respect to only chr:pos, GRCh37" > desc_removed_duplicated_rows_GRCh37_HARD_BA

      rowsBefore="\$(wc -l gb_unique_rows3 | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows4 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved duplicated rows in respect to only chr:pos, GRCh38" > desc_removed_duplicated_rows_GRCh38_HARD_BA

      rowsBefore="\$(wc -l gb_unique_rows4 | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows5 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved multi-allelics" > desc_removed_multiallelic_rows_BA
      """
  }


  process split_off_GRCh38 {
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, liftedandmapped from ch_liftover_4
      
      output:
      tuple datasetID, val("GRCh37"), mfile, file("gb_lifted_GRCh37") into ch_mapped_GRCh37
      tuple datasetID, file("gb_lifted_GRCh38") into ch_mapped_GRCh38
      tuple datasetID, file("desc_keep_only_GRCh37_version_BA.txt") into ch_desc_keep_only_GRCh37_version_BA  
      tuple datasetID, file("desc_keep_a_GRCh38_reference_BA.txt") into ch_desc_keep_a_GRCh38_reference_BA  

      script:
      """
      #prepare GRCh37 for downstream analysis
      awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' $liftedandmapped > gb_lifted_GRCh37

      #split off GRCh38 to use only for coordinate reference
      awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$1,\$4,\$5,\$6}' $liftedandmapped > gb_lifted_GRCh38

      
      #process before and after stats
      rowsBefore="\$(wc -l $liftedandmapped | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lifted_GRCh37 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tKeep only GRCh37 coordinates alleleinfo, which will be the file subjected to further cleaning" > desc_keep_only_GRCh37_version_BA.txt

      
      #process before and after stats
      #rowsBefore="\$(wc -l $liftedandmapped | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_lifted_GRCh38 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tSplit off a version of GRCh38 as coordinate reference" > desc_keep_a_GRCh38_reference_BA.txt
      """
  }
  


  process split_multiallelics_and_resort_index {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, liftgrs from ch_mapped_GRCh37
      
      output:
      tuple datasetID, build, mfile, file("gb_multialleles_splittorows") into ch_allele_correction
      tuple datasetID, file("desc_split_multi_allelics_and_sort_on_rowindex_BA.txt") into ch_desc_split_multi_allelics_and_sort_on_rowindex_BA  
      file("gb_splitted_multiallelics")
  
      script:
      """
      split_multiallelics_to_rows.sh $liftgrs > gb_splitted_multiallelics
      echo -e "0\tCHRPOS\tRSID\tA1\tA2" > gb_multialleles_splittorows
      LC_ALL=C sort -k1,1 gb_splitted_multiallelics >> gb_multialleles_splittorows
      
      #process before and after stats (rows is -1 because of header)
      rowsBefore="\$(wc -l $liftgrs | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_multialleles_splittorows | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tSplit multi-allelics to multiple rows and sort on original rowindex (Should make no diffrence because of the multi-allelic filter)" > desc_split_multi_allelics_and_sort_on_rowindex_BA.txt

      """
  }
  
  ch_allele_correction_combine=ch_allele_correction.combine(ch_sfile_on_stream2, by: 0)
  ch_allele_correction_combine.into{ ch_allele_correction_combine1; ch_allele_correction_combine2 }
  
  process does_exist_A2 {
  
      input:
      tuple datasetID, mfile from ch_mfile_ok2
      
      output:
      tuple datasetID, env(A2exists) into ch_present_A2
  
      script:
      """
      A2exists=\$(doesA2exist.sh $mfile)
      """
  }
  
  //Create filter for when A2 exists or not
  ch_present_A2_br=ch_present_A2.branch { key, value -> 
                  A2exists: value == "true"
                  A2missing: value == "false"
                  }
  
  //split the channels based on filter
  ch_present_A2_br2=ch_present_A2_br.A2exists
  ch_present_A2_br3=ch_present_A2_br.A2missing
  
  //combine each channel with the matching datasetID
  ch_A2_exists=ch_allele_correction_combine1.combine(ch_present_A2_br2, by: 0)
  ch_A2_missing=ch_allele_correction_combine2.combine(ch_present_A2_br3, by: 0)
  
  process allele_correction_A1_A2 {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, mapped, sfile, A2exists from ch_A2_exists
      
      output:
      tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_exists2
      tuple datasetID, file("disc_notGCTA"),file("disc_indel"), file("disc_hom"), file("disc_palin"), file("disc_notPossPair"), file("disc_notExpA2") into ch_describe_allele_filter1
      tuple datasetID, file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notGCTA_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_indel_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_hom_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_palin_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notPossPair_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notExpA2_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_sanity_BA.txt") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA

      script:
      """
      echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
      
      #init some the files collecting variants removed because of allele composition
      touch disc_notGCTA
      touch disc_indel
      touch disc_hom
      touch disc_palin
      touch disc_notPossPair
      touch disc_notExpA2

      colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
      colA2=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "altallele")
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}|\${colA2}" -n"0,A1,A2" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 1.3 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${mapped} | tail -n+2 | sstools-eallele correction -f - >> ${build}_acorrected

      #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
      rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l disc_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notGCTA_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels" > desc_filtered_allele-pairs_with_dbsnp_as_reference_indel_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes" > desc_filtered_allele-pairs_with_dbsnp_as_reference_hom_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes" > desc_filtered_allele-pairs_with_dbsnp_as_reference_palin_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notPossPair_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notExpA2_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l ${build}_acorrected | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tsanity sanity check that final filtered file before and after file have same row count" > desc_filtered_allele-pairs_with_dbsnp_as_reference_sanity_BA.txt
      """
  }
  
  
  process allele_correction_A1 {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, mapped, sfile, A2missing from ch_A2_missing
      
      output:
      tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_missing2
      file("${build}_mapped2")
      //tuple datasetID, file("disc_indel"), file("disc_notGCTA"), file("disc_notPossPair"), file("disc_palin") into ch_describe_allele_filter2
      tuple datasetID, file("disc_notGCTA"),file("disc_indel"), file("disc_hom"), file("disc_palin"), file("disc_notPossPair"), file("disc_notExpA2") into ch_describe_allele_filter2
      //tuple datasetID, file("desc_filtered_allele-pairs_with_dbsnp_as_reference_BA.txt") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA
      tuple datasetID, file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notGCTA_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_indel_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_hom_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_palin_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notPossPair_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_notExpA2_BA.txt"), file("desc_filtered_allele-pairs_with_dbsnp_as_reference_sanity_BA.txt") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA
  
      script:
      """
      multiallelic_filter.sh $mapped > ${build}_mapped2
      echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
      #
      #init some the files collecting variants removed because of allele composition
      touch disc_notGCTA
      touch disc_indel
      touch disc_hom
      touch disc_palin
      touch disc_notPossPair
      touch disc_notExpA2
  
      colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
      cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}" -n"0,A1" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${build}_mapped2 | tail -n+2 | sstools-eallele correction -f - -a >> ${build}_acorrected 

      #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
      rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l disc_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notGCTA_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels" > desc_filtered_allele-pairs_with_dbsnp_as_reference_indel_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes" > desc_filtered_allele-pairs_with_dbsnp_as_reference_hom_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes" > desc_filtered_allele-pairs_with_dbsnp_as_reference_palin_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notPossPair_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l disc_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" > desc_filtered_allele-pairs_with_dbsnp_as_reference_notExpA2_BA.txt

      rowsBefore="\${rowsAfter}"
      rowsAfter="\$(wc -l ${build}_acorrected | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tsanity sanity check that final filtered file before and after file have same row count" > desc_filtered_allele-pairs_with_dbsnp_as_reference_sanity_BA.txt
  
      """
  }

  //put the two brances into the same channel (as only one will be used per file, there will be no duplicates)
  ch_describe_allele_filter1
    .mix(ch_describe_allele_filter2)
    .set{ ch_describe_allele_filter }

  ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
    .mix(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA)
    .set{ ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA }

  //mix the A1_A2_both and A1_solo channels
  ch_A2_exists2
    .mix(ch_A2_missing2)
    .set{ ch_allele_corrected_mix_X }
  
  process remove_duplicated_chr_position_rows {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, acorrected from ch_allele_corrected_mix_X
      
      output:
      tuple datasetID, build, mfile, file("gb_unique_rows3_sorted") into ch_allele_corrected_mix_Y
      file("gb_unique_rows3")
      file("gb_acorrected_sorted_on_chrpos")
      file("gb_duplicated_chr_pos_rows_removed")
      tuple datasetID, file("desc_removed_duplicated_chr_pos_rows_BA") into ch_desc_removed_duplicated_chr_pos_rows_BA

      script:
      """
      #sort on fourth column
      LC_ALL=C sort -k4,1 $acorrected > gb_acorrected_sorted_on_chrpos
      
      #remove all but the first ecnountered row where chr:pos (seems some chrpos that mapped to more than one rsid are caught here)
      touch gb_duplicated_chr_pos_rows_removed
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "gb_duplicated_chr_pos_rows_removed"}; r0=var}' gb_acorrected_sorted_on_chrpos > gb_unique_rows3
      
      #re-sort on first column
      LC_ALL=C sort -k1,1 gb_unique_rows3 > gb_unique_rows3_sorted
      
      #process before and after stats
      rowsBefore="\$(wc -l ${acorrected} | awk '{print \$1}')"
      rowsAfter="\$(wc -l gb_unique_rows3_sorted | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tRemoved duplicated rows in respect to chr:pos" > desc_removed_duplicated_chr_pos_rows_BA

      """
  }
  ch_allele_corrected_mix_Y
    .into{ ch_allele_corrected_mix1; ch_allele_corrected_mix2 }

  process describe_rows_filtered_by_allele_filter {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, disc_notGCTA, disc_indel, disc_hom, disc_palin, disc_notPossPair, disc_notExpA2 from ch_describe_allele_filter
      
      output:
      tuple datasetID, file("desc_gb_filt_remove_by_allele_filter.txt") into ch_desc_allele_filter_removed
  
      script:
      """

      # prepare process specific descriptive statistics
      echo -e "rowsremoved\tfiltertype" > desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_notGCTA)" | awk -vOFS="\t" '{print \$1, "notAnyOfATGC"}' >> desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_indel)" | awk -vOFS="\t" '{print \$1, "indels"}' >> desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_hom)" | awk -vOFS="\t" '{print \$1, "homozygotes"}' >> desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_palin)" | awk -vOFS="\t" '{print \$1, "palindromes"}' >> desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_notPossPair)" | awk -vOFS="\t" '{print \$1, "notPossiblePairs"}' >> desc_gb_filt_remove_by_allele_filter.txt
      echo "\$(wc -l $disc_notExpA2)" | awk -vOFS="\t" '{print \$1, "notExpectedOtherAllele"}' >> desc_gb_filt_remove_by_allele_filter.txt
      """
  }
  
  
  process filter_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfile from ch_stats_inference
      
      output:
      tuple datasetID, file("st_filtered_remains") into ch_stats_filtered_remain
      tuple datasetID, file("st_filtered_removed")  into ch_stats_filtered_removed
      tuple datasetID, file("desc_filtered_stat_rows_with_non_numbers_BA.txt")  into ch_desc_filtered_stat_rows_with_non_numbers_BA
  
      script:
      """
      filter_stat_values.sh $mfile $sfile > st_filtered_remains 2> st_filtered_removed
      
      #process before and after stats
      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
      rowsAfter="\$(wc -l st_filtered_remains | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered out rows with stats impossible to do calculations from" > desc_filtered_stat_rows_with_non_numbers_BA.txt
      """
  }
  
  ch_stats_filtered_remain
    .into { ch_stats_filtered_remain1; ch_stats_filtered_remain2}

  ch_stats_filtered_remain1
    .combine(ch_mfile_ok5, by: 0)
    .set{ ch_stats_filtered_remain3 }

  process infer_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, st_filtered, mfile from ch_stats_filtered_remain3
      
      output:
      tuple datasetID, mfile, file("st_inferred_stats") into ch_stats_selection
      file("st_which_to_do") into out_st_which_to_do
      tuple datasetID, file("desc_inferred_stats_if_inferred_BA.txt") into ch_desc_inferred_stats_if_inferred_BA
  
      script:
      """
      check_stat_inference.sh $mfile > st_which_to_do
  
      if [ -s st_which_to_do ]; then
        if grep -q "Z_fr_OR_P" st_which_to_do; then
  
          Px="\$(grep "^col_P=" $mfile)"
          P="\$(echo "\${Px#*=}")"
  
          echo -e "QNORM" > prepared_qnorm_vals
          cat $st_filtered | sstools-utils ad-hoc-do -f - -k "\${P}" -n"\${P}" | awk 'NR>1{print \$1/2}' | /home/projects/ip_10000/IBP_pipelines/cleansumstats/cleansumstats_v1.0.0-alpha/cleansumstats_images/2020-04-11-ubuntu-1804_stat_r_in_c.simg stat_r_in_c qnorm >> prepared_qnorm_vals
          cut -f 1 $st_filtered | paste - prepared_qnorm_vals > prepared_qnorm_vals2
          LC_ALL=C join -1 1 -2 1 -t "\$(printf '\t')" $st_filtered prepared_qnorm_vals2 > st_filtered2
  
          nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
          nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
          cat st_filtered2 | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
  
        else
          nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
          nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
          cat $st_filtered | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
        fi
      else
        touch st_inferred_stats
      fi
      
      #process before and after stats
      rowsBefore="\$(wc -l ${st_filtered} | awk '{print \$1}')"
      rowsAfter="\$(wc -l st_inferred_stats | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tInferred stats, if stats are inferred" > desc_inferred_stats_if_inferred_BA.txt
      """
  }
  
  ch_stats_selection
    .combine(ch_sfile_on_stream3, by: 0)
    .set{ ch_stats_selection2 }
  
  process select_stats {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, inferred, sfile from ch_stats_selection2
      
      output:
      tuple datasetID, file("st_stats_for_output") into ch_stats_for_output
      tuple datasetID, file("desc_from_inferred_to_joined_selection_BA.txt") into ch_desc_from_inferred_to_joined_selection_BA
      tuple datasetID, file("desc_from_sumstats_to_joined_selection_BA.txt") into ch_desc_from_sumstats_to_joined_selection_BA
  
      script:
      """
      select_stats_for_output.sh $mfile $sfile $inferred > st_stats_for_output 
      
      #process before and after stats
      rowsBefore="\$(wc -l ${inferred} | awk '{print \$1}')"
      rowsAfter="\$(wc -l st_stats_for_output | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFrom inferred to joined selection of stats" > desc_from_inferred_to_joined_selection_BA.txt
      
      #process before and after stats
      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
      rowsAfter="\$(wc -l st_stats_for_output | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFrom raw sumstat to joined selection of stats" > desc_from_sumstats_to_joined_selection_BA.txt
      """
  }


  ch_stats_filtered_remain2
    .combine(ch_stats_filtered_removed, by: 0)
    .set{ ch_combined_desc_material }

  process describe_rows_formatted_or_filtered {
  
      //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, filtered_stats_remain, filtered_stats_removed from ch_combined_desc_material
      
      output:
      tuple datasetID, file("desc_st_filt_remain.txt") into ch_extra_st_filt_remain
      tuple datasetID, file("desc_st_filt_removed.txt") into ch_extra_st_filt_removed
  
      script:
      """
      # prepare process specific descriptive statistics
      echo "\$(wc -l $filtered_stats_remain)" | awk '{print \$1}' > desc_st_filt_remain.txt
      cat $filtered_stats_removed | awk '{ seen[\$2] += 1 } END { for (i in seen) print i, seen[i] }' > desc_st_filt_removed.txt
      
      """
  }

  
  
  ch_allele_corrected_mix1
    .combine(ch_stats_for_output, by: 0)
    .combine(ch_mapped_GRCh38, by: 0)
    .set{ ch_allele_corrected_and_outstats }
  
  process final_assembly {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, acorrected, stats, grch38 from ch_allele_corrected_and_outstats
      
      output:
      tuple datasetID, build, file("${datasetID}_${build}_cleaned"), file("${datasetID}_GRCh38") into ch_cleaned_file
      tuple datasetID, file("desc_final_merge_BA.txt") into ch_desc_final_merge_BA
  
      script:
      """
      
      apply_modifier_on_stats.sh $acorrected $stats > ${datasetID}_${build}_cleaned
      
      # match the GRCh38 build and publish it as separate file
      echo -e "0\tCHR\tPOS" > ${datasetID}_GRCh38
      awk -vOFS="\t" 'NR>1{split(\$2,out,":"); print \$1, out[1], out[2]}' $grch38 | LC_ALL=C sort -k 1 - > sorted_GRCh38
      LC_ALL=C join -t "\$(printf '\t')" -1 1 -2 1 -o 1.1 1.2 1.3 sorted_GRCh38 ${datasetID}_${build}_cleaned > ${datasetID}_GRCh38
      
      # process before and after stats
      rowsBefore="\$(wc -l $acorrected | awk '{print \$1}')"
      rowsAfter="\$(wc -l ${datasetID}_${build}_cleaned | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFrom dbsnp mapped to merged selection of stats, final step" > desc_final_merge_BA.txt
      """
  }

  ch_cleaned_file
    .combine(ch_input_sfile, by: 0)
    .combine(ch_sfile_on_stream5, by: 0)
    .set{ ch_to_write_to_filelibrary2 }

  process gzip_outfiles {

      input:
      tuple datasetID, build, sclean, scleanGRCh38, inputsfile, inputformatted from ch_to_write_to_filelibrary2

      output:
      tuple datasetID, path("sclean.gz"), path("scleanGRCh38.gz"), inputsfile, path("raw_formatted_rowindexed.gz"), path("cleanedheader") into ch_to_write_to_filelibrary3
      val datasetID into ch_check_avail

      script:
      """
      # Make a header file to use when deciding on what cols are present for the new meta file
      head -n1 ${sclean} > cleanedheader

      # Store data in library
      gzip -c ${sclean} > sclean.gz
      gzip -c ${scleanGRCh38} > scleanGRCh38.gz
      gzip -c ${inputformatted} > raw_formatted_rowindexed.gz
      """
  }
  

  //Do actual collection, placed in corresponding step order
  ch_desc_prep_force_tab_sep_BA
   .combine(ch_desc_prep_add_sorted_rowindex_BA, by: 0)
   .combine(ch_desc_prep_for_dbsnp_mapping_BA, by: 0)
   .combine(ch_desc_liftover_to_GRCh37_and_GRCh38_and_map_to_dbsnp_BA, by: 0)
   .combine(ch_desc_removed_duplicated_rows_GRCh37_BA, by: 0)
   .combine(ch_desc_removed_duplicated_rows_GRCh38_BA, by: 0)
   .combine(ch_desc_removed_duplicated_rows_GRCh37_HARD_BA, by: 0)
   .combine(ch_desc_removed_duplicated_rows_GRCh38_HARD_BA, by: 0)
   .combine(ch_desc_removed_multiallelic_rows_BA, by: 0)
   .combine(ch_desc_keep_a_GRCh38_reference_BA, by: 0)
   .combine(ch_desc_keep_only_GRCh37_version_BA, by: 0)
   .combine(ch_desc_split_multi_allelics_and_sort_on_rowindex_BA, by: 0)
   .combine(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA, by: 0)
   .combine(ch_desc_removed_duplicated_chr_pos_rows_BA, by: 0)
   .combine(ch_desc_filtered_stat_rows_with_non_numbers_BA, by: 0)
   .combine(ch_desc_inferred_stats_if_inferred_BA, by: 0)
   .combine(ch_desc_from_inferred_to_joined_selection_BA, by: 0)
   .combine(ch_desc_from_sumstats_to_joined_selection_BA, by: 0)
   .combine(ch_desc_final_merge_BA, by: 0)
   .set{ ch_collected_workflow_stepwise_stats }

//ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA  - contains all allele filtering substeps

  process collect_and_prepare_stepwise_readme {
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true

      input:
      tuple datasetID, step1, step2, step3, step4, step5, step6, step7, step8, step9, step10a, step10b, step11, step12a, step12b, step12c, step12d, step12e, step12f, step12g, step13, step14, step15, step16a, step16b, step17 from ch_collected_workflow_stepwise_stats

      output:
      tuple datasetID, file("desc_collected_workflow_stepwise_stats.txt") into ch_overview_workflow_steps

      script:
      """
      echo -e "Steps\tBefore\tAfter\tDescription" > desc_collected_workflow_stepwise_stats.txt
      cat $step1 | awk -vFS="\t" -vOFS="\t" '{print "Step1", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step2 | awk -vFS="\t" -vOFS="\t" '{print "Step2", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step3 | awk -vFS="\t" -vOFS="\t" '{print "Step3", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step4 | awk -vFS="\t" -vOFS="\t" '{print "Step4", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step5 | awk -vFS="\t" -vOFS="\t" '{print "Step5", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step6 | awk -vFS="\t" -vOFS="\t" '{print "Step6", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step7 | awk -vFS="\t" -vOFS="\t" '{print "Step7", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step8 | awk -vFS="\t" -vOFS="\t" '{print "Step8", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step9 | awk -vFS="\t" -vOFS="\t" '{print "Step9", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step10a | awk -vFS="\t" -vOFS="\t" '{print "Step10a", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step10b | awk -vFS="\t" -vOFS="\t" '{print "Step10b", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step11 | awk -vFS="\t" -vOFS="\t" '{print "Step11", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12a | awk -vFS="\t" -vOFS="\t" '{print "Step12a", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12b | awk -vFS="\t" -vOFS="\t" '{print "Step12b", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12c | awk -vFS="\t" -vOFS="\t" '{print "Step12c", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12d | awk -vFS="\t" -vOFS="\t" '{print "Step12d", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12e | awk -vFS="\t" -vOFS="\t" '{print "Step12e", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12f | awk -vFS="\t" -vOFS="\t" '{print "Step12f", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step12g | awk -vFS="\t" -vOFS="\t" '{print "Step12g", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step13 | awk -vFS="\t" -vOFS="\t" '{print "Step13", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step14 | awk -vFS="\t" -vOFS="\t" '{print "Step14", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step15 | awk -vFS="\t" -vOFS="\t" '{print "Step15", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step16a | awk -vFS="\t" -vOFS="\t" '{print "Step16a", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step16b | awk -vFS="\t" -vOFS="\t" '{print "Step16b", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      cat $step17 | awk -vFS="\t" -vOFS="\t" '{print "Step17", \$1, \$2, \$3}' >> desc_collected_workflow_stepwise_stats.txt
      """
  }

  ch_preassigned_sumstat_id
   .combine(ch_check_avail, by: 0)
   .set{ ch_assign_sumstat_id }

  process assign_sumstat_id {
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
      publishDir "${params.libdirsumstats}", mode: 'copy', overwrite: false, pattern: 'sumstat_*'

      input:
      tuple datasetID, sumstatname from ch_assign_sumstat_id

      output:
      tuple datasetID, env(libfolder) into ch_assigned_sumstat_id
      file("assigned_sumstat_id")      

      script:
      """
      if [ "${sumstatname}" == "missing" ] ; then
        # Scan for available ID and move directory there
        libfolder="\$(assign_folder_id.sh ${params.libdirsumstats})"
        mkdir "\${libfolder}"
        echo "\${libfolder}" > assigned_sumstat_id 
      else
        libfolder="${sumstatname}"
        mkdir "\${libfolder}"
        echo "${sumstatname}" > assigned_sumstat_id 
      fi
      """
  }

  process check_pdf_library {
      publishDir "${params.libdirpdfs}", mode: 'copy', overwrite: false, pattern: 'pmid_*'
      input:
      tuple datasetID, pmid, pdfpath, pdfsuppdir from ch_input_pdf_stuff

      output:
      tuple datasetID, pmid, pdfpath, pdfsuppdir, env(makePDF), env(makePDFsupp) into ch_input_pdf_stuff2
      path("pmid_*") optional true

      script:
      """
      if [ -f "${params.libdirpdfs}/pmid_${pmid}.pdf" ]
      then
        makePDF="false"
      else
        makePDF="true"
        #will be overwritten in update library step (but here to limit parallell processes to change the same file)
        touch pmid_${pmid}.pdf
      fi

      if [ -d "${params.libdirpdfs}/pmid_${pmid}_supp" ]
      then
        makePDFsupp="false"
      else
        makePDFsupp="true"
        #will be overwritten in update library step
        mkdir pmid_${pmid}_supp
      fi

      """
  }

  process update_pdf_library {
      publishDir "${params.libdirpdfs}", mode: 'copy', overwrite: true, pattern: 'pmid_*'

      input:
      tuple datasetID, pmid, pdfpath, pdfsuppdir, makePDF, makePDFsupp from ch_input_pdf_stuff2

      output:
      tuple datasetID, pmid, pdfpath, pdfsuppdir into ch_input_pdf_stuff3
      path("pmid_*") optional true

      script:
      """
      if [ "${makePDF}" == "true" ]
      then
        cp ${pdfpath} pmid_${pmid}.pdf
      else
        :
      fi
      
      #check supplementary materail folder
      if [ "${makePDFsupp}" == "true" ]
      then
        mkdir pmid_${pmid}_supp
        i=1
        cat ${pdfsuppdir} | while read -r supp; do 
          if [ "\${supp}" != "missing" ]
          then
            supp2="\$(basename "\${supp}")" 
            extension="\${supp2##*.}" 
            cp -r \$supp pmid_${pmid}_supp/pmid_${pmid}_supp_\${i}.\${extension} 
            i=\$((i+1))
          else
            :
          fi
        done
      else
        :
      fi

      """
  }



  ch_assigned_sumstat_id
   .combine(ch_to_write_to_filelibrary3, by: 0)
   .combine(ch_mfile_ok3, by: 0)
   .combine(ch_input_readme, by: 0)
   .combine(ch_input_pdf_stuff3, by: 0)
   .combine(ch_one_line_metafile, by: 0)
   .combine(ch_overview_workflow_steps, by: 0)
   .combine(ch_extra_st_filt_removed, by: 0)
   .combine(ch_desc_allele_filter_removed, by: 0)
   .combine(ch_stats_genome_build, by: 0)
   .combine(ch_software_versions)
   .set{ ch_to_write_to_filelibrary7 }


  process put_in_library {
  
      publishDir "${params.libdirsumstats}/${libfolder}", mode: 'copyNoFollow', overwrite: false, pattern: 'sumstat_*'
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true, pattern: 'libprep_*'

      input:
      tuple datasetID, libfolder, sclean, scleanGRCh38, inputsfile, inputformatted, cleanedheader, mfile, readme, pmid, pdfpath, pdfsuppdir, onelinemeta, overviewworkflow, stfiltremoved, allelefiltremoved, gbdetect, softv from ch_to_write_to_filelibrary7
      
      output:
      path("sumstat_*")
      path("libprep_*") 
      tuple datasetID, libfolder, mfile, file("tmp_onelinemeta") into ch_update_library_info_file
  
      script:
      """
      
      # Restructure output to allow replace or extending some variables, save changes in changes_mfile
      echo "cleansumstats_ID=${libfolder}" > libprep_changes_mfile

      # To make batch updates easier, change the path to raw files to the new name convention, but save old paths for traceability
      echo "path_sumStats=${libfolder}_raw.gz" >> libprep_changes_mfile
      Sx="\$(grep "^path_sumStats=" $mfile)"
      S="\$(echo "\${Sx#*=}")"
      echo "path_original_sumStats=\${S}" >> libprep_changes_mfile

      if [ "${readme}" != "missing" ] ; then
        Rx="\$(grep "^path_readMe=" $mfile)"
        R="\$(echo "\${Rx#*=}")"
        echo "path_readMe=${libfolder}_raw_README.txt" >> libprep_changes_mfile
        echo "path_original_readMe=\${R}" >> libprep_changes_mfile
      else
        echo "path_readMe=missing" >> libprep_changes_mfile
        echo "path_original_readMe=missing" >> libprep_changes_mfile
      fi

      Px="\$(grep "^path_pdf=" $mfile)"
      P="\$(echo "\${Px#*=}")"
      echo "path_pdf=${libfolder}_pmid_${pmid}.pdf" >> libprep_changes_mfile
      echo "path_original_pdf=\${P}" >> libprep_changes_mfile

      #Add cleaned output files
      echo "cleansumstats_cleaned_GRCh37=${libfolder}_cleaned_GRCh37.gz" >> libprep_changes_mfile
      echo "cleansumstats_cleaned_GRCh38_coordinates=${libfolder}_cleaned_GRCh38.gz" >> libprep_changes_mfile

      #Calcualate effective N using meta data info
      sh try_infere_Neffective.sh ${mfile} >> libprep_changes_mfile
      
      # copy the pdf and supplemental material if missing in pdf library
      # and prepare changes for the new mfile
      #cp ${pdfpath} pmid_${pmid}.pdf
      ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf ${libfolder}_pmid_${pmid}.pdf

      mkdir -p ${libfolder}_pmid_${pmid}_supp

      if [ -d "${params.libdirpdfs}/pmid_${pmid}_supp" ]
      then
        #Check dir if is not empty
        count="\$(ls -1 ${params.libdirpdfs}/pmid_${pmid}_supp | wc -l)"
        if [ "\${count}" -gt 0 ]
        then
          # If supplementary is already available, then use those, and do not use new from meta data file
          for fil in ${params.libdirpdfs}/pmid_${pmid}_supp/*
          do 
            supp="\$(basename "\${fil}")" 
            echo "path_pdfSupp=${libfolder}_pmid_${pmid}_supp/${libfolder}_\${supp}" >> libprep_changes_mfile 
            ln -s \${fil} ${libfolder}_pmid_${pmid}_supp/${libfolder}_\${supp}
            #Will be set to same if the one already in library is used (only keep basename)
            echo "path_original_pdfSupp=\${supp}" >> libprep_changes_mfile 
          done
        else
          # If empty then set missing (if supps exist but not in the dedicated library, then it has to be manually inserted there, and will be included in next batch update)
            echo "path_pdfSupp=missing" >> libprep_changes_mfile
            echo "path_original_pdfSupp=missing" >> libprep_changes_mfile
        fi
      else 
        i=1
        cat ${pdfsuppdir} | while read -r supp; do 
          if [ "\${supp}" != "missing" ]
          then
            supp2="\$(basename "\${supp}")" 
            extension="\${supp2##*.}" 
            echo "path_pdfSupp=${libfolder}_pmid_${pmid}_supp/${libfolder}_pmid_${pmid}_supp_\${i}.\${extension}" >> libprep_changes_mfile 
            ln -s ${params.libdirpdfs}/pmid_${pmid}_supp/pmid_${pmid}_supp_\${i}.\${extension} ${libfolder}_pmid_${pmid}_supp/${libfolder}_pmid_${pmid}_supp_\${i}.\${extension}
            echo "path_original_pdfSupp=\$supp2" >> libprep_changes_mfile 
            i=\$((i+1))
          else
            # Keep missing for path_pdfSupp and set missing to path_pdfSupp_original
            echo "path_pdfSupp=missing" >> libprep_changes_mfile
            echo "path_original_pdfSupp=missing" >> libprep_changes_mfile
          fi
        done
      fi
      

      # Apply changes when making the new_mfile
      cat ${mfile} > libprep_raw_mfile
      create_output_meta_data_file.sh libprep_raw_mfile libprep_changes_mfile ${cleanedheader} > libprep_new_mfile
      
      # make one_line_meta data for info file
      create_output_one_line_meta_data_file.sh libprep_new_mfile tmp_onelinemeta "${params.libdirinventory}"

      # Store data in library by moving
      cp ${sclean} ${libfolder}_cleaned_GRCh37.gz
      cp ${scleanGRCh38} ${libfolder}_cleaned_GRCh38.gz
      cp ${inputformatted} ${libfolder}_raw_formatted_rowindexed.gz
      cp $inputsfile ${libfolder}_raw.gz
      if [ "${readme}" != "missing" ] ; then
        cp $readme ${libfolder}_raw_README.txt
      fi
      cp tmp_onelinemeta ${libfolder}_one_line_summary_of_metadata.txt
      cp $softv ${libfolder}_software_versions.csv
      cp libprep_raw_mfile ${libfolder}_raw_meta.txt
      cp libprep_new_mfile ${libfolder}_new_meta.txt

      # Make a folder with detailed data of the cleaning
      mkdir ${libfolder}_cleaning_details
      cp $overviewworkflow ${libfolder}_cleaning_details/${libfolder}_stepwise_overview.txt
      cp ${stfiltremoved} ${libfolder}_cleaning_details/${libfolder}_stat_filter_table_of_removed_types.txt
      cp $allelefiltremoved ${libfolder}_cleaning_details/${libfolder}_allele_filter_table_of_removed_types.txt
      cp $gbdetect ${libfolder}_cleaning_details/${libfolder}_genome_build_map_count_table.txt
      
      # Add link to the pdf and supplemental material
      #ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf ${libfolder}_pmid_${pmid}.pdf
      
      """
  }


  process update_inventory_file {
      publishDir "${params.libdirinventory}", mode: 'copy', overwrite: false

      input:
      tuple datasetID, libfolder, mfile, onelinemeta from ch_update_library_info_file

      output:
      path("*_inventory.txt")

      script:
      """
      # This is a little risky as two parallel flows in theory could enter this process at the same time
      # An idea for the future is to use a simple dbmanager (or use a lock file)
      # Or perhaps use a smarter channeling mixing the different files
     
      # Extract the most recent added row, except the header
      tail -n+2 ${onelinemeta} | head -n1 > oneline
      dateOfCreation="\$(date +%F-%H%M%S-%N)"

      # Select most recent inventory file (if any exists)
      if [ -d "${params.libdirinventory}" ]
      then
        count="\$(ls -1 ${params.libdirinventory} | wc -l)"
        if [ "\${count}" -gt 0 ]
        then
          ls -1 ${params.libdirinventory}/*_inventory.txt | awk '{old=\$1; sub(".*/","",\$1); gsub("-","",\$1); print \$1, old}' | sort -rn -k1.1,1.23 | awk '{print \$2}' > libprep_sorted_inventory_files
          mostrecentfile="\$(head -n1 libprep_sorted_inventory_files)"
          cat \${mostrecentfile} oneline > \${dateOfCreation}_inventory.txt
        else
          # Make header if the file does not exist
          head -n1 ${onelinemeta} > \${dateOfCreation}_inventory.txt
          cat oneline >> \${dateOfCreation}_inventory.txt 
        fi
      else
        # Make header if the file does not exist
        head -n1 ${onelinemeta} > \${dateOfCreation}_inventory.txt
        cat oneline >> \${dateOfCreation}_inventory.txt 
      fi
      """
  }

}

/*
 * Completion e-mail notification
 */
workflow.onComplete {

    // Set up the e-mail variables
    def subject = "[nf-core/cleansumstats] Successful: $workflow.runName"
    if (!workflow.success) {
      subject = "[nf-core/cleansumstats] FAILED: $workflow.runName"
    }
    def email_fields = [:]
    email_fields['version'] = workflow.manifest.version
    email_fields['runName'] = custom_runName ?: workflow.runName
    email_fields['success'] = workflow.success
    email_fields['dateComplete'] = workflow.complete
    email_fields['duration'] = workflow.duration
    email_fields['exitStatus'] = workflow.exitStatus
    email_fields['errorMessage'] = (workflow.errorMessage ?: 'None')
    email_fields['errorReport'] = (workflow.errorReport ?: 'None')
    email_fields['commandLine'] = workflow.commandLine
    email_fields['projectDir'] = workflow.projectDir
    email_fields['summary'] = summary
    email_fields['summary']['Date Started'] = workflow.start
    email_fields['summary']['Date Completed'] = workflow.complete
    email_fields['summary']['Pipeline script file path'] = workflow.scriptFile
    email_fields['summary']['Pipeline script hash ID'] = workflow.scriptId
    if (workflow.repository) email_fields['summary']['Pipeline repository Git URL'] = workflow.repository
    if (workflow.commitId) email_fields['summary']['Pipeline repository Git Commit'] = workflow.commitId
    if (workflow.revision) email_fields['summary']['Pipeline Git branch/tag'] = workflow.revision
    if (workflow.container) email_fields['summary']['Docker image'] = workflow.container
    email_fields['summary']['Nextflow Version'] = workflow.nextflow.version
    email_fields['summary']['Nextflow Build'] = workflow.nextflow.build
    email_fields['summary']['Nextflow Compile Timestamp'] = workflow.nextflow.timestamp

    // Check if we are only sending emails on failure
    email_address = params.email
    if (!params.email && params.email_on_fail && !workflow.success) {
        email_address = params.email_on_fail
    }

    // Render the TXT template
    def engine = new groovy.text.GStringTemplateEngine()
    def tf = new File("$baseDir/assets/email_template.txt")
    def txt_template = engine.createTemplate(tf).make(email_fields)
    def email_txt = txt_template.toString()

    // Render the HTML template
    def hf = new File("$baseDir/assets/email_template.html")
    def html_template = engine.createTemplate(hf).make(email_fields)
    def email_html = html_template.toString()

    // Render the sendmail template
    def smail_fields = [ email: email_address, subject: subject, email_txt: email_txt, email_html: email_html, baseDir: "$baseDir", mqcFile: mqc_report, mqcMaxSize: params.maxMultiqcEmailFileSize.toBytes() ]
    def sf = new File("$baseDir/assets/sendmail_template.txt")
    def sendmail_template = engine.createTemplate(sf).make(smail_fields)
    def sendmail_html = sendmail_template.toString()

    // Send the HTML e-mail
    if (email_address) {
        try {
          if ( params.plaintext_email ){ throw GroovyException('Send plaintext e-mail, not HTML') }
          // Try to send HTML e-mail using sendmail
          [ 'sendmail', '-t' ].execute() << sendmail_html
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (sendmail)"
        } catch (all) {
          // Catch failures and try with plaintext
          [ 'mail', '-s', subject, email_address ].execute() << email_txt
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (mail)"
        }
    }

    // Write summary e-mail HTML to a file
    def output_d = new File( "${params.outdir}/pipeline_info/" )
    if (!output_d.exists()) {
      output_d.mkdirs()
    }
    def output_hf = new File( output_d, "pipeline_report.html" )
    output_hf.withWriter { w -> w << email_html }
    def output_tf = new File( output_d, "pipeline_report.txt" )
    output_tf.withWriter { w -> w << email_txt }

    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_red = params.monochrome_logs ? '' : "\033[0;31m";

    if (workflow.stats.ignoredCount > 0 && workflow.success) {
      log.info "${c_purple}Warning, pipeline completed, but with errored process(es) ${c_reset}"
      log.info "${c_red}Number of ignored errored process(es) : ${workflow.stats.ignoredCount} ${c_reset}"
      log.info "${c_green}Number of successfully ran process(es) : ${workflow.stats.succeedCount} ${c_reset}"
    }

    if (workflow.success) {
        log.info "${c_purple}[nf-core/cleansumstats]${c_green} Pipeline completed successfully${c_reset}"
    } else {
        checkHostname()
        log.info "${c_purple}[nf-core/cleansumstats]${c_red} Pipeline completed with errors${c_reset}"
    }

}


def nfcoreHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
    ${c_blue}        ___     __   __   __   ___     ${c_green}/,-._.--~\'${c_reset}
    ${c_blue}  |\\ | |__  __ /  ` /  \\ |__) |__         ${c_yellow}}  {${c_reset}
    ${c_blue}  | \\| |       \\__, \\__/ |  \\ |___     ${c_green}\\`-._,-`-,${c_reset}
                                            ${c_green}`._,._,\'${c_reset}
    ${c_purple}  nf-core/cleansumstats v${workflow.manifest.version}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
