#!/usr/bin/env nextflow
// -*- mode: groovy; -*-
/*
vim: syntax=groovy

========================================================================================
                         cleansumstats
========================================================================================
cleansumstats Pipeline.
 #### Homepage / Documentation
 https://github.com/BioPsyk/cleansumstats
----------------------------------------------------------------------------------------
*/

nextflow.enable.dsl=2      

pipelineVersion = new File("$projectDir/VERSION").text.trim()

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run cleansumstats --input 'gwas_sumstats_meta_file.txt' -profile singularity

    Mandatory arguments:
      --input                       Path to metadata file in YAML format

    References:                     If not set here, it has to be specified in the configuration file
      --dbsnp_38                    Path to dbsnp GRCh38 reference
      --dbsnp_38_37                 Path to dbsnp GRCh38 to GRCh37 map reference
      --dbsnp_37_38                 Path to dbsnp GRCh37 to GRCh38 map reference
      --dbsnp_36_38                 Path to dbsnp GRCh36 to GRCh38 map reference
      --dbsnp_35_38                 Path to dbsnp GRCh35 to GRCh38 map reference
      --dbsnp_RSID_38               Path to dbsnp RSID to GRCh38 map reference

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Filtering:
      --beforeLiftoverFilter        A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_keys
                                    Example(default): --beforeLiftoverFilter duplicated_keys

      --afterLiftoverFilter         A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_refalt_in_GRCh37
                                      duplicated_chrpos_refalt_in_GRCh38
                                      duplicated_chrpos_in_GRCh37
                                      duplicated_chrpos_in_GRCh38
                                      multiallelics_in_dbsnp
                                    Example(default): --afterLiftoverFilter duplicated_chrpos_refalt_in_GRCh37,duplicated_chrpos_refalt_in_GRCh38,duplicated_chrpos_in_GRCh37,duplicated_chrpos_in_GRCh38,multiallelics_in_dbsnp


      --afterAlleleCorrectionFilter A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_in_GRCh37
                                    Example(default): --afterAlleleCorrectionFilter duplicated_chrpos_in_GRCh37

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

      --generateDbSNPreference      Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline
      --hg38ToHg19chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg18chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg17chain             chain file used for liftover (required for --generateDbSNPreference)

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// checker only
if(params.checkonly){
  doCompleteCleaningWorkflow = false
}else{
  doCompleteCleaningWorkflow = true
}

// check filter
beforeLiftoverFilter = params.beforeLiftoverFilter
afterLiftoverFilter = params.afterLiftoverFilter
afterAlleleCorrectionFilter = params.afterAlleleCorrectionFilter

// Set channels
if (params.generateDbSNPreference) {
  if (params.hg38ToHg19chain) { ch_hg38ToHg19chain = file(params.hg38ToHg19chain, checkIfExists: true) }
  if (params.hg19ToHg18chain) { ch_hg19ToHg18chain = file(params.hg19ToHg18chain, checkIfExists: true) }
  if (params.hg19ToHg17chain) { ch_hg19ToHg17chain = file(params.hg19ToHg17chain, checkIfExists: true) }

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38) }
}else if(params.generate1KgAfSNPreference){

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }

}else {

  if (params.kg1000AFGRCh38) { ch_kg1000AFGRCh38 = file(params.kg1000AFGRCh38, checkIfExists: true) }
  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }
}

ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}



// Header log info
log.info cleansumstatsHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
//if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38
//if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37
//if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36
//if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35
//if (params.dbsnpRSID) summary['dbsnpRSID'] = params.dbsnpRSID

summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
//checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}


process get_software_versions {
    publishDir "${params.outdir}/pipeline_info", mode: 'copy', overwrite: true, pattern: '*.csv'

    output:
    file "software_versions" into ch_software_versions


    script:
    """
    echo $pipelineVersion > v_pipeline.txt
    echo $workflow.nextflow.version > v_nextflow.txt
    sstools-version > v_sumstattools.txt
    echo "placeholder" > software_versions
    """
}

import dk.biopsyk.PipelineSession

def sess = new PipelineSession<Metadata>(
  Metadata.class,
  baseDir,
  workflow.workDir,
  params.input
)


include { prepare_dbsnp_reference } from './modules/subworkflow/prepare_dbsnp.nf' 
include { prepare_1kgaf_reference } from './modules/subworkflow/prepare_1kgaf.nf' 

include { main_init_checks_crucial_paths } from './modules/subworkflow/main_init_checks_crucial_paths.nf' 
include {
  calculate_checksum_on_metafile_input
  make_metafile_unix_friendly
  calculate_checksum_on_sumstat_input
  check_sumstat_format
  add_sorted_rowindex_to_sumstat
} from './modules/process/main_init_checks.nf' 

include { map_to_dbsnp } from './modules/subworkflow/map_to_dbsnp.nf' 




workflow {
  main:

  if (params.generateMetafile){
    sess.metadata_paths.each {
      log.info "Writing metadata template"
  
      def metadata_id = it.getBaseName().toString()
  
      def template_file = new File("${params.outdir}/${metadata_id}.template.yaml")
      template_file.write(
        sess.metadata_schema.generate_metadata_template()
      )
  
      log.info "Metadata template written to ${params.outdir}/${metadata_id}.template.yaml"
    }
  }else if(params.generateMetaClass){
    log.info "Metadata class written to ${params.outdir}/Metadata.groovy"
  
    def class_file = new File("${params.outdir}/Metadata.groovy")
    class_file.write(
      sess.metadata_schema.generate_metadata_groovy_class()
    )
  
    log.info "Metadata class written to ${params.outdir}/Metadata.groovy"
  }else if(params.generateDbSNPreference){
    prepare_dbsnp_reference(
      "${params.input}", 
      ch_hg38ToHg19chain, 
      ch_hg19ToHg18chain, 
      ch_hg19ToHg17chain,
      ch_dbsnp_RSID_38,
      ch_dbsnp_38,
      ch_dbsnp_38_37,
      ch_dbsnp_37_38,
      ch_dbsnp_36_38,
      ch_dbsnp_35_38

    )
  }else if(params.generate1KgAfSNPreference){
    prepare_1kgaf_reference("${params.input}", ch_dbsnp_38)
    //#check for not agreeing ref alleles and alt alleles
    // awk '{if($2!=$10){print $0}}' 1kg_af_ref.sorted.joined | head
    // awk '{if($3!=$11){print $0}}' 1kg_af_ref.sorted.joined | head
  }else {
  
    //=================================================================================
    // Pre-execution validation
    //=================================================================================
  
    log.info("Reading metadata files")
  
    sess.read_metadata_files()
  
    log.info("All metadata files read")
    log.info("Validating pipeline parameters")
  
    ParametersValidator.validate_filters_allowed(
      "before",
      params.beforeLiftoverFilter,
      "${baseDir}/assets/allowed_names_beforeLiftoverFilter.txt"
    )
  
    ParametersValidator.validate_filters_allowed(
      "after",
      params.afterLiftoverFilter,
      "${baseDir}/assets/allowed_names_afterLiftoverFilter.txt"
    )
  
    log.info("All pipeline parameters validated")
  }

  //=================================================================================
  // Start of execution
  //=================================================================================

  Channel
    .fromPath("${params.input}", type: 'file')
    .map { file -> tuple(file.baseName, file) }
    .set { ch_mfile_checkX } 

  //ch_mfile_checkX.into { ch_mfile_user_1; ch_mfile_user_2; ch_mfile_user_3; }
  main_init_checks_crucial_paths(ch_mfile_checkX, sess)
  calculate_checksum_on_metafile_input(ch_mfile_checkX)
  calculate_checksum_on_sumstat_input(main_init_checks_crucial_paths.out.spath)
  make_metafile_unix_friendly(ch_mfile_checkX)
  check_sumstat_format(main_init_checks_crucial_paths.out.mfile_check_format)
  add_sorted_rowindex_to_sumstat(main_init_checks_crucial_paths.out.spath)
//    ch_mfile_ok.into { ch_mfile_ok1; ch_mfile_ok2; ch_mfile_ok3; ch_mfile_ok4; ch_mfile_ok5; ch_mfile_rerun_7; ch_mfile_ok_8}
//
//    ch_sfile_on_stream.into { ch_sfile_on_stream1; ch_sfile_on_stream2; ch_sfile_on_stream4; ch_sfile_on_stream5; ch_before_liftover }
//    ch_mfile_and_stream=ch_mfile_ok1.join(ch_sfile_on_stream1)
//    ch_mfile_and_stream.into { ch_check_gb; ch_liftover; ch_liftover1; ch_liftover2; ch_stats_inference }

  if (doCompleteCleaningWorkflow){
    map_to_dbsnp(main_init_checks_crucial_paths.out.mfile_check_format, sess)
  }
}

//
//
//
//
//
//    ch_allele_correction_combine=ch_allele_correction.join(ch_sfile_on_stream2, by: 0)
//    ch_allele_correction_combine.into{ ch_allele_correction_combine1; ch_allele_correction_combine2 }
//
//    process does_exist_A2 {
//
//        input:
//        tuple datasetID, mfile from ch_mfile_ok2
//
//        output:
//        tuple datasetID, A2exists into ch_present_A2
//
//        script:
//        def metadata = session.get_metadata(datasetID)
//        A2exists="${metadata.col_OtherAllele ? true : false}"
//        """
//        echo ${A2exists} > A2exists
//        """
//    }
//
//    //Create filter for when A2 exists or not
//    ch_present_A2_br=ch_present_A2.branch { key, value ->
//                    A2exists: value == "true"
//                    A2missing: value == "false"
//                    }
//
//    //split the channels based on filter
//    ch_present_A2_br2=ch_present_A2_br.A2exists
//    ch_present_A2_br3=ch_present_A2_br.A2missing
//
//    //combine each channel with the matching datasetID
//    ch_A2_exists=ch_allele_correction_combine1.join(ch_present_A2_br2, by: 0)
//    ch_A2_missing=ch_allele_correction_combine2.join(ch_present_A2_br3, by: 0)
//
//    process allele_correction_A1_A2 {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//        publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev
//
//        input:
//        tuple datasetID, build, mfile, mapped, sfile, A2exists from ch_A2_exists
//
//        output:
//        tuple datasetID, build, mfile, file("allele_correction_A1_A2__acorrected") into ch_A2_exists2
//        tuple datasetID, file("allele_correction_A1_A2__removed_allele_filter_ix") into ch_removed_by_allele_filter_ix1
//        tuple datasetID, file("allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
//
//        script:
//        def metadata = session.get_metadata(datasetID)
//
//        """
//
//        colEff="${metadata.col_EffectAllele ?: "missing"}"
//        colAlt="${metadata.col_OtherAllele ?: "missing"}"
//        map_to_adhoc_function.sh ${ch_regexp_lexicon} ${sfile} "effallele" "\${colEff}" > adhoc_func1
//        map_to_adhoc_function.sh ${ch_regexp_lexicon} ${sfile} "altallele" "\${colAlt}" > adhoc_func2
//        colA1="\$(cat adhoc_func1)"
//        colA2="\$(cat adhoc_func2)"
//
//        allele_correction.sh ${sfile} ${mapped} "\${colA1}" "\${colA2}" allele_correction_A1_A2__acorrected allele_correction_A1_A2__removed_allele_filter_ix
//
//        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
//        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
//        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes. Usually a substantial amount." >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l allele_correction_A1_A2__acorrected | awk '{print \$1-1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tAllele corretion sanity check that final filtered file before and after file have same row count" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
//        """
//    }
//
//    process allele_correction_A1 {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//        publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev
//
//        input:
//        tuple datasetID, build, mfile, mapped, sfile, A2missing from ch_A2_missing
//
//        output:
//        tuple datasetID, build, mfile, file("allele_correction_A1__acorrected") into ch_A2_missing2
//        tuple datasetID, file("allele_correction_A1__removed_allele_filter_ix") into ch_removed_by_allele_filter_ix2
//        tuple datasetID, file("allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA
//        script:
//        def metadata = session.get_metadata(datasetID)
//        """
//        colEff="${metadata.col_EffectAllele ?: "missing"}"
//        map_to_adhoc_function.sh ${ch_regexp_lexicon} ${sfile} "effallele" "\${colEff}" > adhoc_func1
//        colA1="\$(cat adhoc_func1)"
//
//        #NOTE to use A1 allele only complicates the filtering on possible pairs etc, so we always need a multiallelic filter in how the filter works right now.
//        # This is something we should try to accomodate to, so that it is not required.
//        multiallelic_filter.sh $mapped > allele_correction_A1__multifiltered
//
//        allele_correction_onlyA1.sh ${sfile} allele_correction_A1__multifiltered "\${colA1}" allele_correction_A1__acorrected allele_correction_A1__removed_allele_filter_ix
//        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
//        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
//        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//        rowsBefore="\${rowsAfter}"
//        rowsAfter="\$(wc -l allele_correction_A1__acorrected | awk '{print \$1-1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tsanity sanity check that final filtered file before and after file have same row count" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference
//
//
//        """
//    }
//
//    //put the two branches into the same channel (as only one will be used per file, there will be no duplicates)
//    ch_removed_by_allele_filter_ix1
//      .mix(ch_removed_by_allele_filter_ix2)
//      .set{ ch_removed_by_allele_filter_ix }
//
//
//    ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
//      .mix(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA)
//      .set{ ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA }
//
//
//    //mix the A1_A2_both and A1_solo channels
//    ch_A2_exists2
//      .mix(ch_A2_missing2)
//      .set{ ch_allele_corrected_mix_X }
//
//    process rm_dup_chrpos_rows_after_acor {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, build, mfile, acorrected from ch_allele_corrected_mix_X
//
//        output:
//        tuple datasetID, build, mfile, file("rm_dup_chrpos_rows_after_acor__ac_unique_rows_sorted") into ch_allele_corrected_mix_Y
//        tuple datasetID, file("rm_dup_chrpos_rows_after_acor__desc_removed_duplicated_rows") into ch_desc_removed_duplicated_chr_pos_rows_BA
//        //file("ac_*")
//        //file("afterAlleleCorrection_executionorder")
//        //file("removed_*")
//
//        script:
//        """
//
//        #Can be used as a sanitycheck-filter to discover potential misbehaviour
//        # Added white space after "afterAlleleCorrectionFilter" as if it is empty, then the third argument will go one step forward
//        filter_after_allele_correction.sh ${acorrected} "${afterAlleleCorrectionFilter} " "rm_dup_chrpos_rows_after_acor__"
//
//        """
//    }
//    ch_allele_corrected_mix_Y
//      .into{ ch_allele_corrected_mix1; ch_allele_corrected_mix2; ch_allele_corrected_mix3 }
//
//    process numeric_filter_stats {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//      publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev
//
//      input:
//      tuple datasetID, mfile, sfile from ch_stats_inference
//
//      output:
//      tuple datasetID, mfile, file("numeric_filter_stats__st_filtered_remains") into ch_stats_filtered_remain00
//      tuple datasetID, file("numeric_filter_stats__removed_stat_non_numeric_in_awk_ix") into ch_stats_filtered_removed_ix
//      tuple datasetID, file("numeric_filter_stats__desc_filtered_stat_rows_with_non_numbers_BA.txt") into ch_desc_filtered_stat_rows_with_non_numbers_BA
//      file("numeric_filter_stats__removed_stat_non_numeric_in_awk")
//
//      script:
//      def metadata = session.get_metadata(datasetID)
//      Map stat_fields = metadata.resolve_stat_fields()
//      int se_column_id = -1
//
//      stat_fields.eachWithIndex { entry, i ->
//        if (entry.key == "SE") {
//          se_column_id = i
//        }
//      }
//
//      """
//      if [[ \$(wc -l $sfile | awk '{print \$1}') == "1" ]]
//      then
//        echo "[ERROR] The inputted file sfile did not have any data"
//        exit 1
//      fi
//
//      touch numeric_filter_stats__removed_stat_non_numeric_in_awk
//      touch numeric_filter_stats__removed_stat_non_numeric_in_awk_ix
//
//      sstools-utils ad-hoc-do -f $sfile \
//        -k "0|${stat_fields.values().join("|")}" \
//        -n "0,${stat_fields.values().join(",")}" | \
//        filter_stat_values_awk.sh -vzeroSE="${se_column_id}" \
//          > numeric_filter_stats__st_filtered_remains 2> numeric_filter_stats__removed_stat_non_numeric_in_awk
//
//      awk -vOFS="\t" '{print \$1,"stat_non_numeric_in_awk"}' numeric_filter_stats__removed_stat_non_numeric_in_awk > numeric_filter_stats__removed_stat_non_numeric_in_awk_ix
//
//      if [[ \$(wc -l numeric_filter_stats__st_filtered_remains | awk '{print \$1}') == "1" ]]
//      then
//        echo "[ERROR] The outputted file st_filtered_remains did not have any data"
//        exit 1
//      fi
//
//      #process before and after stats
//      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
//      rowsAfter="\$(wc -l numeric_filter_stats__st_filtered_remains | awk '{print \$1}')"
//      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered out rows with stats impossible to do calculations from" > numeric_filter_stats__desc_filtered_stat_rows_with_non_numbers_BA.txt
//      """
//    }
//
//    process convert_neglogP {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, mfile, sfile from ch_stats_filtered_remain00
//
//        output:
//        tuple datasetID, mfile, file("convert_neglogP") into ch_convert_neglog10P
//        tuple datasetID, file("convert_neglogP_BA.txt") into ch_desc_convert_neglog10P
//
//        script:
//        def metadata = session.get_metadata(datasetID)
//        """
//        colneglog10P="${metadata.stats_neglog10P ?: "missing"}"
//        colP="${metadata.col_P ?: "missing"}"
//
//        if [ "\${colneglog10P}" == 'true' ]; then
//          convert_neglogP.sh ${sfile} "\${colP}" > convert_neglogP
//        else
//          cp ${sfile} convert_neglogP
//        fi
//
//        #process before and after stats
//        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
//        rowsAfter="\$(wc -l convert_neglogP | awk '{print \$1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tneglog10 Pvalue fix" > convert_neglogP_BA.txt
//        """
//    }
//
//    process force_eaf {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, mfile, sfile from ch_convert_neglog10P
//
//        output:
//        tuple datasetID, file("force_eaf__st_forced_eaf") into ch_stats_filtered_remain
//        tuple datasetID, file("force_eaf__desc_forced_eaf_BA.txt") into ch_desc_forced_eaf_BA
//
//        script:
//        """
//        if [[ \$(wc -l $sfile | awk '{print \$1}') == "1" ]]
//        then
//          echo "[ERROR] The inputted file sfile did not have any data"
//          exit 1
//        fi
//
//        force_effect_allele_frequency.sh $mfile $sfile > force_eaf__st_forced_eaf
//
//        if [[ \$(wc -l st_forced_eaf | awk '{print \$1}') == "1" ]]
//        then
//          echo "[ERROR] The outputted file st_forced_eaf did not have any data"
//          exit 1
//        fi
//
//        #process before and after stats
//        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
//        rowsAfter="\$(wc -l force_eaf__st_forced_eaf | awk '{print \$1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tForced Effect Allele Frequency" > force_eaf__desc_forced_eaf_BA.txt
//        """
//    }
//
//    ch_stats_filtered_remain
//      .join(ch_mfile_ok5, by: 0)
//      .set{ ch_stats_filtered_remain3 }
//
//
//
//
//    process prep_af_stats {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, build, mfile, sfile from ch_allele_corrected_mix3
//
//        output:
//        tuple datasetID, env(avail), file("prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx") into ch_prep_ref_allele_frequency
//        file("prep_af_stats__st_1kg_af_ref_sorted")
//        file("prep_af_stats__st_1kg_af_ref_sorted_joined")
//
//        script:
//        """
//        # Check if the ancestry is one of the ones we have frequencies for (EAS, EUR, AFR, AMR, SAS)
//        Ax="\$(grep "^study_Ancestry:" $mfile)"
//        A="\$(echo "\${Ax#*: }")"
//
//        avail="false"
//        count=0
//        # Important that this order is the same as in the allele frequency file
//        for anc in EAS EUR AFR AMR SAS; do
//          if [ "\${anc}" == "\${A}" ]; then
//            avail="true"
//          fi
//          count=\$((count+1))
//        done
//
//        # If we have an available ancestry reference frequency
//        if [ \${avail} == "true" ]; then
//          # Join with AF table using chrpos column (keep only rowindex and allele frequency, merge later)
//          awk -vFS="\t" -vOFS="\t" '{print \$4"-"\$2"-"\$3, \$1}' ${sfile} | LC_ALL=C sort -k 1,1 -t "\$(printf '\t')" > prep_af_stats__st_1kg_af_ref_sorted
//          awk -vFS=" " -vOFS="\t" -vcount=\${count} '{print \$1"-"\$2"-"\$3,\$count}' ${ch_kg1000AFGRCh38} | LC_ALL=C sort -k 1,1 -t "\$(printf '\t')"| LC_ALL=C join -1 1 -2 1 -t "\$(printf '\t')" -o 2.2 1.2 - prep_af_stats__st_1kg_af_ref_sorted > prep_af_stats__st_1kg_af_ref_sorted_joined
//          echo -e "0\tAF_1KG_CS" > prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
//          LC_ALL=C sort -k 1,1 prep_af_stats__st_1kg_af_ref_sorted_joined >> prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
//        else
//          touch prep_af_stats__st_1kg_af_ref_sorted
//          touch prep_af_stats__st_1kg_af_ref_sorted_joined
//          touch prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
//        fi
//
//        """
//    }
//
//    ch_stats_filtered_remain3
//    .join(ch_prep_ref_allele_frequency, by: 0)
//    .set { ch_add_ref_freq }
//
//
//    //if ancestry code (eg EUR)is available, add allele_frequency
//    process add_af_stats {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, st_filtered, mfile, availAF, afFreqs from ch_add_ref_freq
//
//        output:
//        tuple datasetID, val("g1kaf_stats_branch"), file("add_af_stats__st_added_1kg_ref"), mfile into ch_added_ref_allele_frequency_kg
//        tuple datasetID, val("default_stats_branch"), file("add_af_stats__st_added_1kg_ref"), mfile into ch_added_ref_allele_frequency_default
//
//        script:
//        """
//        if [[ \$(wc -l $st_filtered | awk '{print \$1}') == "1" ]]
//        then
//          echo "[ERROR] The inputted file st_filtered did not have any data"
//          exit 1
//        fi
//
//        # If we have an available ancestry reference frequency
//        if [ "${availAF}" == "true" ]; then
//          # Join with AF table using chrpos column add NA for missing fields
//         LC_ALL=C join -e "NA" -t "\$(printf '\t')" -a 1 -1 1 -2 1 -o auto ${st_filtered} ${afFreqs} > add_af_stats__st_added_1kg_ref
//        else
//          head -n1 ${st_filtered} > add_af_stats__st_added_1kg_ref
//        fi
//        """
//    }
//    //re-merge these stats in the select_stats process
//    ch_added_ref_allele_frequency_default.into{ ch_added_ref_allele_frequency_default1; ch_added_ref_allele_frequency_default2 }
//
//    //mix and run inference for 1kg-AF version and for a version without
//    ch_added_ref_allele_frequency_kg
//      .mix(ch_added_ref_allele_frequency_default1)
//      .set{ ch_stats_to_infer }
//
//    process infer_stats {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates/${af_branch}", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, af_branch, st_filtered, mfile from ch_stats_to_infer
//
//        output:
//        tuple datasetID, af_branch, mfile, file("infer_stats__st_inferred_stats") into ch_stats_selection
//        tuple datasetID, file("infer_stats__desc_inferred_stats_if_inferred_BA.txt") into ch_desc_inferred_stats_if_inferred_BA
//        file("st_which_to_infer")
//        file("colfields")
//        file("colnames")
//        file("colpositions")
//
//        script:
//        """
//        if [[ \$(wc -l $st_filtered | awk '{print \$1}') == "1" ]]
//        then
//          echo "[ERROR] The inputted file st_filtered did not have any data"
//          exit 1
//        fi
//
//        check_stat_inference_functionfile.sh ${mfile} $af_branch > st_which_to_infer
//        check_stat_inference_avail.sh $mfile colfields colnames colpositions $af_branch
//
//        cf="\$(cat colfields)"
//        cn="\$(cat colnames)"
//        cp="\$(cat colpositions)"
//
//        if [ -s st_which_to_infer ]; then
//
//          Sx="\$(grep "^stats_Model:" ${mfile})"
//          STATM="\${Sx#*: }"
//
//          if [ "\${STATM}" == "linear" ]; then
//            STATM2="lin"
//          elif [ "\${STATM}" == "logistic" ]; then
//            STATM2="log"
//          else
//            echo "Requires a statmodel defined in metafile"
//          fi
//
//          thisdir="\$(pwd)"
//
//          cat $st_filtered | sstools-utils ad-hoc-do -f - -k "\${cf}" -n"\${cn}" | r-stats-c-streamer --functionfile st_which_to_infer --skiplines 1 \${cp} --statmodel \${STATM2} --allelefreqswitch > infer_stats__st_inferred_stats
//
//        else
//          touch infer_stats__st_inferred_stats
//        fi
//
//        #process before and after stats
//        rowsBefore="\$(wc -l ${st_filtered} | awk '{print \$1}')"
//        rowsAfter="\$(wc -l infer_stats__st_inferred_stats | awk '{print \$1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tInferred stats, if stats are inferred" > infer_stats__desc_inferred_stats_if_inferred_BA.txt
//        """
//    }
//
//
//    //branch the stats_genome_build
//    ch_stats_selection_filter=ch_stats_selection.branch { key, value, file1, file2 ->
//                    g1kaf_stats_branch: value == "g1kaf_stats_branch"
//                    default_stats_branch: value == "default_stats_branch"
//                    }
//    g1kaf_stats_branch=ch_stats_selection_filter.g1kaf_stats_branch
//    default_stats_branch=ch_stats_selection_filter.default_stats_branch
//
//    //combine the 1kg af branch and default branch for inferred information
//    g1kaf_stats_branch
//      .join(default_stats_branch, by: 0)
//      .map { key, val, mfile, file, val2, mfile2, file2 -> tuple(key, mfile, file, file2) }
//      .set{ ch_inferred_stats_combined }
//
//    process merge_inferred_data {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//      input:
//      tuple datasetID, mfile, kgversion, defaultversion from ch_inferred_stats_combined
//
//      output:
//      tuple datasetID, file("merge_inferred_data__st_combined_set_of_inferred_data") into ch_combined_set_of_inferred_data
//      file("merge_inferred_data__st_added_suffix")
//
//      script:
//      """
//      # Add _1KG to all 1KG inferred variables
//      add_suffix_to_colnames.sh $kgversion "_1KG" > merge_inferred_data__st_added_suffix
//
//      # Merge the data add NA for missing fields
//      LC_ALL=C join -e "NA" -t "\$(printf '\t')" -a 1 -1 1 -2 1 -o auto $defaultversion merge_inferred_data__st_added_suffix > merge_inferred_data__st_combined_set_of_inferred_data
//      """
//
//    }
//
//
//    //ch_stats_selection_only_contains_inferred_variables
//    ch_combined_set_of_inferred_data
//      .join(ch_added_ref_allele_frequency_default2, by: 0)
//      .set{ ch_stats_selection2 }
//
//    process select_stats_for_output {
//
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, inferred, stats_branch, sfile, mfile from ch_stats_selection2
//
//        output:
//        tuple datasetID, file("select_stats__st_stats_for_output") into ch_stats_for_output
//        tuple datasetID, file("select_stats__selected_source.txt") into ch_stats_for_output_selected_source
//        tuple datasetID, file("select_stats__desc_from_inferred_to_joined_selection_BA.txt") into ch_desc_from_inferred_to_joined_selection_BA
//        tuple datasetID, file("select_stats__desc_from_sumstats_to_joined_selection_BA.txt") into ch_desc_from_sumstats_to_joined_selection_BA
//
//        script:
//        """
//        select_stats_for_output.sh $mfile $sfile $inferred select_stats__selected_source.txt > select_stats__st_stats_for_output
//
//        #process before and after stats
//        rowsBefore="\$(wc -l ${inferred} | awk '{print \$1}')"
//        rowsAfter="\$(wc -l select_stats__st_stats_for_output | awk '{print \$1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFrom inferred to joined selection of stats" > select_stats__desc_from_inferred_to_joined_selection_BA.txt
//
//        #process before and after stats
//        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
//        rowsAfter="\$(wc -l select_stats__st_stats_for_output | awk '{print \$1}')"
//        echo -e "\$rowsBefore\t\$rowsAfter\tFrom raw sumstat to joined selection of stats" > select_stats__desc_from_sumstats_to_joined_selection_BA.txt
//        """
//    }
//
//
//
//    ch_allele_corrected_mix1
//      .join(ch_stats_for_output, by: 0)
//      .set{ ch_allele_corrected_and_outstats }
//
//  process final_assembly {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//      input:
//      tuple datasetID, build, mfile, acorrected, stats from ch_allele_corrected_and_outstats
//
//      output:
//      tuple datasetID, file("final_assembly__cleaned2"), file("final_assembly__header") into ch_cleaned_file_1
//      tuple datasetID, file("final_assembly__desc_final_merge_BA.txt") into ch_desc_final_merge_BA
//      path("final_assembly__cleaned")
//
//      script:
//      """
//      apply_modifier_on_stats.sh $acorrected $stats > final_assembly__cleaned
//
//      #sort on chrpos (which will make header not on top, so lift that out, and prepare order for next process)
//      head -n1 final_assembly__cleaned | awk -vFS="\t" -vOFS="\t" '{printf "%s%s%s%s%s%s", \$2, OFS, \$3, OFS, \$1, OFS; for(i=4; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' > final_assembly__header
//      awk -vFS="\t" -vOFS="\t" 'NR>1{printf "%s%s%s%s", \$2":"\$3, OFS, \$1, OFS; for(i=4; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' final_assembly__cleaned | LC_ALL=C sort -k1,1 > final_assembly__cleaned2
//
//      # process before and after stats
//      rowsBefore="\$(wc -l $acorrected | awk '{print \$1}')"
//      rowsAfter="\$(wc -l final_assembly__cleaned2 | awk '{print \$1}')"
//      echo -e "\$rowsBefore\t\$rowsAfter\tFrom dbsnp mapped to merged selection of stats, final step" > final_assembly__desc_final_merge_BA.txt
//      """
//  }
//
//  process prep_GRCh37_coord {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//      input:
//      tuple datasetID, cleaned_chrpos_sorted, header from ch_cleaned_file_1
//
//      output:
//      tuple datasetID, file("prep_GRCh37_coord__cleaned_chrpos_sorted_header"), file("prep_GRCh37_coord__cleaned_GRCh37") into ch_cleaned_file
//      //file("cleaned_chrpos_sorted")
//      //file("inx_chrpos_GRCh37")
//
//      script:
//
//      """
//      echo -e "CHR\tPOS\tRSID" > prep_GRCh37_coord__cleaned_GRCh37
//      LC_ALL=C join -e "NA" -a1 -1 1 -2 1 -o 2.1 2.2 2.3 ${cleaned_chrpos_sorted} ${ch_dbsnp_38_37} | awk -vOFS="\t" '{split(\$2,out,":"); print out[1], out[2],\$3 }' >> prep_GRCh37_coord__cleaned_GRCh37
//      cat $header > prep_GRCh37_coord__cleaned_chrpos_sorted_header
//      awk -vFS="\t" -vOFS="\t" '{split(\$1,out,":");printf "%s%s%s%s", out[1], OFS, out[2], OFS; for(i=2; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' $cleaned_chrpos_sorted >> prep_GRCh37_coord__cleaned_chrpos_sorted_header
//      """
//
//  }
//
//    //Collect and place in corresponding stepwise order
//    ch_removed_not_possible_to_lift_over_for_combined_set_ix
//     .join(ch_removed_by_allele_filter_ix, by: 0)
//     .join(ch_stats_filtered_removed_ix, by: 0)
//     .set{ ch_collected_removed_lines }
//
//    process collect_rmd_lines {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, step1, step2, step3 from ch_collected_removed_lines
//
//        output:
//        tuple datasetID, file("collect_rmd_lines__removed_lines_collected.txt") into ch_collected_removed_lines2
//
//        script:
//        """
//        echo -e "RowIndex\tExclusionReason" > collect_rmd_lines__removed_lines_collected.txt
//        cat ${step1} ${step2} ${step3} >> collect_rmd_lines__removed_lines_collected.txt
//        """
//    }
//
//    ch_collected_removed_lines2
//      .into { ch_collected_removed_lines3; ch_collected_removed_lines4 }
//
//    process desc_rmd_lines_as_table {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, filtered_stats_removed from ch_collected_removed_lines3
//
//        output:
//        tuple datasetID, file("desc_rmd_lines_as_table__desc_removed_lines_table.txt") into ch_removed_lines_table
//
//        script:
//        """
//        # prepare process specific descriptive statistics
//        echo -e "NrExcludedRows\tExclusionReason" > desc_rmd_lines_as_table__desc_removed_lines_table.txt
//        cat $filtered_stats_removed | tail -n+2 | awk -vOFS="\t" '{ seen[\$2] += 1 } END { for (i in seen) print seen[i],i }' >> desc_rmd_lines_as_table__desc_removed_lines_table.txt
//
//        """
//    }
//
//
//    ch_cleaned_file
//      .join(ch_input_sfile2, by: 0)
//      .join(ch_sfile_on_stream5, by: 0)
//      .join(ch_collected_removed_lines4, by: 0)
//      .set{ ch_to_write_to_filelibrary2 }
//
//    process gzip_outfiles {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, inputsfile, inputformatted, removedlines from ch_to_write_to_filelibrary2
//
//        output:
//        tuple datasetID, path("gzip_outfiles__sclean.gz"), path("gzip_outfiles__scleanGRCh37.gz"), path("gzip_outfiles__removed_lines.gz") into ch_to_write_to_filelibrary3
//        tuple datasetID, path("gzip_outfiles__cleanedheader") into ch_cleaned_header
//        tuple datasetID, inputsfile into ch_to_write_to_raw_library
//        val datasetID into ch_check_avail
//
//        script:
//        """
//        # Make a header file to use when deciding on what cols are present for the new meta file
//        head -n1 ${sclean} > gzip_outfiles__cleanedheader
//
//        # Store data in library
//        gzip -c ${sclean} > gzip_outfiles__sclean.gz
//        gzip -c ${scleanGRCh37} > gzip_outfiles__scleanGRCh37.gz
//        gzip -c ${removedlines} > gzip_outfiles__removed_lines.gz
//        """
//    }
//
//    ch_to_write_to_filelibrary3.into { ch_to_write_to_filelibrary3a; ch_to_write_to_filelibrary3b }
//
//    process calculate_checksum_on_sumstat_cleaned {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, removedlines from ch_to_write_to_filelibrary3a
//
//        output:
//        tuple datasetID, env(scleanchecksum), env(scleanGRCh37checksum), env(removedlineschecksum) into ch_cleaned_sumstat_checksums
//
//        script:
//        """
//        scleanchecksum="\$(b3sum ${sclean} | awk '{print \$1}')"
//        scleanGRCh37checksum="\$(b3sum ${scleanGRCh37} | awk '{print \$1}')"
//        removedlineschecksum="\$(b3sum ${removedlines} | awk '{print \$1}')"
//        """
//    }
//
//  ch_cleaned_sumstat_checksums.into { ch_cleaned_sumstat_checksums1; ch_cleaned_sumstat_checksums2 }
//
//
//  //Do actual collection, placed in corresponding step order
//  ch_desc_prep_force_tab_sep_BA
//   .join(ch_desc_prep_add_sorted_rowindex_BA, by: 0)
//   .join(ch_desc_combined_set_after_liftover, by: 0)
//   .join(ch_desc_removed_duplicates_after_liftover, by: 0)
//   .join(ch_desc_keep_a_GRCh38_reference_BA, by: 0)
//   .join(ch_desc_split_multi_allelics_and_sort_on_rowindex_BA, by: 0)
//   .join(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA, by: 0)
//   .join(ch_desc_removed_duplicated_chr_pos_rows_BA, by: 0)
//   .join(ch_desc_filtered_stat_rows_with_non_numbers_BA, by: 0)
//   .join(ch_desc_inferred_stats_if_inferred_BA, by: 0)
//   .join(ch_desc_from_inferred_to_joined_selection_BA, by: 0)
//   .join(ch_desc_from_sumstats_to_joined_selection_BA, by: 0)
//   .join(ch_desc_final_merge_BA, by: 0)
//   .set{ ch_collected_workflow_stepwise_stats }
//
// //Some that now are part of the branched workflow. Unclear how to save the the stepwise branched workflow before after steps, but all info should be exported in channels.
// //  .combine(ch_desc_sex_chrom_formatting_BA, by: 0)
// //  .combine(ch_desc_prep_for_dbsnp_mapping_BA, by: 0)
// //  .combine(ch_removed_rows_before_liftover, by: 0)
// //  .combine(ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA, by: 0)
//
//  process collect_and_prep_stepwise_readme {
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//      input:
//      tuple datasetID, step1, step2, step3, step4, step5, step6, step7, step8, step9, step10, step11, step12, step13 from ch_collected_workflow_stepwise_stats
//
//      output:
//      tuple datasetID, file("collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt") into ch_overview_workflow_steps
//
//      script:
//      """
//      cat $step1 $step2 $step3 $step4 $step5 $step6 $step7 $step8 $step9 $step10 $step11 $step12 $step13 > all_removed_steps
//
//      echo -e "Steps\tBefore\tAfter\tDescription" > collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt
//      awk -vFS="\t" -vOFS="\t" '{print "Step"NR, \$1, \$2, \$3}' all_removed_steps >> collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt
//
//      """
//  }
//
//    ch_mfile_ok4
//      .join(ch_usermeta_checksum, by: 0)
//      .join(ch_rawsumstat_checksum, by: 0)
//      .join(ch_cleaned_sumstat_checksums2, by: 0)
//      .join(ch_cleaned_header, by: 0)
//      .set { ch_mfile_cleaned_x }
//
//    process prepare_cleaned_metadata_file {
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, mfile, usermetachecksum, rawsumstatchecksum, scleanchecksum, scleanGRCh37checksum, removedlineschecksum, cleanedheader from ch_mfile_cleaned_x
//
//        output:
//        tuple datasetID, path("prepare_cleaned_metadata_file__prepared_cleaned_metafile") into ch_mfile_cleaned_1
//
//        script:
//        """
//
//        #Add cleaned output lines
//        dateOfCreation="\$(date +%F-%H%M)"
//        echo "cleansumstats_date: \${dateOfCreation}" > mfile_additions
//        echo "cleansumstats_user: \$(id -u -n)" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh38: sumstat_cleaned_GRCh38.gz" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh38_checksum: ${scleanchecksum}" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh37_coordinates: sumstat_cleaned_GRCh37.gz" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh37_coordinates_checksum: ${scleanGRCh37checksum}" >> mfile_additions
//        echo "cleansumstats_removed_lines: sumstat_removed_lines.gz" >> mfile_additions
//        echo "cleansumstats_removed_lines_checksum: ${removedlineschecksum}" >> mfile_additions
//        echo "cleansumstats_metafile_user_checksum: ${usermetachecksum}" >> mfile_additions
//        echo "cleansumstats_sumstat_raw_checksum: ${rawsumstatchecksum}" >> mfile_additions
//
//        #Calcualate effective N using meta data info
//        try_infere_Neffective.sh ${mfile} >> mfile_additions
//
//        # Apply additions to make the cleaned meta file ready
//        create_output_meta_data_file_cleaned.sh mfile_additions ${cleanedheader} > prepare_cleaned_metadata_file__prepared_cleaned_metafile
//        """
//    }
//
//
//    // Collect all metafiles in one channel
//     ch_mfile_user_2
//     .join(ch_mfile_cleaned_1, by: 0)
//     .set { ch_all_mfiles }
//
//     ch_to_write_to_filelibrary3b
//      .join(ch_input_readme, by: 0)
//      .join(ch_overview_workflow_steps, by: 0)
//      .join(ch_removed_lines_table, by: 0)
//      .join(ch_gb_stats_combined, by: 0)
//      .join(ch_all_mfiles, by: 0)
//      .join(ch_to_write_to_raw_library, by: 0)
//      .join(ch_input_pdf_stuff, by: 0)
//      .join(ch_stats_for_output_selected_source, by: 0)
//      .set{ ch_to_write_to_filelibrary7 }
//
//     //.combine(ch_collected_removed_lines2)
//
//    process put_in_cleaned_library {
//
//        publishDir "${params.outdir}/${datasetID}", mode: 'copy', overwrite: true
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, removedlines, readme, overviewworkflow, removedlinestable, gbdetectCHRPOS, gbdetectSNPCHRPOS, usermfile, cleanmfile, rawfile, pmid, pdfpath, pdfsuppdir, selected_source from ch_to_write_to_filelibrary7
//
//        output:
//        path("*")
//
//        script:
//
//        """
//        # Store data in library by copying (move is faster, but debug gets slower as input disappears)
//        cp ${sclean} cleaned_GRCh38.gz
//        cp ${scleanGRCh37} cleaned_GRCh37.gz
//        cp ${cleanmfile} cleaned_metadata.yaml
//
//        # Make a folder with detailed data of the cleaning
//        mkdir details
//        cp $overviewworkflow details/stepwise_overview.txt
//        cp ${removedlinestable} details/removed_lines_per_type_table.txt
//        cp $gbdetectCHRPOS details/genome_build_map_count_table_chrpos.txt
//        cp $gbdetectSNPCHRPOS details/genome_build_map_count_table_markername.txt
//        cp ${removedlines} details/removed_lines.gz
//        cp ${selected_source} details/selected_source_stats.txt
//
//
//        # copy all raw stuff into raw
//        mkdir raw
//        cp ${rawfile} raw/.
//        cp ${usermfile} raw/.
//
//        #reference material
//        mkdir raw/reference
//        mkdir raw/reference/supplemental_material
//
//        if [ "${pdfpath}" != "missing" ]
//        then
//          cp ${pdfpath} raw/reference/.
//        fi
//
//        cat ${pdfsuppdir} | while read -r supp; do
//           if [ "\${supp}" != "missing" ]
//           then
//             cp \$supp raw/reference/supplemental_material/.
//           else
//             :
//           fi
//        done
//        """
//    }
//  }
//}

def cleansumstatsHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
                                            ${c_cyan}`._,._,\'${c_reset}
    ${c_purple} cleansumstats v${pipelineVersion}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
