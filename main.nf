#!/usr/bin/env nextflow
/*
========================================================================================
                         nf-core/cleansumstats
========================================================================================
 nf-core/cleansumstats Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/nf-core/cleansumstats
----------------------------------------------------------------------------------------
*/

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run nf-core/cleansumstats --infile 'gwas-sumstats.gz' -profile singularity

    Mandatory arguments:
      --infile                      Path to tab-separated input data (must be surrounded with quotes)
      -profile                      Configuration profile to use. Can use multiple (comma separated)
                                    Available: conda, docker, singularity, awsbatch, test and more.

    References:                     If not specified in the configuration file or you wish to overwrite any of the references. Has to be sorted on chr:pos as first column using LC_ALL=C.
      --dbsnp_38                    Path to dbsnp GRCh38 reference. 
      --dbsnp_38_37                 Path to dbsnp GRCh38 to GRCh37 map reference.
      --dbsnp_37_38                 Path to dbsnp GRCh37 to GRCh38 map reference.
      --dbsnp_36_38                 Path to dbsnp GRCh36 to GRCh38 map reference.
      --dbsnp_35_38                 Path to dbsnp GRCh35 to GRCh38 map reference.
      --dbsnp_RSID_38               Path to dbsnp RSID to GRCh38 map reference.

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Filtering:
      --beforeLiftoverFilter        A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_keys
                                    Example(default): --beforeLiftoverFilter duplicated_keys

      --afterLiftoverFilter         A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_refalt_in_GRCh37
                                      duplicated_chrpos_refalt_in_GRCh38
                                      duplicated_chrpos_in_GRCh37
                                      duplicated_chrpos_in_GRCh38
                                      multiallelics_in_dbsnp
                                    Example(default): --afterLiftoverFilter duplicated_chrpos_refalt_in_GRCh37,duplicated_chrpos_refalt_in_GRCh38,duplicated_chrpos_in_GRCh37,duplicated_chrpos_in_GRCh38,multiallelics_in_dbsnp


      --afterAlleleCorrectionFilter A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_in_GRCh37
                                    Example(default): --afterAlleleCorrectionFilter duplicated_chrpos_in_GRCh37

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

      --generateDbSNPreference      Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline
      --hg38ToHg19chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg18chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg17chain             chain file used for liftover (required for --generateDbSNPreference)

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    AWSBatch options:
      --awsqueue                    The AWSBatch JobQueue that needs to be set when running on AWSBatch
      --awsregion                   The AWS Region for your AWS Batch job to run on
    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// Check if genome exists in the config file
//if (params.genomes && params.genome && !params.genomes.containsKey(params.genome)) {
//    exit 1, "The provided genome '${params.genome}' is not available in the iGenomes file. Currently the available genomes are ${params.genomes.keySet().join(", ")}"
//}

// check filter
beforeLiftoverFilter = params.beforeLiftoverFilter
afterLiftoverFilter = params.afterLiftoverFilter
afterAlleleCorrectionFilter = params.afterAlleleCorrectionFilter

// Set channels
if (params.generateDbSNPreference) {
  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38) }
}else {
  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }
}

ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)

if (params.hg38ToHg19chain) { ch_hg38ToHg19chain = file(params.hg38ToHg19chain, checkIfExists: true) }
if (params.hg19ToHg18chain) { ch_hg19ToHg18chain = file(params.hg19ToHg18chain, checkIfExists: true) }
if (params.hg19ToHg17chain) { ch_hg19ToHg17chain = file(params.hg19ToHg17chain, checkIfExists: true) }


// Stage config files
ch_multiqc_config = file(params.multiqc_config, checkIfExists: true)
ch_output_docs = file("$baseDir/docs/output.md", checkIfExists: true)

//example from nf-core how to use fasta
//params.fasta = params.genome ? params.genomes[ params.genome ].fasta ?: false : false
//if (params.fasta) { ch_fasta = file(params.fasta, checkIfExists: true) }

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}

if ( workflow.profile == 'awsbatch') {
  // AWSBatch sanity checking
  if (!params.awsqueue || !params.awsregion) exit 1, "Specify correct --awsqueue and --awsregion parameters on AWSBatch!"
  // Check outdir paths to be S3 buckets if running on AWSBatch
  // related: https://github.com/nextflow-io/nextflow/issues/813
  if (!params.outdir.startsWith('s3:')) exit 1, "Outdir not on S3 - specify S3 Bucket to run on AWSBatch!"
  // Prevent trace files to be stored on S3 since S3 does not support rolling files.
  if (workflow.tracedir.startsWith('s3:')) exit 1, "Specify a local tracedir or run without trace! S3 cannot be used for tracefiles."
}



// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
//if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38 
//if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37 
//if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36 
//if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35 
//if (params.dbsnpRSID) summary['dbsnpRSID'] = params.dbsnpRSID 

summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
if (params.email || params.email_on_fail) {
  summary['E-mail Address']    = params.email
  summary['E-mail on failure'] = params.email_on_fail
  summary['MultiQC maxsize']   = params.maxMultiqcEmailFileSize
}
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'nf-core-cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}



if (params.generateMetafile){
  ch_metatemplate = file("${baseDir}/assets/meta_data_readMe_v4.txt")
  ch_sumstat_file = Channel
                   .fromPath(params.input, type: 'file')
                   .map { file -> tuple(file.baseName, file) }

  process create_meta_data_template {
  
      publishDir "${params.outdir}", mode: 'copy', overwrite: false
  
      input:
      tuple basefilename, sfilename from ch_sumstat_file
  
      output:
      file("${basefilename}.meta") into ch_metafile_template_out
  
      script:
      """
      cat ${ch_metatemplate} > ${basefilename}.meta
      """
  }
}else if(params.generateDbSNPreference){

  // ##Download from web
  // #wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.*

  ch_file = Channel
    .fromPath(params.input, type: 'file')
    .map { file -> tuple(file.baseName, file) }

  dbsnpsplits = 10

  process dbsnp_reference_convert_and_split {

      cpus 2

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple basefilename, dbsnpvcf from ch_file

      output:
      file("chunk_*") into ch_dbsnp_split
      file("dbsnp_GRCh38")

      script:
      """
      ##Download from web
      #wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.*
      module load tools
      module load pigz/2.3.4
      
      #reformat
      pigz --decompress --stdout --processes 2 ${dbsnpvcf} | grep -v "#" > dbsnp_GRCh38 

      #split into dbsnpsplit number of unix split files
      split -dn ${dbsnpsplits} dbsnp_GRCh38 chunk_

      """
  }

  ch_dbsnp_split
    .flatten()
    .map { file -> tuple(file.baseName, file) }
    .set { ch_dbsnp_split2 }

  process dbsnp_reference_reformat {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_split2

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed") into ch_dbsnp_preformatted

      script:
      """
      awk '{print "chr"\$1, \$2, \$2,  \$1":"\$2, \$3, \$4, \$5}' ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed
      """
  }

  process dbsnp_reference_rm_indels {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_preformatted

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed.noindel") into ch_dbsnp_rmd_indels

      script:
      """
      # Remove all insertions or deletions
      # this will eliminate some rsids, both the ones with multiple rsids for the exact same snp, but also the ones with ref and alt switched.
      awk ' \$7 !~ /,/{if(length(\$6)!=1 || length(\$7)!=1 || \$6=="." || \$7=="."){print \$0 > "rm_indels"}else{print \$0}}; \$7 ~ /,/{if(\$7 ~ /\\w\\w/){print \$0 > "rm_indels2"}else{print \$0}} ' ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed.noindel
      """
  }

  ch_dbsnp_rmd_indels.into { ch_dbsnp_rmd_indels1; ch_dbsnp_rmd_indels2 }

  process dbsnp_reference_report_number_of_biallelic_multiallelics {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_indels1

      output:
      tuple cid, file("*") into ch_dbsnp_report_biallelic_mult_alleles

      script:
      """
      ## investigate the amount of single base multi allelics left (without filtering them out from the main workflow)
      awk ' \$7 ~ /,/{print \$0} ' ${dbsnp_chunk} > ${cid}_biallelic_multiallelics
      """
  }

  process dbsnp_reference_rm_dup_positions_GRCh38 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_indels2

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed.noindel.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh38
      file("${cid}_All_20180418_GRCh38.bed.noindel.sorted")

      script:
      """
      # Remove all duplicated positions GRCh38
      # this will eliminate some rsids, both the ones with multiple rsids for the exact same snp, but also the ones with ref and alt swithed.
      LC_ALL=C sort -k 4,4 --parallel 8 ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed.noindel.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh38"}; r0=var}' ${cid}_All_20180418_GRCh38.bed.noindel.sorted > ${cid}_All_20180418_GRCh38.bed.noindel.sorted.nodup

      """
  }

  ch_dbsnp_rmd_dup_positions_GRCh38.into { ch_dbsnp_rmd_dup_positions_GRCh38_1; ch_dbsnp_rmd_dup_positions_GRCh38_2; ch_dbsnp_rmd_dup_positions_GRCh38_3 }

  process dbsnp_reference_liftover_GRCh37 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh38_1

      output:
      tuple cid, file("${cid}_dbsnp_chunk_GRCh37_GRCh38") into ch_dbsnp_lifted_to_GRCh37
      file("${cid}_dbsnp_chunk_GRCh37")

      script:
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg38ToHg19chain} chain2.gz

      # Map to GRCh37
      /home/projects/ip_10000/IBP_pipelines/cleansumstats/cleansumstats_dev/cleansumstats_images/liftover-lates.img bed chain2.gz ${dbsnp_chunk} ${cid}_dbsnp_chunk_GRCh37
      awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print \$1, \$2, \$3, tmp":"\$2, \$4, \$5, \$6, \$7}' ${cid}_dbsnp_chunk_GRCh37 > ${cid}_dbsnp_chunk_GRCh37_GRCh38
      """
  }

  process dbsnp_reference_rm_dup_positions_GRCh37 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_lifted_to_GRCh37

      output:
      tuple cid, file("All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh37
      file("All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted")

      script:
      """
      # Remove all duplicated positions GRCh37 (as some positions might have become duplicates after the liftover)
      LC_ALL=C sort -k 4,4 --parallel 4 ${dbsnp_chunk} > All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh37"}; r0=var}' All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted > All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup
      """
  }

  process dbsnp_reference_rm_liftover_remaining_ambigous_GRCh37 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh37

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup.chromclean") into ch_dbsnp_rmd_ambig_GRCh37_liftovers
      file("${cid}_all_chr_types_GRCh37")

      script:
      """
      #To get a list of all chromosomes types
      awk '{gsub(":.*","",\$1); print \$1}' ${dbsnp_chunk} | awk '{ seen[\$1] += 1 } END { for (i in seen) print seen[i],i }' > ${cid}_all_chr_types_GRCh37
      
      #remove non standard chromosome names (seems like they include a "_" in the name)
      awk '{tmp=\$1; gsub(":.*","",\$1); if(\$1 !~ /_/ ){print tmp,\$2,\$3,\$4,\$5,\$6,\$7,\$8}}' ${dbsnp_chunk} > ${cid}_All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup.chromclean
      """
  }

  ch_dbsnp_rmd_ambig_GRCh37_liftovers.into { ch_dbsnp_rmd_ambig_GRCh37_liftovers1; ch_dbsnp_rmd_ambig_GRCh37_liftovers2; ch_dbsnp_rmd_ambig_GRCh37_liftovers3 }

  process dbsnp_reference_liftover_GRCh36 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_ambig_GRCh37_liftovers1

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh36.bed"), build into ch_dbsnp_lifted_to_GRCh36

      script:
      build = "36"
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg19ToHg18chain} chain2.gz

      #liftover
      /home/projects/ip_10000/IBP_pipelines/cleansumstats/cleansumstats_dev/cleansumstats_images/liftover-lates.img bed chain2.gz ${dbsnp_chunk} ${cid}_All_20180418_liftcoord_GRCh36.bed
      """
  }

  process dbsnp_reference_liftover_GRCh35 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_ambig_GRCh37_liftovers2

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh35.bed"), build into ch_dbsnp_lifted_to_GRCh35

      script:
      build = "35"
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg19ToHg17chain} chain2.gz

      #liftover
      /home/projects/ip_10000/IBP_pipelines/cleansumstats/cleansumstats_dev/cleansumstats_images/liftover-lates.img bed chain2.gz ${dbsnp_chunk} ${cid}_All_20180418_liftcoord_GRCh35.bed
      """
  }

  ch_dbsnp_lifted_to_GRCh35
    .mix(ch_dbsnp_lifted_to_GRCh36)
    .set{ ch_dbsnp_lifted_to_GRCh3x }

  process dbsnp_reference_rm_duplicates_GRCh36_GRCh35 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk, build from ch_dbsnp_lifted_to_GRCh3x

      output:
      tuple build, cid, file("${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh3x
      file("${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted")

      script:
      """
      # Remove all duplicated positions GRCh36 (as some positions might have become duplicates after the liftover)
      LC_ALL=C sort -k 4,4 --parallel 4 ${dbsnp_chunk} > ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh${build}"}; r0=var}' ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted > ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted.nodup
      """
  }

  process dbsnp_reference_rm_liftover_remaining_ambigous_GRCh36_GRCh35 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple build, cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh3x

      output:
      tuple build, cid, file("${cid}_All_20180418_liftcoord_GRCh${build}_GRCh38.bed.sorted.nodup.chromclean") into ch_dbsnp_rmd_ambig_positions_GRCh3x
      file("${cid}_all_chr_types_GRCh${build}")

      script:
      """
      #To get a list of all chromosomes types
      awk '{gsub(":.*","",\$1); print \$1}' ${dbsnp_chunk} | awk '{ seen[\$1] += 1 } END { for (i in seen) print seen[i],i }' > ${cid}_all_chr_types_GRCh${build}
      
      #remove non standard chromosome names (seems like they include a "_" in the name)
      awk '{tmp=\$1; gsub(":.*","",\$1); if(\$1 !~ /_/ ){print tmp,\$2,\$3,\$4,\$5,\$6,\$7,\$8}}' ${dbsnp_chunk} > ${cid}_All_20180418_liftcoord_GRCh${build}_GRCh38.bed.sorted.nodup.chromclean
      """
  }

  process dbsnp_reference_make_rsid_version_from_GRCh38 {

      publishDir "${params.outdir}", mode: 'symlink', overwrite: true

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh38_2

      output:
      file("${cid}_All_20180418_RSID_GRCh38.bed") into ch_dbsnp_rsid_to_GRCh38

      script:
      """
      # Make version sorted on RSID to get correct coordinates
      awk '{print \$5, \$4, \$6, \$7}' ${dbsnp_chunk} > ${cid}_All_20180418_RSID_GRCh38.bed
      """
  }

  process dbsnp_reference_merge_and_put_files_in_reference_library_RSID {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false

      input:
      file dbsnp_chunks from ch_dbsnp_rsid_to_GRCh38.collect()

      output:
      file("${ch_dbsnp_RSID_38.baseName}.bed")

      script:
      """
      # Concatenate 
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_RSID_GRCh38.map
      done

      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_RSID_GRCh38.map > "${ch_dbsnp_RSID_38.baseName}.bed"
      
      """
  }

//
// collect and STORE dbsnp files in reference library
//

  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh38 {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false

      input:
      file dbsnp_chunks from ch_dbsnp_rmd_dup_positions_GRCh38_3.collect()

      output:
      file("${ch_dbsnp_38.baseName}.bed")

      script:
      """
      # Concatenate 
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_RSID_GRCh38.map
      done

      # Sort
      awk '{print \$4,\$5,\$6,\$7}' All_20180418_RSID_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_38.baseName}.bed"
      
      """
  }


  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh38_GRCh37 {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false
      publishDir "${params.outdir}", mode: 'symlink', overwrite: true, pattern: '*.map'

      input:
      file dbsnp_chunks from ch_dbsnp_rmd_ambig_GRCh37_liftovers3.collect()

      output:
      file("${ch_dbsnp_38_37.baseName}.bed")
      file("${ch_dbsnp_37_38.baseName}.bed")
      file("*map")

      script:
      """
      # Concatenate 
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_GRCh38_GRCh37_tmp.map
      done

      awk '{print \$4, \$5, \$6, \$7, \$8}' All_20180418_GRCh38_GRCh37_tmp.map > All_20180418_GRCh37_GRCh38.map
      awk '{print \$5, \$4, \$6, \$7, \$8}' All_20180418_GRCh38_GRCh37_tmp.map > All_20180418_GRCh38_GRCh37.map

      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh38_GRCh37.map > "${ch_dbsnp_38_37.baseName}.bed"
      
      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh37_GRCh38.map > "${ch_dbsnp_37_38.baseName}.bed"
      
      """
  }

  ch_dbsnp_rmd_ambig_positions_GRCh3x_grouped = ch_dbsnp_rmd_ambig_positions_GRCh3x.groupTuple(by:0)

  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh3x_GRCh38 {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false, pattern: '*.bed'
      publishDir "${params.outdir}", mode: 'copy', overwrite: true, pattern: '*.map'

      input:
      tuple build, cid, file(dbsnp_chunks) from ch_dbsnp_rmd_ambig_positions_GRCh3x_grouped

      output:
      file("*.bed")
      file("*.map")

      script:
      """
      # Concatenate 
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_GRCh${build}_GRCh38.map
      done

      # Sort
      if [ "${build}" == "36" ]; then
        awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"\$2, \$5, \$6, \$7, \$8}' All_20180418_GRCh36_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_36_38.baseName}.bed"
      else
        awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"\$2, \$5, \$6, \$7, \$8}' All_20180418_GRCh35_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_35_38.baseName}.bed"
      fi

      """
  }

     // #
     // #
     // #awk '{print $4, $5, $6, $7, $8}' All_20180418_liftcoord_GRCh37_GRCh38.bed > All_20180418_GRCh37_GRCh38.bed
     // #awk '{print $5, $4, $6, $7, $8}' All_20180418_liftcoord_GRCh37_GRCh38.bed > All_20180418_GRCh38_GRCh37.bed
     // #LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh37_GRCh38.bed > All_20180418_GRCh37_GRCh38.sorted.bed
     // #LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh38_GRCh37.bed > All_20180418_GRCh38_GRCh37.sorted.bed
     // #
     // # 

}else {


  process get_software_versions {
      publishDir "${params.outdir}/pipeline_info", mode: 'copy',
          saveAs: { filename ->
              if (filename.indexOf(".csv") > 0) filename
              else null
          }
  
      output:
      file 'software_versions_mqc.yaml' into software_versions_yaml
      file "software_versions.csv" into ch_software_versions

  
      script:
      """
      echo $workflow.manifest.version > v_pipeline.txt
      echo $workflow.nextflow.version > v_nextflow.txt
      #sstools-version > v_sumstattools.txt
      scrape_software_versions.py &> software_versions_mqc.yaml
      """
  }
  

  process check_filter_params {
      publishDir "${params.outdir}/pipeline_info", mode: 'symlink', overwrite: true

      output:
      file("params_check_filtername_afterLiftoverFilter.log")
  
      script:
      """
      check_filter_names.sh ${afterLiftoverFilter} ${baseDir}/assets/allowed_names_afterLiftoverFilter.txt params_check_filtername_afterLiftoverFilter.log

      """
  }

  
  //use metafile filename as datasetID through the pipeline
  ch_mfile_checkX = Channel
                  .fromPath(params.input, type: 'file')
                  .map { file -> tuple(file.baseName, file) }
  
  ch_mfile_checkX.into { ch_mfile_check; ch_mfile_check2 }

  process make_meta_file_unix_friendly {
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile from ch_mfile_check
  
      output:
      tuple datasetID, mfile, file("mfile_unix_safe") into ch_mfile_unix_safe
      file("mfile_unix_safe2")
  
      script:
      """
      cat ${mfile} > mfile_sent_in
      
      # Clean meta file from windows return characters
      awk '{ sub("\\r\$", ""); print }' ${mfile} > mfile_unix_safe2
      
      # Remove obviously misplaced whitespaces (i.e., any whitespace before =, and leading whitespace directly after =)
      # Remove trailing whitespaces (i.e., any whitespace between the newline character and the last non whitespace character of the string)
      awk '\$1 !~ "#" && \$1 !~ "^ *\$"{split(\$0,out,"="); gsub(/ */, "", out[1]); sub(/ */, "", out[2]); sub(/ *\$/, "", out[2]); print out[1]"="out[2]} \$1 ~ "#" || \$1 ~ "^ *\$"{print \$0}' mfile_unix_safe2  > mfile_unix_safe
      """
  }


  process check_most_crucial_paths_exists {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, mfile_unix_safe from ch_mfile_unix_safe
  
      output:
      tuple datasetID, mfile_unix_safe, env(spath) into ch_mfile_check_format
      tuple datasetID, env(spath) into ch_input_sfile
      tuple datasetID, env(rpath) into ch_input_readme
      tuple datasetID, env(pmid) into ch_pmid
      tuple datasetID, env(pmid), env(pdfpath), file("${datasetID}_pdf_suppfiles.txt") into ch_input_pdf_stuff
      file("mfile_sent_in") 
  
      script:
      """

      metaDir="\$(dirname ${mfile})"
      cat ${mfile} > mfile_sent_in
      
      # Check if the datasetID folder is already present, if so just increment a number to get the new outname
      #  this is because a metafile can have the same name as another even though the content might be different. 

      # Check if field for variable exists and if the file specified exists
      check_meta_file_references.sh "path_sumStats" $mfile_unix_safe \$metaDir > spath.txt
      spath="\$(cat spath.txt)"
      check_meta_file_references.sh "path_readMe" $mfile_unix_safe \$metaDir > rpath.txt
      rpath="\$(cat rpath.txt)"
      check_meta_file_references.sh "path_pdf" $mfile_unix_safe \$metaDir > pdfpath.txt
      pdfpath="\$(cat pdfpath.txt)"
      check_meta_file_references.sh "path_supplementary" $mfile_unix_safe \${metaDir} > ${datasetID}_pdf_suppfiles.txt

      Px="\$(grep "^study_PMID=" $mfile_unix_safe)"
      pmid="\$(echo "\${Px#*=}")"

      # Check library if this has been processed before
      # TODO after the script producing the 00inventory.txt has been created 

      """
  }

  process check_mfile_format {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfilePath from ch_mfile_check_format
  
      output:
      tuple datasetID, mfile, sfilePath into ch_mfile_check_libID
      file("${datasetID}_header") 
      tuple datasetID, file("*.log")

      script:
      """
      # Make complete metafile check
      echo "\$(head -n 1 < <(zcat ${sfilePath}))" > ${datasetID}_header
      check_meta_data_format.sh ${mfile} ${datasetID}_header ${datasetID}_mfile_format.log

      """

  }

  process check_preassigned_libraryID {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfilePath from ch_mfile_check_libID
  
      output:
      tuple datasetID, mfile, sfilePath into ch_check_if_already_in_library
      tuple datasetID, env(sumstatID) into ch_preassigned_sumstat_id

      script:
      """
      
      #Check if new sumstatname should be assigned
      if grep -Pq "^cleansumstats_ID=" ${mfile}
      then
        SIDx="\$(grep "^cleansumstats_ID=" ${mfile})"
        sumstatID="\$(echo "\${SIDx#*=}")"
      else
        sumstatID="missing"
      fi

      """
  }

  process check_if_already_in_library {
     
      // This functionality is inactive right now, as it is not exactly clear what info to look for 

      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfilePath from ch_check_if_already_in_library
  
      output:
      tuple datasetID, mfile, sfilePath into ch_check_and_force_sumstat_format
      tuple datasetID, file("${datasetID}_one_line_summary_of_metadata.txt") into ch_one_line_metafile

      script:
      """
      
      # Make a one line metafile to use for the inventory file and test if dataset is already in library
      cat ${baseDir}/assets/columns_for_one_line_summary.txt | while read -r varx; do 
        Px="\$(grep "^\${varx}=" ${mfile})"
        var="\$(echo "\${Px#*=}")"
        printf "\t%s" "\${var}" >> one_line_summary
      done
      printf "\\n" >> one_line_summary
      cat one_line_summary | sed -e 's/^[\t]//' > ${datasetID}_one_line_summary_of_metadata.txt
      """
  }

  process check_and_force_basic_sumstat_format {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, mfile, sfilePath from ch_check_and_force_sumstat_format
  
      output:
      tuple datasetID, mfile into ch_mfile_ok
      tuple datasetID, file("${datasetID}_sfile") into ch_sfile_ok
      tuple datasetID, file("desc_force_tab_sep_BA.txt") into ch_desc_prep_force_tab_sep_BA
      file("${datasetID}_sfile_1000")
      file("${datasetID}_sfile_1000_formatted")
      file("*.log")

      script:
      """
      # Sumstat file check on first 1000 lines
      echo "\$(head -n 1000 < <(zcat ${sfilePath}))" | gzip -c > ${datasetID}_sfile_1000
      check_and_format_sfile.sh ${datasetID}_sfile_1000 ${datasetID}_sfile_1000_formatted ${datasetID}_sfile_1000_format.log
      
      # Make second sumstat file check on all lines
      check_and_format_sfile.sh ${sfilePath} ${datasetID}_sfile ${datasetID}_sfile_format.log
      #check_and_format_sfile.sh ${datasetID}_sfile_1000 ${datasetID}_sfile ${datasetID}_sfile_format.log
      
      #process before and after stats (the -1 is to remove the header count)
      rowsBefore="\$(zcat ${sfilePath} | wc -l | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l ${datasetID}_sfile | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tForce tab separation" > desc_force_tab_sep_BA.txt
      """
  }

if (params.checkerOnly == false){

    process add_sorted_index_sumstat {
    
        input:
        tuple datasetID, sfile from ch_sfile_ok
  
        //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        output:
        tuple datasetID, file("prep_sfile_added_rowindex") into ch_sfile_on_stream
        tuple datasetID, file("desc_add_sorted_rowindex_BA.txt") into ch_desc_prep_add_sorted_rowindex_BA
    
        script:
        """
        cat $sfile | sstools-raw add-index | LC_ALL=C sort -k 1 - > prep_sfile_added_rowindex
        
        #process before and after stats (the -1 is to remove the header count)
        rowsBefore="\$(wc -l $sfile | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l prep_sfile_added_rowindex | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tAdd sorted rowindex, which maps back to the unfiltered file" > desc_add_sorted_rowindex_BA.txt
        """
    }
  
    ch_mfile_ok.into { ch_mfile_ok1; ch_mfile_ok2; ch_mfile_ok3; ch_mfile_ok4; ch_mfile_ok5; ; ch_mfile_ok6}
  
   // ch_mfile_ok7
   //  .combine(ch_sfile_on_stream0, by: 0)
   //  .set { ch_sfile_on_stream00 }
  
    
    ch_sfile_on_stream.into { ch_sfile_on_stream1; ch_sfile_on_stream2; ch_sfile_on_stream3; ch_sfile_on_stream4; ch_sfile_on_stream5 }
    ch_mfile_and_stream=ch_mfile_ok1.join(ch_sfile_on_stream1)
    ch_mfile_and_stream.into { ch_check_gb; ch_liftover; ch_liftover1; ch_liftover2; ch_stats_inference }
    
    process does_chrpos_exist {
    
        input:
        tuple datasetID, mfile from ch_mfile_ok6
        
        output:
        tuple datasetID, env(CHRPOSexists) into ch_present_CHRPOS
    
        script:
        """
        CHRPOSexists=\$(does_CHR_and_POS_exist.sh $mfile)
        """
    }
  
    //Create filter for when CHR and POS exists or not
    ch_present_CHRPOS_br=ch_present_CHRPOS.branch { key, value -> 
                   CHRPOSexists: value == "true"
                   CHRPOSmissing: value == "false"
                    }
    
    //split the channels based on filter
    ch_present_CHRPOS_br2=ch_present_CHRPOS_br.CHRPOSexists
    ch_present_CHRPOS_br3=ch_present_CHRPOS_br.CHRPOSmissing
    
    //combine each channel with the matching datasetID
    ch_CHRPOS_exists=ch_liftover1.combine(ch_present_CHRPOS_br2, by: 0)
    ch_CHRPOS_missing=ch_liftover2.combine(ch_present_CHRPOS_br3, by: 0)
  
    // LIFTOVER BRANCH 1
  
    process prep_dbsnp_mapping_by_sorting_rsid_version {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, mfile, sfile, chrposExists from ch_CHRPOS_missing
    
        output:
        tuple datasetID, mfile, file("gb_lift") into ch_liftover_33
        tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos_rsid
        tuple datasetID, file("desc_sex_chrom_formatting_BA.txt") into ch_desc_sex_chrom_formatting_BA_1
  
        script:
        """
    
        Sx="\$(grep "^col_SNP=" $mfile)"
        colSNP="\$(echo "\${Sx#*=}")"
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colSNP}" -n"0,RSID" | awk -vFS="\t" -vOFS="\t" '{print \$2,\$1}' > gb_lift
        #LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
        
  
        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l gb_lift | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
        # Blow as dummy channel (as rsid will be automatically forced to correct annotation)
        # Process before and after stats (the -1 is to remove the header count)
        # RowsBefore="\$(wc -l ${sfile} | awk '{print \$1-1}')"
        # RowsAfter="\$(wc -l gb_lift | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tforced sex chromosomes and mitochondria chr annotation to the numbers 23-26" > desc_sex_chrom_formatting_BA.txt
    
        """
    }
  
    process remove_duplicated_rsid_before_liftover_rsid_version {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, mfile, rsidprep from ch_liftover_33
        
        output:
        tuple datasetID, mfile, file("gb_unique_rows_sorted") into ch_liftover_3333
        tuple datasetID, file("desc_removed_duplicated_rows") into ch_removed_rows_before_liftover_rsids
        tuple datasetID, file("removed_duplicated_rows") into ch_removed_rows_before_liftover_ix_rsids
        file("removed_*")
        file("beforeLiftoverFiltering_executionorder")
  
        script:
        """
        filter_before_liftover.sh $rsidprep ${beforeLiftoverFilter}
  
        """
    }
  
  
    process liftover_to_GRCh38_and_map_to_dbsnp_rsid_version {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'

        input:
        tuple datasetID, mfile, fsorted from ch_liftover_3333
        
        output:
        tuple datasetID, mfile, file("gb_lifted_and_mapped_to_GRCh38") into ch_liftover_49
        tuple datasetID, file("desc_liftover_to_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_rsid
        tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build_rsid
        tuple datasetID, file("removed_not_matching_during_liftover_ix") into ch_not_matching_during_liftover_rsid
        file("removed_*")
         
        script:
        """
        #in gb_lifted_and_mapped_to_GRCh38, the order will be 
        #GRCh38, GRCh37, rowIndex, RSID, REF, ALT
        #chr:pos | inx | rsid | a1 | a2 | chr:pos2 (if available)
        LC_ALL=C join -1 1 -2 1 ${fsorted} ${ch_dbsnp_RSID_38} | awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$1,\$4,\$5}'  > gb_lifted_and_mapped_to_GRCh38
  
         
        # Lines not possible to map
        LC_ALL=C join -v 1 -1 1 -2 3 ${fsorted} gb_lifted_and_mapped_to_GRCh38 > removed_not_matching_during_liftover
        awk -vOFS="\t" '{print \$2,"not_matching_during_liftover"}' removed_not_matching_during_liftover > removed_not_matching_during_liftover_ix

        # Process before and after stats
        rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l gb_lifted_and_mapped_to_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh38_and_map_to_dbsnp_BA

        # Make an empty stats file as we are not trying to infer genome build
        echo "No inference of build going from RSID" > ${datasetID}.stats
        #
        """
    }
    
    
    // LIFTOVER BRANCH 2
  
    process reformat_X_Y_XY_and_MT_and_remove_noninterpretables {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
        input:
        tuple datasetID, mfile, sfile, chrposexist from ch_CHRPOS_exists
    
        output:
        tuple datasetID, mfile, file("prep_sfile_forced_sex_chromosome_format"), chrposexist into ch_chromosome_fixed
        path("new_chr_sex_format")
        path("new_chr_sex_format2")
        path("new_chr_sex_format3")
        tuple datasetID, file("desc_sex_chrom_formatting_BA.txt") into ch_desc_sex_chrom_formatting_BA_2
        tuple datasetID, env(rowsAfter) into ch_rowsAfter_number_of_lines
    
        script:
        """
        Cx="\$(grep "^col_CHR=" $mfile)"
        colCHR="\$(echo "\${Cx#*=}")"
  
        #make
        cat $sfile | sstools-utils ad-hoc-do -k "0|funx_force_sex_chromosomes_format(\${colCHR})" -n"0,\${colCHR}" > new_chr_sex_format
  
        #remove sex formats of unknown origin
        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        echo "\${colCHR}" > gb_ad-hoc-do_funx_CHR_sex_chrom_filter
        cat new_chr_sex_format | sstools-utils ad-hoc-do -k "0|\${colCHR}" -n"0,CHR" | awk -vFS="\t" -vOFS="\t" 'BEGIN{getline; print \$0}; {if(\$2 > 0 && \$2 < 27){ print \$1, \$2 }}' > new_chr_sex_format2
        #use the index to remove everything no part of chr numers 1-26 but keep original format
        LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 -1 1 -2 1 new_chr_sex_format new_chr_sex_format2 > new_chr_sex_format3
  
        #replace (if bp or allele info is in the same column it will be kept, as the function above only replaces the chr info part)
        head -n1 $sfile > header
        to_keep_from_join="\$(awk -vFS="\t" -vobj=\${colCHR} '{for (i=1; i<=NF; i++){if(obj==\$i){print "2."2}else{print "1."i}}}' header)"
        LC_ALL=C join -t "\$(printf '\t')" -o \${to_keep_from_join} -1 1 -2 1 $sfile new_chr_sex_format3 > prep_sfile_forced_sex_chromosome_format
        
        #process before and after stats (the -1 is to remove the header count)
        rowsBefore="\$(wc -l $sfile | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l prep_sfile_forced_sex_chromosome_format | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tforced sex chromosomes and mitochondria chr annotation to the numbers 23-26" > desc_sex_chrom_formatting_BA.txt
        """
    }
  
    whichbuild = ['GRCh35', 'GRCh36', 'GRCh37', 'GRCh38']
    
    process genome_build_stats {
    
        //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, mfile, sfile, chrposexist from ch_chromosome_fixed
        each build from whichbuild
    
        output:
        tuple datasetID, file("${datasetID}*.res") into ch_genome_build_stats
        file("gb_*")
    
        script:
        """
    
        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
        echo "\${colCHR}" > gb_ad-hoc-do_funx_CHR
        echo "\${colPOS}" > gb_ad-hoc-do_funx_POS

        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" > gb_extract_and_format_chr_and_pos_to_detect_build
        awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' gb_extract_and_format_chr_and_pos_to_detect_build > gb_ready_to_join_to_detect_build
        LC_ALL=C sort -k1,1 gb_ready_to_join_to_detect_build > gb_ready_to_join_to_detect_build_sorted
        format_chrpos_for_dbsnp.sh ${build} gb_ready_to_join_to_detect_build_sorted ${ch_dbsnp_35_38} ${ch_dbsnp_36_38} ${ch_dbsnp_37_38} ${ch_dbsnp_38} > ${build}.map
        sort -u -k1,1 ${build}.map | wc -l | awk -vOFS="\t" -vbuild=${build} '{print \$1,build}' > ${datasetID}.${build}.res
  
    
        """
    }
    
    
    
    ch_genome_build_stats_grouped = ch_genome_build_stats.groupTuple(by:0,size:4)
    
    process infer_genome_build {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, file(ujoins) from ch_genome_build_stats_grouped
    
        
        output:
        tuple datasetID, env(GRChmax) into ch_known_genome_build
        tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build_chrpos
        tuple datasetID, file("GRChOther"), env(GRChmaxVal) into ch_build_stats_for_failsafe
    
        script:
        """
        for gbuild in ${ujoins}
        do
            cat \$gbuild >> ${datasetID}.stats
        done
        GRChmax="\$(cat ${datasetID}.stats | sort -nr -k1,1 | head -n1 | awk '{print \$2}')"
        GRChmaxVal="\$(cat ${datasetID}.stats | sort -nr -k1,1 | head -n1 | awk '{print \$1}')"

        cat ${datasetID}.stats | sort -nr -k1,1 | tail -n+2 > GRChOther

        """
    
    }

    ch_rowsAfter_number_of_lines
      .combine(ch_build_stats_for_failsafe, by: 0)
      .set{ ch_failsafe }

    process build_failsafe {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, tot, buildstat, grmax from ch_failsafe
    
        script:
        """

        #check that GRChmax has at least 90% hits in dbsnp
        echo "${grmax}" | awk -vtot="${tot}" '{if( \$1/tot < 0.9){print "too few hits of the best matching build"}else{print "ok"}}' | while read t; do 
          if [ "\$t" == "ok" ]; then
            :
          else
            echo "\$t"
            exit
          fi
        done

        #check that the others have less than 60% hits in dbsnp
        awk -vtot="${tot}" '{if( \$1/tot > 0.6){print "too many hits of the not selected builds"}else{print "ok"}}' ${buildstat} | while read t; do 
          if [ "\$t" == "ok" ]; then
            :
          else
            echo "\$t"
            exit
          fi
        done

        """
    
    }


    ch_liftover_2=ch_liftover.join(ch_known_genome_build)
    
    process prep_dbsnp_mapping_by_sorting_chrpos_version {
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, mfile, sfile, gbmax from ch_liftover_2
    
        output:
        tuple datasetID, mfile, file("gb_lift"), gbmax into ch_liftover_3
        tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos
    
        script:
        """
        
        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
    
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" | awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' > gb_lift
        #LC_ALL=C sort -k1,1 gb_lift > gb_lift_sorted
        
        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l gb_lift | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
        """
    
    }
  
    process remove_duplicated_chr_position_before_liftover {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, mfile, chrposprep, gbmax from ch_liftover_3
        
        output:
        tuple datasetID, mfile, file("gb_unique_rows_sorted"), gbmax into ch_liftover_333
        tuple datasetID, file("desc_removed_duplicated_rows") into ch_removed_rows_before_liftover_chrpos
        tuple datasetID, file("removed_duplicated_rows") into ch_removed_rows_before_liftover_ix_chrpos
        file("removed_*")
        file("beforeLiftoverFiltering_executionorder")
  
        script:
        """
        filter_before_liftover.sh $chrposprep ${beforeLiftoverFilter}
  
        """
    }
  
    
  process liftover_to_GRCh38_and_map_to_dbsnp_chrpos_version {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, mfile, fsorted, gbmax from ch_liftover_333
        
        output:
        tuple datasetID, mfile, file("gb_lifted_and_mapped_to_GRCh38") into ch_liftover_44
        tuple datasetID, file("desc_liftover_to_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_chrpos
        tuple datasetID, file("removed_not_matching_during_liftover_ix") into ch_not_matching_during_liftover_chrpos
        file("removed_*")
        file("lifted_middle_step*")
    
        script:
        """
        
        #in gb_lifted_and_mapped_to_GRCh37_and_GRCh38, the order will be 
        #GRCh38, GRCh37, rowIndex, RSID, REF, ALT
        #chr:pos | inx | rsid | a1 | a2 | chr:pos2 (if available)
        if [ "${gbmax}" == "GRCh38" ] ; then
          LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_38} > lifted_middle_step 
          awk -vFS="[[:space:]]" -vOFS="\t" '{print \$1,\$2,\$3,\$4,\$5}' lifted_middle_step > gb_lifted_and_mapped_to_GRCh38
        elif [ "${gbmax}" == "GRCh37" ] ; then
          LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_37_38} > lifted_middle_step 
          awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' lifted_middle_step > gb_lifted_and_mapped_to_GRCh38
        elif [ "${gbmax}" == "GRCh36" ] ; then
          LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_36_38} > lifted_middle_step
          awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' lifted_middle_step > gb_lifted_and_mapped_to_GRCh38
        elif [ "${gbmax}" == "GRCh35" ] ; then
          LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_35_38} > lifted_middle_step
          awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' lifted_middle_step > gb_lifted_and_mapped_to_GRCh38
        else
          echo "${gbmax} is none of the available builds 35, 36, 37 or 38"
        fi

  
        # Lines not possible to map
        LC_ALL=C join -v 1 -1 1 -2 1 ${fsorted} lifted_middle_step > removed_not_matching_during_liftover
        awk -vOFS="\t" '{print \$2,"not_matching_during_liftover"}' removed_not_matching_during_liftover > removed_not_matching_during_liftover_ix
  
        #process before and after stats
        rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l gb_lifted_and_mapped_to_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh38_and_map_to_dbsnp_BA
        """
    }
  
    //mix the chrpos and rsid channels
    ch_not_matching_during_liftover_rsid
      .mix(ch_not_matching_during_liftover_chrpos)
      .set{ ch_not_matching_during_liftover }
  
    ch_removed_rows_before_liftover_chrpos
      .mix(ch_removed_rows_before_liftover_rsids)
      .set{ ch_removed_rows_before_liftover }
  
    ch_removed_rows_before_liftover_ix_chrpos
      .mix(ch_removed_rows_before_liftover_ix_rsids)
      .set{ ch_removed_rows_before_liftover_ix }
  
    ch_liftover_49
      .mix(ch_liftover_44)
      .set{ ch_liftover_mix_X }
  
    ch_desc_sex_chrom_formatting_BA_1
      .mix(ch_desc_sex_chrom_formatting_BA_2)
      .set{ ch_desc_sex_chrom_formatting_BA }
  
    ch_stats_genome_build_rsid
      .mix(ch_stats_genome_build_chrpos)
      .set{ ch_stats_genome_build }
  
    ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_rsid
      .mix(ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_chrpos)
      .set{ ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA }
  
    ch_desc_prep_for_dbsnp_mapping_BA_chrpos_rsid
      .mix(ch_desc_prep_for_dbsnp_mapping_BA_chrpos)
      .set{ ch_desc_prep_for_dbsnp_mapping_BA }
  
    process remove_duplicated_chr_position_allele_rows {
     
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, mfile, liftedandmapped from ch_liftover_mix_X
        
        output:
        tuple datasetID, mfile, file("gb_unique_rows_sorted") into ch_liftover_4
        tuple datasetID, file("desc_removed_duplicated_rows") into removed_rows_before_after_liftover
        tuple datasetID, file("removed_duplicated_rows") into removed_rows_before_after_liftover_ix
        file("removed_*")
        file("afterLiftoverFiltering_executionorder")
  
        script:
        """
        filter_after_liftover.sh $liftedandmapped ${afterLiftoverFilter}
  
        """
    }
  
  
    process split_off_GRCh38 {
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, mfile, liftedandmapped from ch_liftover_4
        
        output:
        //tuple datasetID, val("GRCh37"), mfile, file("gb_lifted_GRCh37") into ch_mapped_GRCh37
        tuple datasetID, val("GRCh38"), mfile, file("gb_lifted_GRCh38") into ch_mapped_GRCh38
        //tuple datasetID, file("desc_keep_only_GRCh37_version_BA.txt") into ch_desc_keep_only_GRCh37_version_BA  
        tuple datasetID, file("desc_keep_a_GRCh38_reference_BA.txt") into ch_desc_keep_a_GRCh38_reference_BA  
  
        script:
        """
        #prepare GRCh38 for downstream analysis
        awk -vFS="[[:space:]]" -vOFS="\t" '{print \$2,\$1,\$3,\$4,\$5}' $liftedandmapped > gb_lifted_GRCh38
  
        #split off GRCh37 to use only for coordinate reference
        #awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$1,\$4,\$5,\$6}' $liftedandmapped > gb_lifted_GRCh37
  
        
        #process before and after stats
        #rowsBefore="\$(wc -l $liftedandmapped | awk '{print \$1}')"
        #rowsAfter="\$(wc -l gb_lifted_GRCh37 | awk '{print \$1}')"
        #echo -e "\$rowsBefore\t\$rowsAfter\tKeep only GRCh37 coordinates alleleinfo, which will be the file subjected to further cleaning" > desc_keep_only_GRCh37_version_BA.txt
  
        
        #process before and after stats
        rowsBefore="\$(wc -l $liftedandmapped | awk '{print \$1}')"
        rowsAfter="\$(wc -l gb_lifted_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tSplit off a version of GRCh38 as coordinate reference" > desc_keep_a_GRCh38_reference_BA.txt
        """
    }
    
  
    process split_multiallelics_and_resort_index {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, build, mfile, liftgrs from ch_mapped_GRCh38
        
        output:
        tuple datasetID, build, mfile, file("gb_multialleles_splittorows") into ch_allele_correction
        tuple datasetID, file("desc_split_multi_allelics_and_sort_on_rowindex_BA.txt") into ch_desc_split_multi_allelics_and_sort_on_rowindex_BA  
        file("gb_splitted_multiallelics")
    
        script:
        """
        split_multiallelics_to_rows.sh $liftgrs > gb_splitted_multiallelics
        echo -e "0\tCHRPOS\tRSID\tA1\tA2" > gb_multialleles_splittorows
        LC_ALL=C sort -k1,1 gb_splitted_multiallelics >> gb_multialleles_splittorows
        
        #process before and after stats (rows is -1 because of header)
        rowsBefore="\$(wc -l $liftgrs | awk '{print \$1}')"
        rowsAfter="\$(wc -l gb_multialleles_splittorows | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tSplit multi-allelics to multiple rows and sort on original rowindex " > desc_split_multi_allelics_and_sort_on_rowindex_BA.txt
  
        """
    }
    
    ch_allele_correction_combine=ch_allele_correction.combine(ch_sfile_on_stream2, by: 0)
    ch_allele_correction_combine.into{ ch_allele_correction_combine1; ch_allele_correction_combine2 }
    
    process does_exist_A2 {
    
        input:
        tuple datasetID, mfile from ch_mfile_ok2
        
        output:
        tuple datasetID, env(A2exists) into ch_present_A2
    
        script:
        """
        A2exists=\$(doesA2exist.sh $mfile)
        """
    }
    
    //Create filter for when A2 exists or not
    ch_present_A2_br=ch_present_A2.branch { key, value -> 
                    A2exists: value == "true"
                    A2missing: value == "false"
                    }
    
    //split the channels based on filter
    ch_present_A2_br2=ch_present_A2_br.A2exists
    ch_present_A2_br3=ch_present_A2_br.A2missing
    
    //combine each channel with the matching datasetID
    ch_A2_exists=ch_allele_correction_combine1.combine(ch_present_A2_br2, by: 0)
    ch_A2_missing=ch_allele_correction_combine2.combine(ch_present_A2_br3, by: 0)
    
    process allele_correction_A1_A2 {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, build, mfile, mapped, sfile, A2exists from ch_A2_exists
        
        output:
        tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_exists2
        tuple datasetID, file("removed_allele_filter_ix") into ch_removed_by_allele_filter_ix1
        tuple datasetID, file("desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
  
        script:
        """
        echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
        
        #init some the files collecting variants removed because of allele composition
        touch removed_notGCTA
        touch removed_indel
        touch removed_hom
        touch removed_palin
        touch removed_notPossPair
        touch removed_notExpA2
  
        colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
        colA2=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "altallele")
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}|\${colA2}" -n"0,A1,A2" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 1.3 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${mapped} | tail -n+2 | sstools-eallele correction -f - >> ${build}_acorrected

        #only keep the index to prepare for the file with all removed lines
        touch removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notGCTA"}' removed_notGCTA >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"indel"}' removed_indel >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"hom"}' removed_hom >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"palin"}' removed_palin >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notPossPair"}' removed_notPossPair >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notExpA2"}' removed_notExpA2 >> removed_allele_filter_ix
  
        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes. Usually a substantial amount." >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l ${build}_acorrected | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tAllele corretion sanity check that final filtered file before and after file have same row count" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
        """

  
    }
    
    
    process allele_correction_A1 {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, build, mfile, mapped, sfile, A2missing from ch_A2_missing
        
        output:
        tuple datasetID, build, mfile, file("${build}_acorrected") into ch_A2_missing2
        tuple datasetID, file("removed_allele_filter_ix") into ch_removed_by_allele_filter_ix2
        tuple datasetID, file("desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA
        file("${build}_mapped2")
    
        script:
        """
  
        #NOTE to use A1 allele only complicates the filtering on possible pairs etc, so we always need a multiallelic filter in how the filter works right now.
        # This is something we should try to accomodate to, so that it is not required. 
        multiallelic_filter.sh $mapped > ${build}_mapped2
        echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > ${build}_acorrected
        
        #init some the files collecting variants removed because of allele composition
        touch removed_notGCTA
        touch removed_indel
        touch removed_hom
        touch removed_palin
        touch removed_notPossPair
        touch removed_notExpA2
    
        colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}" -n"0,A1" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${build}_mapped2 | tail -n+2 | sstools-eallele correction -f - -a >> ${build}_acorrected 
  
        #only keep the index to prepare for the file with all removed lines
        touch removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notGCTA"}' removed_notGCTA >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"indel"}' removed_indel >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"hom"}' removed_hom >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"palin"}' removed_palin >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notPossPair"}' removed_notPossPair >> removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notExpA2"}' removed_notExpA2 >> removed_allele_filter_ix
  
        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
  
        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l ${build}_acorrected | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tsanity sanity check that final filtered file before and after file have same row count" >> desc_filtered_allele-pairs_with_dbsnp_as_reference
    
        """
    }
  
    //put the two brances into the same channel (as only one will be used per file, there will be no duplicates)
    ch_removed_by_allele_filter_ix1
      .mix(ch_removed_by_allele_filter_ix2)
      .set{ ch_removed_by_allele_filter_ix }
  
   // ch_describe_allele_filter1
   //   .mix(ch_describe_allele_filter2)
   //   .set{ ch_describe_allele_filter }
  
    ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
      .mix(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA)
      .set{ ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA }
  
  
    //mix the A1_A2_both and A1_solo channels
    ch_A2_exists2
      .mix(ch_A2_missing2)
      .set{ ch_allele_corrected_mix_X }
    
    process remove_duplicated_chr_position_rows {
    
        //if(params.keepIntermediateFiles){ publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true }
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, build, mfile, acorrected from ch_allele_corrected_mix_X
        
        output:
        tuple datasetID, build, mfile, file("ac_unique_rows_sorted") into ch_allele_corrected_mix_Y
        tuple datasetID, file("desc_removed_duplicated_rows") into ch_desc_removed_duplicated_chr_pos_rows_BA
        file("ac_*")
        file("afterAlleleCorrection_executionorder")
        file("removed_*")
  
        script:
        """
  
        #Can be used as a sanitycheck-filter to discover potential misbehaviour
        filter_after_allele_correction.sh $acorrected ${afterAlleleCorrectionFilter}
        
        """
    }
    ch_allele_corrected_mix_Y
      .into{ ch_allele_corrected_mix1; ch_allele_corrected_mix2 }
  
    process filter_stats {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
        publishDir "${params.outdir}/${datasetID}/removed_lines", mode: 'symlink', overwrite: true, pattern: 'removed_*'
    
        input:
        tuple datasetID, mfile, sfile from ch_stats_inference
        
        output:
        tuple datasetID, file("st_filtered_remains") into ch_stats_filtered_remain
        tuple datasetID, file("removed_stat_non_numeric_in_awk")
        tuple datasetID, file("removed_stat_non_numeric_in_awk_ix") into ch_stats_filtered_removed_ix
        tuple datasetID, file("desc_filtered_stat_rows_with_non_numbers_BA.txt") into ch_desc_filtered_stat_rows_with_non_numbers_BA
    
        script:
        """
        touch removed_stat_non_numeric_in_awk 
        touch removed_stat_non_numeric_in_awk_ix
        filter_stat_values.sh $mfile $sfile > st_filtered_remains 2> removed_stat_non_numeric_in_awk
        awk -vOFS="\t" '{print \$1,"stat_non_numeric_in_awk"}' removed_stat_non_numeric_in_awk > removed_stat_non_numeric_in_awk_ix
        
        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
        rowsAfter="\$(wc -l st_filtered_remains | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered out rows with stats impossible to do calculations from" > desc_filtered_stat_rows_with_non_numbers_BA.txt
        """
    }
    
    ch_stats_filtered_remain
      .combine(ch_mfile_ok5, by: 0)
      .set{ ch_stats_filtered_remain3 }
  
    process infer_stats {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, st_filtered, mfile from ch_stats_filtered_remain3
        
        output:
        tuple datasetID, mfile, file("st_inferred_stats") into ch_stats_selection
        file("st_which_to_do") into out_st_which_to_do
        tuple datasetID, file("desc_inferred_stats_if_inferred_BA.txt") into ch_desc_inferred_stats_if_inferred_BA
    
        script:
        """
        check_stat_inference.sh $mfile > st_which_to_do
    
        if [ -s st_which_to_do ]; then
          if grep -q "Z_fr_OR_P" st_which_to_do; then
    
            Px="\$(grep "^col_P=" $mfile)"
            P="\$(echo "\${Px#*=}")"
    
            echo -e "QNORM" > prepared_qnorm_vals
            cat $st_filtered | sstools-utils ad-hoc-do -f - -k "\${P}" -n"\${P}" | awk 'NR>1{print \$1/2}' | /home/projects/ip_10000/IBP_pipelines/cleansumstats/cleansumstats_dev/cleansumstats_images/2020-04-11-ubuntu-1804_stat_r_in_c.simg stat_r_in_c qnorm >> prepared_qnorm_vals
            cut -f 1 $st_filtered | paste - prepared_qnorm_vals > prepared_qnorm_vals2
            LC_ALL=C join -1 1 -2 1 -t "\$(printf '\t')" $st_filtered prepared_qnorm_vals2 > st_filtered2
    
            nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
            nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
            cat st_filtered2 | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
    
          else
            nh="\$(awk '{printf "%s,", \$1}' st_which_to_do | sed 's/,\$//' )"
            nf="\$(awk '{printf "%s|", \$2}' st_which_to_do | sed 's/|\$//' )"
            cat $st_filtered | sstools-utils ad-hoc-do -f - -k "0|\${nf}" -n"0,\${nh}" > st_inferred_stats
          fi
        else
          touch st_inferred_stats
        fi
        
        #process before and after stats
        rowsBefore="\$(wc -l ${st_filtered} | awk '{print \$1}')"
        rowsAfter="\$(wc -l st_inferred_stats | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tInferred stats, if stats are inferred" > desc_inferred_stats_if_inferred_BA.txt
        """
    }
    
    ch_stats_selection
      .combine(ch_sfile_on_stream3, by: 0)
      .set{ ch_stats_selection2 }
    
    process select_stats {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, mfile, inferred, sfile from ch_stats_selection2
        
        output:
        tuple datasetID, file("st_stats_for_output") into ch_stats_for_output
        tuple datasetID, file("desc_from_inferred_to_joined_selection_BA.txt") into ch_desc_from_inferred_to_joined_selection_BA
        tuple datasetID, file("desc_from_sumstats_to_joined_selection_BA.txt") into ch_desc_from_sumstats_to_joined_selection_BA
    
        script:
        """
        select_stats_for_output.sh $mfile $sfile $inferred > st_stats_for_output 
        
        #process before and after stats
        rowsBefore="\$(wc -l ${inferred} | awk '{print \$1}')"
        rowsAfter="\$(wc -l st_stats_for_output | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFrom inferred to joined selection of stats" > desc_from_inferred_to_joined_selection_BA.txt
        
        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
        rowsAfter="\$(wc -l st_stats_for_output | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFrom raw sumstat to joined selection of stats" > desc_from_sumstats_to_joined_selection_BA.txt
        """
    }
  
  
    
    
    ch_allele_corrected_mix1
      .combine(ch_stats_for_output, by: 0)
      .set{ ch_allele_corrected_and_outstats }
    
  process final_assembly {
  
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, build, mfile, acorrected, stats from ch_allele_corrected_and_outstats
      
      output:
      tuple datasetID, file("${datasetID}_cleaned") into ch_cleaned_file_1
      tuple datasetID, file("desc_final_merge_BA.txt") into ch_desc_final_merge_BA
  
      script:
      """
      apply_modifier_on_stats.sh $acorrected $stats > ${datasetID}_cleaned
      
      # process before and after stats
      rowsBefore="\$(wc -l $acorrected | awk '{print \$1}')"
      rowsAfter="\$(wc -l ${datasetID}_cleaned | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFrom dbsnp mapped to merged selection of stats, final step" > desc_final_merge_BA.txt
      """
  }

  process final_assembly_make_GRCh37_reference {

      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
      input:
      tuple datasetID, cleaned from ch_cleaned_file_1
      
      output:
      tuple datasetID, cleaned, file("inx_chrpos_GRCh37_B") into ch_cleaned_file
      file("cleaned_chrpos_sorted")
      file("inx_chrpos_GRCh37")
  
      script:
      """
      # match the GRCh37 build and publish it as separate file (where all GRCh38 rows are present, and missing are NA)
      awk -vFS="\t" '{print \$2":"\$3, \$1}' ${cleaned} | LC_ALL=C sort -k1,1 > cleaned_chrpos_sorted
      LC_ALL=C join  -a 1 -1 1 -2 1 -o 1.2 2.2 cleaned_chrpos_sorted ${ch_dbsnp_38_37} > inx_chrpos_GRCh37
      echo -e "0\tCHRPOS" > inx_chrpos_GRCh37_B
      awk -vOFS="\t" '{if(\$2 ~ /:/){print \$1, \$2 }else{print \$1, "NA"}}' inx_chrpos_GRCh37 >> inx_chrpos_GRCh37_B

      """

  }
  
    //Collect and place in corresponding stepwise order
    ch_removed_rows_before_liftover_ix
     .combine(ch_not_matching_during_liftover, by: 0)
     .combine(removed_rows_before_after_liftover_ix, by: 0)
     .combine(ch_removed_by_allele_filter_ix, by: 0)
     .combine(ch_stats_filtered_removed_ix, by: 0)
     .set{ ch_collected_removed_lines }
  
    process collect_all_removed_lines {
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
        input:
        tuple datasetID, step1, step2, step3, step4, step5 from ch_collected_removed_lines
  
        output:
        tuple datasetID, file("removed_lines_collected.txt") into ch_collected_removed_lines2
  
        script:
        """
        echo -e "RowIndex\tExclusionReason" > removed_lines_collected.txt
        cat ${step1} ${step2} ${step3} ${step4} >> removed_lines_collected.txt
        """
    }
  
    ch_collected_removed_lines2
      .into { ch_collected_removed_lines3; ch_collected_removed_lines4 }
  
    process describe_removed_lines_as_table {
    
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
    
        input:
        tuple datasetID, filtered_stats_removed from ch_collected_removed_lines3
        
        output:
        tuple datasetID, file("desc_removed_lines_table.txt") into ch_removed_lines_table
    
        script:
        """
        # prepare process specific descriptive statistics
        echo -e "NrExcludedRows\tExclusionReason" > desc_removed_lines_table.txt
        cat $filtered_stats_removed | tail -n+2 | awk -vOFS="\t" '{ seen[\$2] += 1 } END { for (i in seen) print seen[i],i }' >> desc_removed_lines_table.txt
  
        """
    }
  
  
    ch_cleaned_file
      .combine(ch_input_sfile, by: 0)
      .combine(ch_sfile_on_stream5, by: 0)
      .combine(ch_collected_removed_lines4, by: 0)
      .set{ ch_to_write_to_filelibrary2 }
  
    process gzip_outfiles {
  
        input:
        tuple datasetID, sclean, scleanGRCh37, inputsfile, inputformatted, removedlines from ch_to_write_to_filelibrary2
  
        output:
        tuple datasetID, path("sclean.gz"), path("scleanGRCh37.gz"), path("cleanedheader"), path("removed_lines.gz") into ch_to_write_to_filelibrary3
        tuple datasetID, inputsfile into ch_to_write_to_raw_library
        val datasetID into ch_check_avail
  
        script:
        """
        # Make a header file to use when deciding on what cols are present for the new meta file
        head -n1 ${sclean} > cleanedheader
  
        # Store data in library
        gzip -c ${sclean} > sclean.gz
        gzip -c ${scleanGRCh37} > scleanGRCh37.gz
        #gzip -c ${inputformatted} > raw_formatted_rowindexed.gz
        gzip -c ${removedlines} > removed_lines.gz
        """
    }
    
  
  //Do actual collection, placed in corresponding step order
  ch_desc_prep_force_tab_sep_BA
   .combine(ch_desc_prep_add_sorted_rowindex_BA, by: 0)
   .combine(ch_desc_sex_chrom_formatting_BA, by: 0)
   .combine(ch_desc_prep_for_dbsnp_mapping_BA, by: 0)
   .combine(ch_removed_rows_before_liftover, by: 0)
   .combine(ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA, by: 0)
   .combine(removed_rows_before_after_liftover, by: 0)
   .combine(ch_desc_keep_a_GRCh38_reference_BA, by: 0)
   .combine(ch_desc_split_multi_allelics_and_sort_on_rowindex_BA, by: 0)
   .combine(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA, by: 0)
   .combine(ch_desc_removed_duplicated_chr_pos_rows_BA, by: 0)
   .combine(ch_desc_filtered_stat_rows_with_non_numbers_BA, by: 0)
   .combine(ch_desc_inferred_stats_if_inferred_BA, by: 0)
   .combine(ch_desc_from_inferred_to_joined_selection_BA, by: 0)
   .combine(ch_desc_from_sumstats_to_joined_selection_BA, by: 0)
   .combine(ch_desc_final_merge_BA, by: 0)
   .set{ ch_collected_workflow_stepwise_stats }

  process collect_and_prepare_stepwise_readme {
      publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true

      input:
      tuple datasetID, step1, step2, step3, step4, step5, step6, step7, step8, step9, step10, step11, step12, step13, step14, step15, step16 from ch_collected_workflow_stepwise_stats

      output:
      tuple datasetID, file("desc_collected_workflow_stepwise_stats.txt") into ch_overview_workflow_steps

      script:
      """
      cat $step1 $step2 $step3 $step4 $step5 $step6 $step7 $step8 $step9 $step10 $step11 $step12 $step13 $step14 $step15 $step16 > all_removed_steps

      echo -e "Steps\tBefore\tAfter\tDescription" > desc_collected_workflow_stepwise_stats.txt
      awk -vFS="\t" -vOFS="\t" '{print "Step"NR, \$1, \$2, \$3}' all_removed_steps >> desc_collected_workflow_stepwise_stats.txt

      """
  }
  
    ch_preassigned_sumstat_id
     .combine(ch_check_avail, by: 0)
     .set{ ch_assign_sumstat_id }
  
    process assign_sumstat_id {
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true
  
        input:
        tuple datasetID, sumstatname from ch_assign_sumstat_id
  
        output:
        tuple datasetID, env(libfolder) into ch_assigned_sumstat_id
        file("assigned_sumstat_id")      
  
        script:
        """
        if [ "${sumstatname}" == "missing" ] ; then
          # Scan for available ID and move directory there
          libfolder="\$(assign_folder_id.sh ${params.libdirsumstats})"
          val=\$(mkdir "${params.libdirsumstats}/\${libfolder}")
          while [ \$? != 0 ]
          do
            sleep 2
            libfolder="\$(assign_folder_id.sh ${params.libdirsumstats})"
            val=\$(mkdir "${params.libdirsumstats}/\${libfolder}")
          done

          echo "\${libfolder}" > assigned_sumstat_id 

        else
          libfolder="${sumstatname}"
            mkdir "${params.libdirsumstats}/\${libfolder}"
          echo "${sumstatname}" > assigned_sumstat_id 
        fi
        """
    }

    ch_assigned_sumstat_id
    .into { ch_assigned_sumstat_id1; ch_assigned_sumstat_id2 }
  
    process check_pdf_library {
        publishDir "${params.libdirpdfs}", mode: 'copy', overwrite: false, pattern: 'pmid_*'
        input:
        tuple datasetID, pmid, pdfpath, pdfsuppdir from ch_input_pdf_stuff
  
        output:
        tuple datasetID, pmid, pdfpath, pdfsuppdir, env(makePDF), env(makePDFsupp) into ch_input_pdf_stuff2
        path("pmid_*") optional true
  
        script:
        """
        if [ -f "${params.libdirpdfs}/pmid_${pmid}.pdf" ]
        then
          makePDF="false"
        else
          makePDF="true"
          #will be overwritten in update library step (but here to limit parallell processes to change the same file)
          touch pmid_${pmid}.pdf
        fi
  
        if [ -d "${params.libdirpdfs}/pmid_${pmid}_supp" ]
        then
          makePDFsupp="false"
        else
          makePDFsupp="true"
          #will be overwritten in update library step
          mkdir pmid_${pmid}_supp
        fi
  
        """
    }
  
    process update_pdf_library {
        publishDir "${params.libdirpdfs}", mode: 'copy', overwrite: true, pattern: 'pmid_*'
  
        input:
        tuple datasetID, pmid, pdfpath, pdfsuppdir, makePDF, makePDFsupp from ch_input_pdf_stuff2
  
        output:
        tuple datasetID, pmid, pdfpath, pdfsuppdir into ch_input_pdf_stuff3
        path("pmid_*") optional true
  
        script:
        """
        if [ "${makePDF}" == "true" ]
        then
          cp ${pdfpath} pmid_${pmid}.pdf
        else
          :
        fi
        
        #check supplementary materail folder
        if [ "${makePDFsupp}" == "true" ]
        then
          mkdir pmid_${pmid}_supp
          i=1
          cat ${pdfsuppdir} | while read -r supp; do 
            if [ "\${supp}" != "missing" ]
            then
              supp2="\$(basename "\${supp}")" 
              extension="\${supp2##*.}" 
              cp -r \$supp pmid_${pmid}_supp/pmid_${pmid}_supp_\${i}.\${extension} 
              i=\$((i+1))
            else
              :
            fi
          done
        else
          :
        fi
  
        """
    }

    ch_input_pdf_stuff3
    .into { ch_input_pdf_stuff4; ch_input_pdf_stuff5 }

    ch_input_readme
    .into { ch_input_readme1; ch_input_readme2 }

    ch_assigned_sumstat_id2
    .combine(ch_to_write_to_raw_library, by: 0)
    .combine(ch_input_readme2, by: 0)
    .combine(ch_mfile_check2, by: 0)
    .combine(ch_input_pdf_stuff5, by: 0)
    .set { ch_to_write_to_raw_library2 }

    process put_in_raw_library {
        publishDir "${params.libdirraw}", mode: 'copyNoFollow', overwrite: false
  
        input:
        tuple datasetID, libfolder, rawfile, readme, rawmfile, pmid, pdfpath, pdfsuppdir from ch_to_write_to_raw_library2

        output:
        path("${libfolder}")
  
        script:
        """
        # Make sumstat folder with corresponding ID as the cleaned one
        mkdir ${libfolder}

        # Copy the raw metafile (useful to get the true original, will not be possible to use to rerun from this folder )
        cp ${rawmfile} ${libfolder}/${libfolder}_raw_meta.txt
        
        # Copy a new metafile (useful to get the new path names that follow our convention)
        # Important is that this file should not be confused with the cleaned metafile (we might want to revisit naming of the metafiles)
        # cp newmfile libfolder/libfolder_new_meta.txt

        # If raw checksum already exists then make sym link instead of copying over the file (but for now, always make physical copy)
        cp $rawfile ${libfolder}/${libfolder}_raw.gz

        if [ "${readme}" != "missing" ] ; then
          cp $readme ${libfolder}/${libfolder}_raw_README.txt
        fi

        # Add pdf stuff as symlinks, this makes it easy to rerun raw sumstats
        ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf ${libfolder}/${libfolder}_pmid_${pmid}.pdf

        """
    }
  
  
    ch_assigned_sumstat_id1
     .combine(ch_to_write_to_filelibrary3, by: 0)
     .combine(ch_mfile_ok3, by: 0)
     .combine(ch_input_readme1, by: 0)
     .combine(ch_input_pdf_stuff4, by: 0)
     .combine(ch_overview_workflow_steps, by: 0)
     .combine(ch_removed_lines_table, by: 0)
     .combine(ch_stats_genome_build, by: 0)
     .combine(ch_software_versions)
     .set{ ch_to_write_to_filelibrary7 }
  
     //.combine(ch_collected_removed_lines2)
  
    process put_in_cleaned_library {
    
        publishDir "${params.libdirsumstats}/${libfolder}", mode: 'copyNoFollow', overwrite: false, pattern: 'sumstat_*'
        publishDir "${params.outdir}/${datasetID}", mode: 'symlink', overwrite: true, pattern: 'libprep_*'
  
        input:
        tuple datasetID, libfolder, sclean, scleanGRCh37, cleanedheader, removedlines, mfile, readme, pmid, pdfpath, pdfsuppdir, overviewworkflow, removedlinestable, gbdetect, softv from ch_to_write_to_filelibrary7
        
        output:
        path("sumstat_*")
        path("libprep_*") 
        tuple datasetID, libfolder, mfile into ch_update_library_info_file
    
        script:
        """
        
        # Restructure output to allow replace or extending some variables, save changes in changes_mfile
        echo "cleansumstats_ID=${libfolder}" > libprep_changes_mfile
  
        # To make batch updates easier, change the path to raw files to the new name convention, but save old paths for traceability
        echo "path_sumStats=${libfolder}_raw.gz" >> libprep_changes_mfile
        Sx="\$(grep "^path_sumStats=" $mfile)"
        S="\$(echo "\${Sx#*=}")"
        echo "path_original_sumStats=\${S}" >> libprep_changes_mfile
  
        if [ "${readme}" != "missing" ] ; then
          Rx="\$(grep "^path_readMe=" $mfile)"
          R="\$(echo "\${Rx#*=}")"
          echo "path_readMe=${libfolder}_raw_README.txt" >> libprep_changes_mfile
          echo "path_original_readMe=\${R}" >> libprep_changes_mfile
        else
          echo "path_readMe=missing" >> libprep_changes_mfile
          echo "path_original_readMe=missing" >> libprep_changes_mfile
        fi
  
        Px="\$(grep "^path_pdf=" $mfile)"
        P="\$(echo "\${Px#*=}")"
        echo "path_pdf=${libfolder}_pmid_${pmid}.pdf" >> libprep_changes_mfile
        echo "path_original_pdf=\${P}" >> libprep_changes_mfile
  
        #Add cleaned output files
        echo "cleansumstats_cleaned_GRCh38=${libfolder}_cleaned_GRCh38.gz" >> libprep_changes_mfile
        echo "cleansumstats_cleaned_GRCh37_coordinates=${libfolder}_cleaned_GRCh37.gz" >> libprep_changes_mfile
  
        #Calcualate effective N using meta data info
        sh try_infere_Neffective.sh ${mfile} >> libprep_changes_mfile
        
        # copy the pdf and supplemental material if missing in pdf library
        # and prepare changes for the new mfile
        #cp ${pdfpath} pmid_${pmid}.pdf
        ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf ${libfolder}_pmid_${pmid}.pdf
  
        mkdir -p ${libfolder}_pmid_${pmid}_supp
  
        if [ -d "${params.libdirpdfs}/pmid_${pmid}_supp" ]
        then
          #Check dir if is not empty
          count="\$(ls -1 ${params.libdirpdfs}/pmid_${pmid}_supp | wc -l)"
          if [ "\${count}" -gt 0 ]
          then
            # If supplementary is already available, then use those, and do not use new from meta data file
            for fil in ${params.libdirpdfs}/pmid_${pmid}_supp/*
            do 
              supp="\$(basename "\${fil}")" 
              echo "path_supplementary=${libfolder}_pmid_${pmid}_supp/${libfolder}_\${supp}" >> libprep_changes_mfile 
              ln -s \${fil} ${libfolder}_pmid_${pmid}_supp/${libfolder}_\${supp}
              #Will be set to same if the one already in library is used (only keep basename)
              echo "path_original_supplementary=\${supp}" >> libprep_changes_mfile 
            done
          else
            # If empty then set missing (if supps exist but not in the dedicated library, then it has to be manually inserted there, and will be included in next batch update)
              echo "path_supplementary=missing" >> libprep_changes_mfile
              echo "path_original_supplementary=missing" >> libprep_changes_mfile
          fi
        else 
          i=1
          cat ${pdfsuppdir} | while read -r supp; do 
            if [ "\${supp}" != "missing" ]
            then
              supp2="\$(basename "\${supp}")" 
              extension="\${supp2##*.}" 
              echo "path_supplementary=${libfolder}_pmid_${pmid}_supp/${libfolder}_pmid_${pmid}_supp_\${i}.\${extension}" >> libprep_changes_mfile 
              ln -s ${params.libdirpdfs}/pmid_${pmid}_supp/pmid_${pmid}_supp_\${i}.\${extension} ${libfolder}_pmid_${pmid}_supp/${libfolder}_pmid_${pmid}_supp_\${i}.\${extension}
              echo "path_original_supplementary=\$supp2" >> libprep_changes_mfile 
              i=\$((i+1))
            else
              # Keep missing for path_supplementary and set missing to path_supplementary_original
              echo "path_supplementary=missing" >> libprep_changes_mfile
              echo "path_original_supplementary=missing" >> libprep_changes_mfile
            fi
          done
        fi
        
  
        # Apply changes when making the new_mfile
        cat ${mfile} > libprep_raw_mfile
        create_output_meta_data_file.sh libprep_raw_mfile libprep_changes_mfile ${cleanedheader} > libprep_new_mfile
        
        # Store data in library by copying (move is faster, but debug gets slower as input disappears)
        cp ${sclean} ${libfolder}_cleaned_GRCh38.gz
        cp ${scleanGRCh37} ${libfolder}_cleaned_GRCh37.gz
        cp ${removedlines} ${libfolder}_removed_lines.gz
        cp $softv ${libfolder}_software_versions.csv
        cp libprep_new_mfile ${libfolder}_new_meta.txt
  
        # Make a folder with detailed data of the cleaning
        mkdir ${libfolder}_cleaning_details
        cp $overviewworkflow ${libfolder}_cleaning_details/${libfolder}_stepwise_overview.txt
        cp ${removedlinestable} ${libfolder}_cleaning_details/${libfolder}_removed_lines_per_type_table.txt
        cp $gbdetect ${libfolder}_cleaning_details/${libfolder}_genome_build_map_count_table.txt
        
        
        """
    }
        //HERE symlink these instead of copying
        //cp $inputsfile ${libfolder}_raw.gz
        //if [ "${readme}" != "missing" ] ; then
        //  cp $readme ${libfolder}_raw_README.txt
        //fi
        //cp libprep_raw_mfile ${libfolder}_raw_meta.txt

        //I stopped using the one line summary of metadata, as it is very fast to compute when needed (2)
        //cp tmp_onelinemeta ${libfolder}_one_line_summary_of_metadata.txt
        
        //# Add link to the pdf and supplemental material
        //ln -s ${params.libdirpdfs}/pmid_${pmid}.pdf ${libfolder}_pmid_${pmid}.pdf
  
  
    process update_inventory_file {
        publishDir "${params.libdirinventory}", mode: 'copy', overwrite: false
  
        input:
        tuple datasetID, libfolder, mfile from ch_update_library_info_file
  
        output:
        path("*_inventory.txt")
  
        script:
        """
        # This is a little risky as two parallel flows in theory could enter this process at the same time
        # An idea for the future is to use a simple dbmanager (or use a lock file)
        # Or perhaps use a smarter channeling mixing the different files
        
        # make one_line_meta data for info file (now movbe to the inventory maker process)
        create_output_one_line_meta_data_file.sh mfile onelinemeta "${params.libdirinventory}"
       
        # Extract the most recent added row, except the header
        tail -n+2 onelinemeta | head -n1 > oneline
        dateOfCreation="\$(date +%F-%H%M%S-%N)"
  
        # Select most recent inventory file (if any exists)
        if [ -d "${params.libdirinventory}" ]
        then
          count="\$(ls -1 ${params.libdirinventory} | wc -l)"
          if [ "\${count}" -gt 0 ]
          then
            ls -1 ${params.libdirinventory}/*_inventory.txt | awk '{old=\$1; sub(".*/","",\$1); gsub("-","",\$1); print \$1, old}' | sort -rn -k1.1,1.23 | awk '{print \$2}' > libprep_sorted_inventory_files
            mostrecentfile="\$(head -n1 libprep_sorted_inventory_files)"
            cat \${mostrecentfile} oneline > \${dateOfCreation}_inventory.txt
          else
            # Make header if the file does not exist
            head -n1 onelinemeta > \${dateOfCreation}_inventory.txt
            cat oneline >> \${dateOfCreation}_inventory.txt 
          fi
        else
          # Make header if the file does not exist
          head -n1 onelinemeta > \${dateOfCreation}_inventory.txt
          cat oneline >> \${dateOfCreation}_inventory.txt 
        fi
        """
    }
  }
}

/*
 * Completion e-mail notification
 */
workflow.onComplete {

    // Set up the e-mail variables
    def subject = "[nf-core/cleansumstats] Successful: $workflow.runName"
    if (!workflow.success) {
      subject = "[nf-core/cleansumstats] FAILED: $workflow.runName"
    }
    def email_fields = [:]
    email_fields['version'] = workflow.manifest.version
    email_fields['runName'] = custom_runName ?: workflow.runName
    email_fields['success'] = workflow.success
    email_fields['dateComplete'] = workflow.complete
    email_fields['duration'] = workflow.duration
    email_fields['exitStatus'] = workflow.exitStatus
    email_fields['errorMessage'] = (workflow.errorMessage ?: 'None')
    email_fields['errorReport'] = (workflow.errorReport ?: 'None')
    email_fields['commandLine'] = workflow.commandLine
    email_fields['projectDir'] = workflow.projectDir
    email_fields['summary'] = summary
    email_fields['summary']['Date Started'] = workflow.start
    email_fields['summary']['Date Completed'] = workflow.complete
    email_fields['summary']['Pipeline script file path'] = workflow.scriptFile
    email_fields['summary']['Pipeline script hash ID'] = workflow.scriptId
    if (workflow.repository) email_fields['summary']['Pipeline repository Git URL'] = workflow.repository
    if (workflow.commitId) email_fields['summary']['Pipeline repository Git Commit'] = workflow.commitId
    if (workflow.revision) email_fields['summary']['Pipeline Git branch/tag'] = workflow.revision
    if (workflow.container) email_fields['summary']['Docker image'] = workflow.container
    email_fields['summary']['Nextflow Version'] = workflow.nextflow.version
    email_fields['summary']['Nextflow Build'] = workflow.nextflow.build
    email_fields['summary']['Nextflow Compile Timestamp'] = workflow.nextflow.timestamp

    // Check if we are only sending emails on failure
    email_address = params.email
    if (!params.email && params.email_on_fail && !workflow.success) {
        email_address = params.email_on_fail
    }

    // Render the TXT template
    def engine = new groovy.text.GStringTemplateEngine()
    def tf = new File("$baseDir/assets/email_template.txt")
    def txt_template = engine.createTemplate(tf).make(email_fields)
    def email_txt = txt_template.toString()

    // Render the HTML template
    def hf = new File("$baseDir/assets/email_template.html")
    def html_template = engine.createTemplate(hf).make(email_fields)
    def email_html = html_template.toString()

    // Render the sendmail template
    def smail_fields = [ email: email_address, subject: subject, email_txt: email_txt, email_html: email_html, baseDir: "$baseDir", mqcFile: mqc_report, mqcMaxSize: params.maxMultiqcEmailFileSize.toBytes() ]
    def sf = new File("$baseDir/assets/sendmail_template.txt")
    def sendmail_template = engine.createTemplate(sf).make(smail_fields)
    def sendmail_html = sendmail_template.toString()

    // Send the HTML e-mail
    if (email_address) {
        try {
          if ( params.plaintext_email ){ throw GroovyException('Send plaintext e-mail, not HTML') }
          // Try to send HTML e-mail using sendmail
          [ 'sendmail', '-t' ].execute() << sendmail_html
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (sendmail)"
        } catch (all) {
          // Catch failures and try with plaintext
          [ 'mail', '-s', subject, email_address ].execute() << email_txt
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (mail)"
        }
    }

    // Write summary e-mail HTML to a file
    def output_d = new File( "${params.outdir}/pipeline_info/" )
    if (!output_d.exists()) {
      output_d.mkdirs()
    }
    def output_hf = new File( output_d, "pipeline_report.html" )
    output_hf.withWriter { w -> w << email_html }
    def output_tf = new File( output_d, "pipeline_report.txt" )
    output_tf.withWriter { w -> w << email_txt }

    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_red = params.monochrome_logs ? '' : "\033[0;31m";

    if (workflow.stats.ignoredCount > 0 && workflow.success) {
      log.info "${c_purple}Warning, pipeline completed, but with errored process(es) ${c_reset}"
      log.info "${c_red}Number of ignored errored process(es) : ${workflow.stats.ignoredCount} ${c_reset}"
      log.info "${c_green}Number of successfully ran process(es) : ${workflow.stats.succeedCount} ${c_reset}"
    }

    if (workflow.success) {
        log.info "${c_purple}[nf-core/cleansumstats]${c_green} Pipeline completed successfully${c_reset}"
    } else {
        checkHostname()
        log.info "${c_purple}[nf-core/cleansumstats]${c_red} Pipeline completed with errors${c_reset}"
    }

}


def nfcoreHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
    ${c_blue}        ___     __   __   __   ___     ${c_green}/,-._.--~\'${c_reset}
    ${c_blue}  |\\ | |__  __ /  ` /  \\ |__) |__         ${c_yellow}}  {${c_reset}
    ${c_blue}  | \\| |       \\__, \\__/ |  \\ |___     ${c_green}\\`-._,-`-,${c_reset}
                                            ${c_green}`._,._,\'${c_reset}
    ${c_purple}  nf-core/cleansumstats v${workflow.manifest.version}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
