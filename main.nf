#!/usr/bin/env nextflow
// -*- mode: groovy; -*-
/*
vim: syntax=groovy
========================================================================================
                         nf-core/cleansumstats
========================================================================================
 nf-core/cleansumstats Analysis Pipeline.
 #### Homepage / Documentation
 https://github.com/nf-core/cleansumstats
----------------------------------------------------------------------------------------
*/

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run cleansumstats --input 'gwas_sumstats_meta_file.txt' -profile singularity

    Mandatory arguments:
      --input                       Path to metadata file in YAML format

    References:                     If not set here, it has to be specified in the configuration file
      --dbsnp_38                    Path to dbsnp GRCh38 reference
      --dbsnp_38_37                 Path to dbsnp GRCh38 to GRCh37 map reference
      --dbsnp_37_38                 Path to dbsnp GRCh37 to GRCh38 map reference
      --dbsnp_36_38                 Path to dbsnp GRCh36 to GRCh38 map reference
      --dbsnp_35_38                 Path to dbsnp GRCh35 to GRCh38 map reference
      --dbsnp_RSID_38               Path to dbsnp RSID to GRCh38 map reference

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Filtering:
      --beforeLiftoverFilter        A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_keys
                                    Example(default): --beforeLiftoverFilter duplicated_keys

      --afterLiftoverFilter         A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_refalt_in_GRCh37
                                      duplicated_chrpos_refalt_in_GRCh38
                                      duplicated_chrpos_in_GRCh37
                                      duplicated_chrpos_in_GRCh38
                                      multiallelics_in_dbsnp
                                    Example(default): --afterLiftoverFilter duplicated_chrpos_refalt_in_GRCh37,duplicated_chrpos_refalt_in_GRCh38,duplicated_chrpos_in_GRCh37,duplicated_chrpos_in_GRCh38,multiallelics_in_dbsnp


      --afterAlleleCorrectionFilter A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_in_GRCh37
                                    Example(default): --afterAlleleCorrectionFilter duplicated_chrpos_in_GRCh37

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

      --generateDbSNPreference      Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline
      --hg38ToHg19chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg18chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg17chain             chain file used for liftover (required for --generateDbSNPreference)

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    AWSBatch options:
      --awsqueue                    The AWSBatch JobQueue that needs to be set when running on AWSBatch
      --awsregion                   The AWS Region for your AWS Batch job to run on
    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// checker only
if(params.checkonly){
  doCompleteCleaningWorkflow = false
}else{
  doCompleteCleaningWorkflow = true
}

// check filter
beforeLiftoverFilter = params.beforeLiftoverFilter
afterLiftoverFilter = params.afterLiftoverFilter
afterAlleleCorrectionFilter = params.afterAlleleCorrectionFilter

// Set channels
if (params.generateDbSNPreference) {
  if (params.hg38ToHg19chain) { ch_hg38ToHg19chain = file(params.hg38ToHg19chain, checkIfExists: true) }
  if (params.hg19ToHg18chain) { ch_hg19ToHg18chain = file(params.hg19ToHg18chain, checkIfExists: true) }
  if (params.hg19ToHg17chain) { ch_hg19ToHg17chain = file(params.hg19ToHg17chain, checkIfExists: true) }

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38) }
}else {
  if (params.kg1000AFGRCh38) { ch_kg1000AFGRCh38 = file(params.kg1000AFGRCh38, checkIfExists: true) }

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }
}

ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)




// Stage config files
ch_multiqc_config = file(params.multiqc_config, checkIfExists: true)
ch_output_docs = file("$baseDir/docs/output.md", checkIfExists: true)

//example from nf-core how to use fasta
//params.fasta = params.genome ? params.genomes[ params.genome ].fasta ?: false : false
//if (params.fasta) { ch_fasta = file(params.fasta, checkIfExists: true) }

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}

if ( workflow.profile == 'awsbatch') {
  // AWSBatch sanity checking
  if (!params.awsqueue || !params.awsregion) exit 1, "Specify correct --awsqueue and --awsregion parameters on AWSBatch!"
  // Check outdir paths to be S3 buckets if running on AWSBatch
  // related: https://github.com/nextflow-io/nextflow/issues/813
  if (!params.outdir.startsWith('s3:')) exit 1, "Outdir not on S3 - specify S3 Bucket to run on AWSBatch!"
  // Prevent trace files to be stored on S3 since S3 does not support rolling files.
  if (workflow.tracedir.startsWith('s3:')) exit 1, "Specify a local tracedir or run without trace! S3 cannot be used for tracefiles."
}



// Header log info
log.info nfcoreHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
//if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38
//if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37
//if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36
//if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35
//if (params.dbsnpRSID) summary['dbsnpRSID'] = params.dbsnpRSID

summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
if (params.email || params.email_on_fail) {
  summary['E-mail Address']    = params.email
  summary['E-mail on failure'] = params.email_on_fail
  summary['MultiQC maxsize']   = params.maxMultiqcEmailFileSize
}
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'nf-core-cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}


process get_software_versions {
    publishDir "${params.outdir}/pipeline_info", mode: 'copy', overwrite: true, pattern: '*.csv'

    output:
    file 'software_versions_mqc.yaml' into software_versions_yaml
    file "software_versions.csv" into ch_software_versions


    script:
    """
    echo $workflow.manifest.version > v_pipeline.txt
    echo $workflow.nextflow.version > v_nextflow.txt
    #sstools-version > v_sumstattools.txt
    scrape_software_versions.py &> software_versions_mqc.yaml
    """
}

import dk.biopsyk.PipelineSession

def session = new PipelineSession<Metadata>(
  Metadata.class,
  baseDir,
  workflow.workDir,
  params.input
)

if (params.generateMetafile){
  session.metadata_paths.each {
    log.info "Writing metadata template"

    def metadata_id = it.getBaseName().toString()

    def template_file = new File("${params.outdir}/${metadata_id}.template.yaml")
    template_file.write(
      session.metadata_schema.generate_metadata_template()
    )

    log.info "Metadata template written to ${params.outdir}/${metadata_id}.template.yaml"
  }
}else if(params.generateMetaClass){
  log.info "Metadata class written to ${params.outdir}/Metadata.groovy"

  def class_file = new File("${params.outdir}/Metadata.groovy")
  class_file.write(
    session.metadata_schema.generate_metadata_groovy_class()
  )

  log.info "Metadata class written to ${params.outdir}/Metadata.groovy"
}else if(params.generateDbSNPreference){

  // ##Download from web
  // #wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.*

  ch_file = Channel
    .fromPath(params.input, type: 'file')
    .map { file -> tuple(file.baseName, file) }

  dbsnpsplits = 10

  process dbsnp_reference_convert_and_split {

      cpus 2

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple basefilename, dbsnpvcf from ch_file

      output:
      file("chunk_*") into ch_dbsnp_split
      file("dbsnp_GRCh38")

      script:
      """
      ##Download from web
      #wget ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.*
      module load tools
      module load pigz/2.3.4

      #reformat
      pigz --decompress --stdout --processes 2 ${dbsnpvcf} | grep -v "#" > dbsnp_GRCh38

      #split into dbsnpsplit number of unix split files
      split -dn ${dbsnpsplits} dbsnp_GRCh38 chunk_

      """
  }

  ch_dbsnp_split
    .flatten()
    .map { file -> tuple(file.baseName, file) }
    .set { ch_dbsnp_split2 }

  process dbsnp_reference_reformat {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_split2

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed") into ch_dbsnp_preformatted

      script:
      """
      awk '{print "chr"\$1, \$2, \$2,  \$1":"\$2, \$3, \$4, \$5}' ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed
      """
  }

  process dbsnp_reference_rm_indels {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_preformatted

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed.noindel") into ch_dbsnp_rmd_indels

      script:
      """
      # Remove all insertions or deletions
      # this will eliminate some rsids, both the ones with multiple rsids for the exact same snp, but also the ones with ref and alt switched.
      awk ' \$7 !~ /,/{if(length(\$6)!=1 || length(\$7)!=1 || \$6=="." || \$7=="."){print \$0 > "rm_indels"}else{print \$0}}; \$7 ~ /,/{if(\$7 ~ /\\w\\w/){print \$0 > "rm_indels2"}else{print \$0}} ' ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed.noindel
      """
  }

  ch_dbsnp_rmd_indels.into { ch_dbsnp_rmd_indels1; ch_dbsnp_rmd_indels2 }

  process dbsnp_reference_report_number_of_biallelic_multiallelics {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_indels1

      output:
      tuple cid, file("*") into ch_dbsnp_report_biallelic_mult_alleles

      script:
      """
      ## investigate the amount of single base multi allelics left (without filtering them out from the main workflow)
      awk ' \$7 ~ /,/{print \$0} ' ${dbsnp_chunk} > ${cid}_biallelic_multiallelics
      """
  }

  process dbsnp_reference_rm_dup_positions_GRCh38 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_indels2

      output:
      tuple cid, file("${cid}_All_20180418_GRCh38.bed.noindel.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh38
      file("${cid}_All_20180418_GRCh38.bed.noindel.sorted")

      script:
      """
      # Remove all duplicated positions GRCh38
      # this will eliminate some rsids, both the ones with multiple rsids for the exact same snp, but also the ones with ref and alt swithed.
      LC_ALL=C sort -k 4,4 --parallel 8 ${dbsnp_chunk} > ${cid}_All_20180418_GRCh38.bed.noindel.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh38"}; r0=var}' ${cid}_All_20180418_GRCh38.bed.noindel.sorted > ${cid}_All_20180418_GRCh38.bed.noindel.sorted.nodup

      """
  }

  ch_dbsnp_rmd_dup_positions_GRCh38.into { ch_dbsnp_rmd_dup_positions_GRCh38_1; ch_dbsnp_rmd_dup_positions_GRCh38_2; ch_dbsnp_rmd_dup_positions_GRCh38_3 }

  process dbsnp_reference_liftover_GRCh37 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh38_1

      output:
      tuple cid, file("${cid}_dbsnp_chunk_GRCh37_GRCh38") into ch_dbsnp_lifted_to_GRCh37
      file("${cid}_dbsnp_chunk_GRCh37")

      script:
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg38ToHg19chain} chain2.gz

      # Map to GRCh37
      CrossMap.py bed chain2.gz ${dbsnp_chunk} ${cid}_dbsnp_chunk_GRCh37
      awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print \$1, \$2, \$3, tmp":"\$2, \$4, \$5, \$6, \$7}' ${cid}_dbsnp_chunk_GRCh37 > ${cid}_dbsnp_chunk_GRCh37_GRCh38
      """
  }

  process dbsnp_reference_rm_dup_positions_GRCh37 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_lifted_to_GRCh37

      output:
      tuple cid, file("All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh37
      file("All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted")

      script:
      """
      # Remove all duplicated positions GRCh37 (as some positions might have become duplicates after the liftover)
      LC_ALL=C sort -k 4,4 --parallel 4 ${dbsnp_chunk} > All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh37"}; r0=var}' All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted > All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup
      """
  }

  process dbsnp_reference_rm_liftover_remaining_ambigous_GRCh37 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh37

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup.chromclean") into ch_dbsnp_rmd_ambig_GRCh37_liftovers
      file("${cid}_all_chr_types_GRCh37")

      script:
      """
      #To get a list of all chromosomes types
      awk '{gsub(":.*","",\$1); print \$1}' ${dbsnp_chunk} | awk '{ seen[\$1] += 1 } END { for (i in seen) print seen[i],i }' > ${cid}_all_chr_types_GRCh37

      #remove non standard chromosome names (seems like they include a "_" in the name)
      awk '{tmp=\$1; gsub(":.*","",\$1); if(\$1 !~ /_/ ){print tmp,\$2,\$3,\$4,\$5,\$6,\$7,\$8}}' ${dbsnp_chunk} > ${cid}_All_20180418_liftcoord_GRCh37_GRCh38.bed.sorted.nodup.chromclean
      """
  }

  ch_dbsnp_rmd_ambig_GRCh37_liftovers.into { ch_dbsnp_rmd_ambig_GRCh37_liftovers1; ch_dbsnp_rmd_ambig_GRCh37_liftovers2; ch_dbsnp_rmd_ambig_GRCh37_liftovers3 }

  process dbsnp_reference_liftover_GRCh36 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_ambig_GRCh37_liftovers1

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh36.bed"), build into ch_dbsnp_lifted_to_GRCh36

      script:
      build = "36"
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg19ToHg18chain} chain2.gz

      #liftover
      CrossMap.py bed chain2.gz ${dbsnp_chunk} ${cid}_All_20180418_liftcoord_GRCh36.bed
      """
  }

  process dbsnp_reference_liftover_GRCh35 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_ambig_GRCh37_liftovers2

      output:
      tuple cid, file("${cid}_All_20180418_liftcoord_GRCh35.bed"), build into ch_dbsnp_lifted_to_GRCh35

      script:
      build = "35"
      """
      #for some reason I have to copy the chain file to the wd for it to be found
      cp ${ch_hg19ToHg17chain} chain2.gz

      #liftover
      CrossMap.py bed chain2.gz ${dbsnp_chunk} ${cid}_All_20180418_liftcoord_GRCh35.bed
      """
  }

  ch_dbsnp_lifted_to_GRCh35
    .mix(ch_dbsnp_lifted_to_GRCh36)
    .set{ ch_dbsnp_lifted_to_GRCh3x }

  process dbsnp_reference_rm_duplicates_GRCh36_GRCh35 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk, build from ch_dbsnp_lifted_to_GRCh3x

      output:
      tuple build, cid, file("${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted.nodup") into ch_dbsnp_rmd_dup_positions_GRCh3x
      file("${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted")

      script:
      """
      # Remove all duplicated positions GRCh36 (as some positions might have become duplicates after the liftover)
      LC_ALL=C sort -k 4,4 --parallel 4 ${dbsnp_chunk} > ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted
      awk 'BEGIN{r0="initrowhere"} {var=\$4; if(r0!=var){print \$0}else{print \$0 > "removed_duplicated_rows_GRCh${build}"}; r0=var}' ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted > ${cid}_All_20180418_GRCh${build}_GRCh38.bed.sorted.nodup
      """
  }

  process dbsnp_reference_rm_liftover_remaining_ambigous_GRCh36_GRCh35 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple build, cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh3x

      output:
      tuple build, cid, file("${cid}_All_20180418_liftcoord_GRCh${build}_GRCh38.bed.sorted.nodup.chromclean") into ch_dbsnp_rmd_ambig_positions_GRCh3x
      file("${cid}_all_chr_types_GRCh${build}")

      script:
      """
      #To get a list of all chromosomes types
      awk '{gsub(":.*","",\$1); print \$1}' ${dbsnp_chunk} | awk '{ seen[\$1] += 1 } END { for (i in seen) print seen[i],i }' > ${cid}_all_chr_types_GRCh${build}

      #remove non standard chromosome names (seems like they include a "_" in the name)
      awk '{tmp=\$1; gsub(":.*","",\$1); if(\$1 !~ /_/ ){print tmp,\$2,\$3,\$4,\$5,\$6,\$7,\$8}}' ${dbsnp_chunk} > ${cid}_All_20180418_liftcoord_GRCh${build}_GRCh38.bed.sorted.nodup.chromclean
      """
  }

  process dbsnp_reference_make_rsid_version_from_GRCh38 {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple cid, dbsnp_chunk from ch_dbsnp_rmd_dup_positions_GRCh38_2

      output:
      file("${cid}_All_20180418_RSID_GRCh38.bed") into ch_dbsnp_rsid_to_GRCh38

      script:
      """
      # Make version sorted on RSID to get correct coordinates
      awk '{print \$5, \$4, \$6, \$7}' ${dbsnp_chunk} > ${cid}_All_20180418_RSID_GRCh38.bed
      """
  }

  process dbsnp_reference_merge_and_put_files_in_reference_library_RSID {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false, enabled: params.dev

      input:
      file dbsnp_chunks from ch_dbsnp_rsid_to_GRCh38.collect()

      output:
      file("${ch_dbsnp_RSID_38.baseName}.bed")

      script:
      """
      # Concatenate
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_RSID_GRCh38.map
      done

      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_RSID_GRCh38.map > "${ch_dbsnp_RSID_38.baseName}.bed"

      """
  }

//
// collect and STORE dbsnp files in reference library
//

  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh38 {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false, enabled: params.dev

      input:
      file dbsnp_chunks from ch_dbsnp_rmd_dup_positions_GRCh38_3.collect()

      output:
      file("${ch_dbsnp_38.baseName}.bed")

      script:
      """
      # Concatenate
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_RSID_GRCh38.map
      done

      # Sort
      awk '{print \$4,\$5,\$6,\$7}' All_20180418_RSID_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_38.baseName}.bed"

      """
  }


  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh38_GRCh37 {

      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false, enabled: params.dev
      publishDir "${params.outdir}", mode: 'rellink', overwrite: true, pattern: '*.map', enabled: params.dev

      input:
      file dbsnp_chunks from ch_dbsnp_rmd_ambig_GRCh37_liftovers3.collect()

      output:
      file("${ch_dbsnp_38_37.baseName}.bed")
      file("${ch_dbsnp_37_38.baseName}.bed")
      file("*map")

      script:
      """
      # Concatenate
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_GRCh38_GRCh37_tmp.map
      done

      awk '{print \$4, \$5, \$6, \$7, \$8}' All_20180418_GRCh38_GRCh37_tmp.map > All_20180418_GRCh37_GRCh38.map
      awk '{print \$5, \$4, \$6, \$7, \$8}' All_20180418_GRCh38_GRCh37_tmp.map > All_20180418_GRCh38_GRCh37.map

      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh38_GRCh37.map > "${ch_dbsnp_38_37.baseName}.bed"

      # Sort
      LC_ALL=C sort -k 1,1 --parallel 8 All_20180418_GRCh37_GRCh38.map > "${ch_dbsnp_37_38.baseName}.bed"

      """
  }

  ch_dbsnp_rmd_ambig_positions_GRCh3x_grouped = ch_dbsnp_rmd_ambig_positions_GRCh3x.groupTuple(by:0)

  process dbsnp_reference_merge_and_put_files_in_reference_library_GRCh3x_GRCh38 {

      publishDir "${params.outdir}/intermediates", mode: 'rellink', overwrite: true, pattern: '*.map', enabled: params.dev
      publishDir "${params.libdirdbsnp}", mode: 'copy', overwrite: false, pattern: '*.bed', enabled: params.dev

      input:
      tuple build, cid, file(dbsnp_chunks) from ch_dbsnp_rmd_ambig_positions_GRCh3x_grouped

      output:
      file("*.bed")
      file("*.map")

      script:
      """
      # Concatenate
      for chunk in ${dbsnp_chunks}
      do
        cat \${chunk} >> All_20180418_GRCh${build}_GRCh38.map
      done

      # Sort
      if [ "${build}" == "36" ]; then
        awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"\$2, \$5, \$6, \$7, \$8}' All_20180418_GRCh36_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_36_38.baseName}.bed"
      else
        awk '{tmp=\$1; sub(/[cC][hH][rR]/, "", tmp); print tmp":"\$2, \$5, \$6, \$7, \$8}' All_20180418_GRCh35_GRCh38.map | LC_ALL=C sort -k 1,1 --parallel 8 > "${ch_dbsnp_35_38.baseName}.bed"
      fi

      """
  }
}else if(params.generate1KgAfSNPreference){

  // ##Download from web
  // ##Download readme describing the new mapping directly to GRCh38
  // ##wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/20190312_biallelic_SNV_and_INDEL_README.txt
  // ##
  // ###then download the datasets from this website portal
  // ##wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz
  // ##wget http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.wgs.shapeit2_integrated_snvindels_v2a.GRCh38.27022019.sites.vcf.gz.tbi
  // ##

  ch_file = Channel
    .fromPath(params.input, type: 'file')
    .map { file -> tuple(file.baseName, file) }

  process extract_frequency_data {

      publishDir "${params.outdir}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple basefilename, af1kgvcf from ch_file

      output:
      tuple basefilename, file("1kg_af_ref") into ch_1kg_af_ref

      script:
      """
      module load tools
      module load bcftools/1.11
      gendb_1kaf_extract_freq_data.sh ${af1kgvcf} > 1kg_af_ref

      """
  }

  // As 1KG by default shows alternative allele frequency, we flip to follow our default on showing effect allele frequency, which in our system will be the reference allele frequency.
  process flip_frequency_data {

      publishDir "${params.outdir}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple basefilename, ref1kg from ch_1kg_af_ref

      output:
      tuple basefilename, file("1kg_af_ref.flipped") into ch_1kg_af_ref_tosort

      script:
      """
      #sort 1kg af reference on position
      awk '{print \$1, \$2, \$3, 1-\$4, 1-\$5, 1-\$6, 1-\$7, 1-\$8}' ${ref1kg} > 1kg_af_ref.flipped
      """
  }

  process sort_frequency_data {

      cpus 4

      publishDir "${params.outdir}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple basefilename, ref1kg from ch_1kg_af_ref_tosort

      output:
      tuple basefilename, file("1kg_af_ref.sorted") into ch_1kg_af_ref_sorted

      script:
      """
      #sort 1kg af reference on position
      LC_ALL=C sort -k 1,1 --parallel 4 ${ref1kg} > 1kg_af_ref.sorted
      """
  }

  process join_frequency_data_on_dbsnp_reference {

      publishDir "${params.outdir}", mode: 'rellink', overwrite: true

      input:
      tuple basefilename, ref1kgsorted from ch_1kg_af_ref_sorted

      output:
      tuple basefilename, file("1kg_af_ref.sorted.joined")

      script:
      """
      #join the two datasets
      LC_ALL=C join -1 1 -2 1 ${ref1kgsorted} ${ch_dbsnp_38} > 1kg_af_ref.sorted.joined

      """
  }
      //#check for not agreeing ref alleles and alt alleles
      // awk '{if($2!=$10){print $0}}' 1kg_af_ref.sorted.joined | head
      // awk '{if($3!=$11){print $0}}' 1kg_af_ref.sorted.joined | head

}else {

  //=================================================================================
  // Pre-execution validation
  //=================================================================================

  log.info("Reading metadata files")

  session.read_metadata_files()

  log.info("All metadata files read")
  log.info("Validating pipeline parameters")

  ParametersValidator.validate_filters_allowed(
    "before",
    params.beforeLiftoverFilter,
    "${baseDir}/assets/allowed_names_beforeLiftoverFilter.txt"
  )

  ParametersValidator.validate_filters_allowed(
    "after",
    params.afterLiftoverFilter,
    "${baseDir}/assets/allowed_names_afterLiftoverFilter.txt"
  )

  log.info("All pipeline parameters validated")

  //=================================================================================
  // Start of execution
  //=================================================================================

  //use metafile filename as datasetID through the pipeline
  ch_mfile_checkX = Channel
                  .fromPath(params.input, type: 'file')
                  .map { file -> tuple(file.baseName, file) }

  ch_mfile_checkX.into { ch_mfile_user_1; ch_mfile_user_2; ch_mfile_user_3; }

  process calculate_checksum_on_metafile_input {
      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, mfile from ch_mfile_user_3

      output:
      tuple datasetID, env(usermetachecksum) into ch_usermeta_checksum
      file("calculate_checksum_on_metafile_input__input_meta_checksum.txt")

      script:
      """
      b3sum ${mfile} | awk '{print \$1}' > calculate_checksum_on_metafile_input__input_meta_checksum.txt
      usermetachecksum="\$(cat calculate_checksum_on_metafile_input__input_meta_checksum.txt)"
      """
  }

  process make_metafile_unix_friendly {
      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, mfile from ch_mfile_user_1

      output:
      tuple datasetID, mfile, file("make_metafile_unix_friendly__mfile_unix_safe") into ch_mfile_unix_safe

      script:
      """
      dos2unix -n ${mfile} make_metafile_unix_friendly__mfile_unix_safe
      """
  }

  process check_crucial_paths {
    publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

    input:
    tuple datasetID, mfile, mfile_unix_safe from ch_mfile_unix_safe

    output:
    tuple datasetID, mfile_unix_safe, env(spath) into ch_mfile_check_format
    tuple datasetID, env(spath) into ch_input_sfile
    tuple datasetID, env(rpath) into ch_input_readme
    tuple datasetID, env(pmid) into ch_pmid
    tuple datasetID, env(pmid), env(pdfpath), file("check_crucial_paths__pdf_suppfiles.txt") into ch_input_pdf_stuff

    script:
    def metadata = session.get_metadata(datasetID)
    def supplementary_echoes = metadata.path_supplementary.collect {
      "echo '${it}' >> 'check_crucial_paths__pdf_suppfiles.txt'"
    }

    """

    spath="${metadata.path_sumStats}"
    rpath="${metadata.path_readMe == null ? "missing" : metadata.path_readMe}"
    pdfpath="${metadata.path_pdf == null ? "missing" : metadata.path_pdf}"
    pmid="${metadata.study_PMID}"

    touch check_crucial_paths__pdf_suppfiles.txt
    ${supplementary_echoes.join("\n")}
    """
  }

  ch_input_sfile.into { ch_input_sfile1; ch_input_sfile2 }

  process calculate_checksum_on_sumstat_input {
      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, sfile from ch_input_sfile1

      output:
      tuple datasetID, env(rawsumstatchecksum) into ch_rawsumstat_checksum
      path("calculate_checksum_on_sumstat_input__input_sumstat_checksum")

      script:
      """
      b3sum ${sfile} | awk '{print \$1}' > calculate_checksum_on_sumstat_input__input_sumstat_checksum
      rawsumstatchecksum="\$(cat 'calculate_checksum_on_sumstat_input__input_sumstat_checksum')"
      """
  }

  // Force into the right format if possible
  process check_sumstat_format {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, mfile, sfilePath from ch_mfile_check_format

      output:
      tuple datasetID, mfile into ch_mfile_ok
      tuple datasetID, file("check_sumstat_format__sumstat_file") into ch_sfile_ok
      tuple datasetID, file("check_sumstat_format__desc_force_tab_sep_BA.txt") into ch_desc_prep_force_tab_sep_BA
      path("check_sumstat_format__sumstat_1000_rows")
      path("check_sumstat_format__sumstat_1000_rows_formatted")
      path("*.log")

      script:
      """
      # Sumstat file check on first 1000 lines
      echo "\$(head -n 1000 < <(zcat ${sfilePath}))" | gzip -c > check_sumstat_format__sumstat_1000_rows
      check_and_format_sfile.sh check_sumstat_format__sumstat_1000_rows check_sumstat_format__sumstat_1000_rows_formatted check_sumstat_format__sumstat_1000_rows_formatted.log

      # Make second sumstat file check on all lines
      check_and_format_sfile.sh ${sfilePath} check_sumstat_format__sumstat_file check_sumstat_format__sumstat_file.log

      # Process before and after stats (the -1 is to remove the header count)
      rowsBefore="\$(zcat ${sfilePath} | wc -l | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l check_sumstat_format__sumstat_file | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tForce tab separation" > check_sumstat_format__desc_force_tab_sep_BA.txt
      """
  }

if (doCompleteCleaningWorkflow){

    process add_index_sumstat {

        input:
        tuple datasetID, sfile from ch_sfile_ok

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        output:
        tuple datasetID, file("add_index_sumstat__added_rowindex_sumstat_file") into ch_sfile_on_stream
        tuple datasetID, file("add_index_sumstat__desc_before_after") into ch_desc_prep_add_sorted_rowindex_BA
        val("$datasetID") into ch_init_does_chrpos_exist

        script:
        """
        cat $sfile | sstools-raw add-index > add_index_sumstat__added_rowindex_sumstat_file

        #process before and after stats (the -1 is to remove the header count)
        rowsBefore="\$(wc -l $sfile | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l add_index_sumstat__added_rowindex_sumstat_file | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tAdd rowindex, which maps back to the unfiltered file" > add_index_sumstat__desc_before_after
        """
    }

    ch_mfile_ok.into { ch_mfile_ok1; ch_mfile_ok2; ch_mfile_ok3; ch_mfile_ok4; ch_mfile_ok5; ch_mfile_rerun_7; ch_mfile_ok_8}

    ch_sfile_on_stream.into { ch_sfile_on_stream1; ch_sfile_on_stream2; ch_sfile_on_stream4; ch_sfile_on_stream5; ch_before_liftover }
    ch_mfile_and_stream=ch_mfile_ok1.join(ch_sfile_on_stream1)
    ch_mfile_and_stream.into { ch_check_gb; ch_liftover; ch_liftover1; ch_liftover2; ch_stats_inference }

    process does_chrpos_exist {

      input:
      val datasetID from ch_init_does_chrpos_exist

      output:
      tuple datasetID, env(CHRPOSexists),env(SNPexists),env(pointsToDifferent) into ch_present_markers

      script:
      def metadata = session.get_metadata(datasetID)

      """
      pointsToDifferent=${!metadata.chrpos_points_to_snp()}
      CHRPOSexists=${metadata.chrpos_exists()}
      SNPexists=${metadata.col_SNP != null}
      """
    }

   ch_present_markersX=ch_liftover1.combine(ch_present_markers, by: 0)
   ch_present_markersX.into { ch_present_markers_1; ch_present_markers_2 }

    process is_chrpos_different_from_snp_and_assign_dID2 {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, mfile, sfile, chrposExists, snpExists, pointsToDifferentCols from ch_present_markers_2

        output:
        tuple datasetID, env(dID2), mfile, file("is_chrpos_different_from_snp_and_assign_dID2__prep_chrpos"), snpExists into ch_chrpos_init

        script:
        """
        dID2="liftover_branch_chrpos"

        if [ "${chrposExists}" == "true" ] && [ "${pointsToDifferentCols}" == "true" ]
        then
          cp ${sfile} is_chrpos_different_from_snp_and_assign_dID2__prep_chrpos
        else
          head -n1 ${sfile} > is_chrpos_different_from_snp_and_assign_dID2__prep_chrpos
        fi

        """
    }

    // LIFTOVER BRANCH 1

    process prepare_dbsnp_mapping_for_rsid {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, mfile, sfile, chrposExists, snpExists, pointsToDifferentCols from ch_present_markers_1

        output:
        tuple datasetID, mfile, file("prepare_dbsnp_mapping_for_rsid__db_maplift"), snpExists into ch_liftover_33
        tuple datasetID, env(dID2), mfile, file("prepare_dbsnp_mapping_for_rsid__gb_lift2"), snpExists into ch_liftover_snpchrpos
        //tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos_rsid
        //tuple datasetID, file("desc_sex_chrom_formatting_BA.txt") into ch_desc_sex_chrom_formatting_BA_1

        script:
        """
        dID2="liftover_branch_markername_chrpos"

        echo -e "0\tRSID" > prepare_dbsnp_mapping_for_rsid__db_maplift
        echo -e "0\tMarkername" > prepare_dbsnp_mapping_for_rsid__gb_lift2

        if [ "${snpExists}" == "true" ]
        then
          Sx="\$(grep "^col_SNP:" $mfile)"
          colSNP="\$(echo "\${Sx#*: }")"
          # Select columns and then split in one rs file and one snpchrpos file
          cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colSNP}" -n"0,RSID" | awk -vFS="\t" -vOFS="\t" '{print \$2,\$1}' | awk -vFS="\t" -vOFS="\t" 'NR>1{if(\$1 ~ /^rs.*/){ print \$0 }else{ print \$0 >> "prepare_dbsnp_mapping_for_rsid__gb_lift2" }}' >> prepare_dbsnp_mapping_for_rsid__db_maplift
        fi
        # Use the empty header data to continue with, which should make this branch quick

        # Process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l prepare_dbsnp_mapping_for_rsid__db_maplift | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
        """
    }

    process remove_duplicated_rsid_before_liftmap {

        publishDir "${params.outdir}/${datasetID}/intermediates/liftover_branch_markername_rsid", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/liftover_branch_markername_rsid/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, mfile, rsidprep, snpExists from ch_liftover_33

        output:
        tuple datasetID, mfile, file("remove_duplicated_rsid_before_liftmap__gb_unique_rows"), snpExists into ch_liftover_3333
        //tuple datasetID, file("remove_duplicated_rsid_before_liftmap__desc_removed_duplicated_rows") into ch_removed_rows_before_liftover_rsids
        tuple datasetID, file("remove_duplicated_rsid_before_liftmap__removed_duplicated_rows") into ch_removed_rows_before_liftover_ix_rsids
        file("remove_duplicated_rsid_before_liftmap__beforeLiftoverFiltering_executionorder")

        script:
        """
        if [ "${snpExists}" == "true" ]
        then
          filter_before_liftover.sh $rsidprep ${beforeLiftoverFilter} "remove_duplicated_rsid_before_liftmap__"
        else
          # Make empty file (should not have header)
          touch remove_duplicated_rsid_before_liftmap__removed_duplicated_rows
          touch remove_duplicated_rsid_before_liftmap__gb_unique_rows
          touch remove_duplicated_rsid_before_liftmap__beforeLiftoverFiltering_executionorder
        fi

        """
    }


    process maplift_dbsnp_GRCh38_rsid {

        publishDir "${params.outdir}/${datasetID}/intermediates/liftover_branch_markername_rsid", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/liftover_branch_markername_rsid/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, mfile, fsorted, snpExists from ch_liftover_3333

        output:
        tuple datasetID, mfile, file("maplift_dbsnp_GRCh38_rsid__gb_lifted_and_mapped_to_GRCh38") into ch_liftover_rsid
        //tuple datasetID, file("desc_liftover_to_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_rsid
        //tuple datasetID, file("${datasetID}.stats") into ch_stats_genome_build_rsid
        tuple datasetID, file("maplift_dbsnp_GRCh38_rsid__removed_not_matching_during_liftover_ix") into ch_not_matching_during_liftover_rsid

        script:
        """

        if [ "${snpExists}" == "true" ]
        then
          #in gb_lifted_and_mapped_to_GRCh38, the order will be
          #GRCh38, GRCh37, rowIndex, RSID, REF, ALT
          #chr:pos | inx | rsid | a1 | a2 | chr:pos2 (if available)
          LC_ALL=C join -1 1 -2 1 ${fsorted} ${ch_dbsnp_RSID_38} | awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$1,\$4,\$5}'  > maplift_dbsnp_GRCh38_rsid__gb_lifted_and_mapped_to_GRCh38

          # Lines not possible to map
          LC_ALL=C join -v 1 -1 1 -2 3 ${fsorted} maplift_dbsnp_GRCh38_rsid__gb_lifted_and_mapped_to_GRCh38 > maplift_dbsnp_GRCh38_rsid__removed_not_matching_during_liftover
          awk -vOFS="\t" '{print \$2,"not_matching_during_liftover"}' maplift_dbsnp_GRCh38_rsid__removed_not_matching_during_liftover > maplift_dbsnp_GRCh38_rsid__removed_not_matching_during_liftover_ix
        else
          #make empty file (has no header)
          touch maplift_dbsnp_GRCh38_rsid__gb_lifted_and_mapped_to_GRCh38
          touch maplift_dbsnp_GRCh38_rsid__removed_not_matching_during_liftover_ix
        fi


        # Process before and after stats
        rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l maplift_dbsnp_GRCh38_rsid__gb_lifted_and_mapped_to_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh38_and_map_to_dbsnp_BA

        """
    }


    // LIFTOVER BRANCH 2 - chrpos mapping

    //mix the snpchrpos (from markername column) with chrpos (from own columns)
    ch_chrpos_init
      .mix(ch_liftover_snpchrpos)
      .set{ ch_liftover_snpchrpos_chrpos_mixed }

    //reformat_X_Y_XY_and_MT_and_remove_noninterpretables
    process reformat_chromosome_information {
      publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, dID2, mfile, sfile, chrposexist from ch_liftover_snpchrpos_chrpos_mixed

      output:
      tuple datasetID, dID2, mfile, file("reformat_chromosome_information__prep_sfile_forced_sex_chromosome_format") into ch_chromosome_fixed
     // path("new_chr_sex_format")
     // path("new_chr_sex_format2")
     // path("new_chr_sex_format3")
      //tuple datasetID, file("desc_sex_chrom_formatting_BA.txt") into ch_desc_sex_chrom_formatting_BA_2
      tuple datasetID, env(rowsAfter) into ch_rowsAfter_number_of_lines

      script:
      def metadata = session.get_metadata(datasetID)

      """
      colCHR="${metadata.col_CHR ?: "missing"}"

      # Check number of rows in file
      nrrows="\$(wc -l ${sfile})"

      # If only header row, then do nothing
      if [ "\${nrrows}" == "1" ]
      then
        # Will just forward the header, as the header should be the only thing present if this is true
        cat ${sfile}  > prep_sfile_forced_sex_chromosome_format
      else
        cat $sfile | sstools-utils ad-hoc-do -k "0|funx_force_sex_chromosomes_format(\${colCHR})" -n"0,\${colCHR}" > new_chr_sex_format

        # Remove sex formats of unknown origin
        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        echo "\${colCHR}" > gb_ad-hoc-do_funx_CHR_sex_chrom_filter
        cat new_chr_sex_format | sstools-utils ad-hoc-do -k "0|\${colCHR}" -n"0,CHR" | awk -vFS="\t" -vOFS="\t" 'BEGIN{getline; print \$0}; {if(\$2 > 0 && \$2 < 27){ print \$1, \$2 }}' > new_chr_sex_format2
        #use the index to remove everything no part of chr numers 1-26 but keep original format
        LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 -1 1 -2 1 new_chr_sex_format new_chr_sex_format2 > new_chr_sex_format3

        # Replace (if bp or allele info is in the same column it will be kept, as the function above only replaces the chr info part)
        head -n1 $sfile > header
        to_keep_from_join="\$(awk -vFS="\t" -vobj=\${colCHR} '{for (i=1; i<=NF; i++){if(obj==\$i){print "2."2}else{print "1."i}}}' header)"
        LC_ALL=C join -t "\$(printf '\t')" -o \${to_keep_from_join} -1 1 -2 1 $sfile new_chr_sex_format3 > reformat_chromosome_information__prep_sfile_forced_sex_chromosome_format
      fi

      # Process before and after stats (the -1 is to remove the header count)
      rowsBefore="\$(wc -l $sfile | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l reformat_chromosome_information__prep_sfile_forced_sex_chromosome_format | awk '{print \$1-1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tforced sex chromosomes and mitochondria chr annotation to the numbers 23-26" > desc_sex_chrom_formatting_BA.txt
      """
    }

    ch_chromosome_fixed.into {ch_chromosome_fixed1; ch_chromosome_fixed2}

    whichbuild = ['GRCh35', 'GRCh36', 'GRCh37', 'GRCh38']

    process detect_genome_build {

        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, dID2, mfile, sfile from ch_chromosome_fixed1
        each build from whichbuild

        output:
        tuple datasetID, dID2, file("detect_genome_build__*.res") into ch_genome_build_stats
        //file("gb_*")

        script:
        """

        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")
        echo "\${colCHR}" > gb_ad-hoc-do_funx_CHR_${build}
        echo "\${colPOS}" > gb_ad-hoc-do_funx_POS_${build}

        #check number of rows in file
        nrrows="\$(wc -l ${sfile})"
        #if only header row, then do nothing
        if [ "\${nrrows}" == "1" ]
        then
          #I here choose to set number of mapped to 0, as nothing has been mapped.
          echo -e "0\t${build}" > ${datasetID}.${build}.res
        else
          cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" > gb_extract_and_format_chr_and_pos_to_detect_build_${build}
          awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' gb_extract_and_format_chr_and_pos_to_detect_build_${build} > gb_ready_to_join_to_detect_build_${build}
          LC_ALL=C sort -k1,1 gb_ready_to_join_to_detect_build_${build} > gb_ready_to_join_to_detect_build_sorted_${build}
          format_chrpos_for_dbsnp.sh ${build} gb_ready_to_join_to_detect_build_sorted_${build} ${ch_dbsnp_35_38} ${ch_dbsnp_36_38} ${ch_dbsnp_37_38} ${ch_dbsnp_38} > ${build}.map
          sort -u -k1,1 ${build}.map | wc -l | awk -vOFS="\t" -vbuild=${build} '{print \$1,build}' > detect_genome_build__${build}.res
        fi

        """
    }


    ch_genome_build_stats_grouped = ch_genome_build_stats.groupTuple(by:[0,1],size:4)

    process decide_genome_build {

        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, dID2, file(ujoins) from ch_genome_build_stats_grouped


        output:
        tuple datasetID, dID2, env(GRChmax) into ch_known_genome_build
        tuple datasetID, dID2, file("decide_genome_build__stats") into ch_stats_genome_build_chrpos
        tuple datasetID, dID2, file("decide_genome_build__GRChOther"), env(GRChmaxVal) into ch_build_stats_for_failsafe
        path("decide_genome_build__GRChmax")

        script:
        """
        for gbuild in ${ujoins}
        do
            cat \$gbuild >> decide_genome_build__stats
        done
        GRChmax="\$(cat decide_genome_build__stats | sort -nr -k1,1 | head -n1 | awk '{print \$2}')"
        GRChmaxVal="\$(cat decide_genome_build__stats | sort -nr -k1,1 | head -n1 | awk '{print \$1}')"

        cat decide_genome_build__stats | sort -nr -k1,1 | tail -n+2 > decide_genome_build__GRChOther
        echo \${GRChmax} > decide_genome_build__GRChmax

        """
    }


    ch_rowsAfter_number_of_lines
      .combine(ch_build_stats_for_failsafe, by: 0)
      .set{ ch_failsafe }

    process build_warning {

      publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, tot, dID2, buildstat, grmax from ch_failsafe

        output:
        tuple datasetID, file("warningsFile") into ch_warning_liftover

        script:
        """
        #make empty warningsfile
        touch warningsFile

        #if tot is not 0
        if [ "${tot}" == "0" ]; then
          :
        else
          #check if anything should be added to the warningsfile
          warnings_liftover_percentage.sh ${grmax} ${tot} ${buildstat} ${dID2} >> warningsFile
        fi
        """
    }

    // Add respective sumstat file from the parallell paths
    ch_liftover_2=ch_known_genome_build.combine(ch_chromosome_fixed2, by: [0,1])

    process sort_by_chrpos_before_maplift {
        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, dID2, gbmax, mfile, sfile from ch_liftover_2

        output:
        tuple datasetID, dID2, mfile, file("sort_by_chrpos_before_maplift__gb_lift"), gbmax into ch_liftover_3
        //tuple datasetID, file("desc_prepare_format_for_dbsnp_mapping_BA.txt") into ch_desc_prep_for_dbsnp_mapping_BA_chrpos

        script:
        """

        colCHR=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "chr")
        colPOS=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "bp")

        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colCHR}|\${colPOS}" -n"0,CHR,BP" | awk -vFS="\t" -vOFS="\t" '{print \$2":"\$3,\$1}' > sort_by_chrpos_before_maplift__gb_lift

        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l sort_by_chrpos_before_maplift__gb_lift | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tPrepare file for mapping to dbsnp by sorting the mapping index" > desc_prepare_format_for_dbsnp_mapping_BA.txt
        """

    }


    process rm_dup_chrpos_before_maplift {

        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, dID2, mfile, chrposprep, gbmax from ch_liftover_3

        output:
        tuple datasetID, dID2, mfile, file("rm_dup_chrpos_before_maplift__gb_unique_rows"), gbmax into ch_liftover_333
        //tuple datasetID, file("desc_removed_duplicated_rows") into ch_removed_rows_before_liftover_chrpos
        tuple datasetID, file("rm_dup_chrpos_before_maplift__removed_duplicated_rows") into ch_removed_rows_before_liftover_ix_chrpos
        file("rm_dup_chrpos_before_maplift__beforeLiftoverFiltering_executionorder")

        script:
        """
        filter_before_liftover.sh $chrposprep ${beforeLiftoverFilter} "rm_dup_chrpos_before_maplift__"

        """
    }



  process maplift_dbsnp_GRCh38_chrpos {

        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/${dID2}/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, dID2, mfile, fsorted, gbmax from ch_liftover_333

        output:
        tuple datasetID, dID2, mfile, file("maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38") into ch_liftover_44
        //tuple datasetID, file("desc_liftover_to_GRCh38_and_map_to_dbsnp_BA") into ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA_chrpos
        tuple datasetID, file("maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover_ix") into ch_not_matching_during_liftover_chrpos
        //file("removed_*")
        file("maplift_dbsnp_GRCh38_chrpos__lifted_middle_step*")

        script:
        """

        #check number of rows in file
        nrrows="\$(wc -l ${fsorted})"
        #if only header row, then do nothing
        if [ "\${nrrows}" == "1" ]
        then
          #I here choose to set number of mapped to 0, as nothing has been mapped. This file does not have a header.
          touch maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38

          #nothing should be in here
          touch maplift_dbsnp_GRCh38_chrpos__lifted_middle_step

          #as the this subset of the data is empty, we have to make this file empty as well
          # even though it would have been more logical to fill it with all lines from the original sfile
          touch maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover
          touch maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover_ix
        else

          #in gb_lifted_and_mapped_to_GRCh37_and_GRCh38, the order will be
          #GRCh38, GRCh37, rowIndex, RSID, REF, ALT
          #chr:pos | inx | rsid | a1 | a2 | chr:pos2 (if available)
          if [ "${gbmax}" == "GRCh38" ] ; then
            LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_38} > maplift_dbsnp_GRCh38_chrpos__lifted_middle_step
            awk -vFS="[[:space:]]" -vOFS="\t" '{print \$1,\$2,\$3,\$4,\$5}' maplift_dbsnp_GRCh38_chrpos__lifted_middle_step > maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38
          elif [ "${gbmax}" == "GRCh37" ] ; then
            LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_37_38} > maplift_dbsnp_GRCh38_chrpos__lifted_middle_step
            awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' maplift_dbsnp_GRCh38_chrpos__lifted_middle_step > maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38
          elif [ "${gbmax}" == "GRCh36" ] ; then
            LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_36_38} > maplift_dbsnp_GRCh38_chrpos__lifted_middle_step
            awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' maplift_dbsnp_GRCh38_chrpos__lifted_middle_step > maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38
          elif [ "${gbmax}" == "GRCh35" ] ; then
            LC_ALL=C join -1 1 -2 1 $fsorted ${ch_dbsnp_35_38} > maplift_dbsnp_GRCh38_chrpos__lifted_middle_step
            awk -vFS="[[:space:]]" -vOFS="\t" '{print \$3,\$2,\$4,\$5,\$6}' maplift_dbsnp_GRCh38_chrpos__lifted_middle_step > maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38
          else
            echo "${gbmax} is none of the available builds 35, 36, 37 or 38"
          fi


          # Lines not possible to map
          LC_ALL=C join -v 1 -1 1 -2 1 ${fsorted} maplift_dbsnp_GRCh38_chrpos__lifted_middle_step > maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover
          awk -vOFS="\t" '{print \$2,"not_matching_during_liftover"}' maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover > maplift_dbsnp_GRCh38_chrpos__removed_not_matching_during_liftover_ix

        fi

        #process before and after stats
        rowsBefore="\$(wc -l ${fsorted} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l maplift_dbsnp_GRCh38_chrpos__gb_lifted_and_mapped_to_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tLiftover to GRCh38 and simultaneously map to dbsnp" > desc_liftover_to_GRCh38_and_map_to_dbsnp_BA
        """
    }

    //branch the chrpos and snpchrpos channels
    ch_chrpos_snp_filter=ch_liftover_44.branch { key, value, mfile, liftedGRCh38 ->
                    liftover_branch_markername_chrpos: value == "liftover_branch_markername_chrpos"
                    liftover_branch_chrpos: value == "liftover_branch_chrpos"
                    }
    ch_chrpos=ch_chrpos_snp_filter.liftover_branch_chrpos
    ch_snpchrpos=ch_chrpos_snp_filter.liftover_branch_markername_chrpos

    //join the chrpos and snpchrpos channels
    ch_chrpos
      .join(ch_snpchrpos, by: 0)
      .join(ch_liftover_rsid, by: 0)
      .join(ch_before_liftover, by: 0)
      .set{ ch_combined_chrpos_snpchrpos_rsid }

process select_chrpos_or_snpchrpos {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, dID2, mfile, liftedGRCh38, dID2SNP, mfileSNP, liftedGRCh38SNP, mfileRSID, liftedGRCh38RSID, beforeLiftover from ch_combined_chrpos_snpchrpos_rsid

      output:
      tuple datasetID, mfile, file("select_chrpos_or_snpchrpos__combined_set_from_the_three_liftover_branches_sorted") into ch_liftover_final
      tuple datasetID, file("select_chrpos_or_snpchrpos__beforeAndAfterFile") into ch_desc_combined_set_after_liftover
      tuple datasetID, file("select_chrpos_or_snpchrpos__removed_not_possible_to_lift_over_for_combined_set_ix") into ch_removed_not_possible_to_lift_over_for_combined_set_ix
     // file("liftedGRCh38_sorted")
     // file("rsid_to_add")
     // file("snpchrpos_unique")
     // file("snpchrpos_to_add")
     // file("tmp_test")

      script:
      """
      cp ${beforeLiftover} tmp_test
      #any row inx from rsid or snpchrpos not in chrpos
      LC_ALL=C sort -k2,2 ${liftedGRCh38} > liftedGRCh38_sorted
      LC_ALL=C sort -k2,2 ${liftedGRCh38RSID} > liftedGRCh38RSID_sorted
      LC_ALL=C sort -k2,2 ${liftedGRCh38SNP} > liftedGRCh38SNP_sorted
      LC_ALL=C join -t "\$(printf '\t')" -v 1 -1 2 -2 2 -o 1.1 1.2 1.3 1.4 1.5 liftedGRCh38RSID_sorted liftedGRCh38_sorted > rsid_to_add
      LC_ALL=C join -t "\$(printf '\t')" -v 1 -1 2 -2 2 -o 1.1 1.2 1.3 1.4 1.5 liftedGRCh38SNP_sorted liftedGRCh38_sorted > snpchrpos_unique
      LC_ALL=C join -t "\$(printf '\t')" -v 1 -1 2 -2 2 -o 1.1 1.2 1.3 1.4 1.5 snpchrpos_unique rsid_to_add > snpchrpos_to_add

      #if so, then add it to the output
      cat liftedGRCh38_sorted rsid_to_add snpchrpos_to_add > combined_set_from_the_three_liftover_branches
      LC_ALL=C sort -k2,2 combined_set_from_the_three_liftover_branches > select_chrpos_or_snpchrpos__combined_set_from_the_three_liftover_branches_sorted

      # Lines not possible to map for the combined set
      LC_ALL=C join -t "\$(printf '\t')" -v 1 -1 2 -2 1 -o 2.1 select_chrpos_or_snpchrpos__combined_set_from_the_three_liftover_branches_sorted ${beforeLiftover} > select_chrpos_or_snpchrpos__removed_not_possible_to_lift_over_for_combined_set
      awk -vOFS="\t" '{print \$1,"not_available_for_any_of_the_three_liftover_branches"}' select_chrpos_or_snpchrpos__removed_not_possible_to_lift_over_for_combined_set > select_chrpos_or_snpchrpos__removed_not_possible_to_lift_over_for_combined_set_ix

      #process before and after stats
      rowsBefore="\$(wc -l ${beforeLiftover} | awk '{print \$1-1}')"
      rowsAfter="\$(wc -l select_chrpos_or_snpchrpos__combined_set_from_the_three_liftover_branches_sorted | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tAfter creating the combined set from the three liftover paths" > select_chrpos_or_snpchrpos__beforeAndAfterFile
      """
}

    //branch the stats_genome_build
    ch_stats_genome_build_filter=ch_stats_genome_build_chrpos.branch { key, value, file ->
                    liftover_branch_markername_chrpos: value == "liftover_branch_markername_chrpos"
                    liftover_branch_chrpos: value == "liftover_branch_chrpos"
                    }
    ch_stats_chrpos_gb=ch_stats_genome_build_filter.liftover_branch_chrpos
    ch_stats_snpchrpos_gb=ch_stats_genome_build_filter.liftover_branch_markername_chrpos

    //combine the chrpos and snpchrpos channels for genome build
    ch_stats_chrpos_gb
      .join(ch_stats_snpchrpos_gb, by: 0)
      .map { key, val, file, val2, file2 -> tuple(key, file, file2) }
      .set{ ch_gb_stats_combined }


    process rm_dup_chrpos_allele_rows {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, mfile, liftedandmapped from ch_liftover_final

        output:
        tuple datasetID, mfile, file("rm_dup_chrpos_allele_rows__gb_unique_rows_sorted") into ch_liftover_4
        tuple datasetID, file("rm_dup_chrpos_allele_rows__desc_removed_duplicated_rows") into ch_desc_removed_duplicates_after_liftover
        tuple datasetID, file("rm_dup_chrpos_allele_rows__removed_duplicated_rows") into ch_removed_duplicates_after_liftover_ix
        //file("removed_*")
        //file("afterLiftoverFiltering_executionorder")

        script:
        """
        filter_after_liftover.sh $liftedandmapped "${afterLiftoverFilter} " "rm_dup_chrpos_allele_rows__"

        """
    }


    process reformat_sumstat {
        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, mfile, liftedandmapped from ch_liftover_4

        output:
        tuple datasetID, val("GRCh38"), mfile, file("reformat_sumstat__gb_lifted_GRCh38") into ch_mapped_GRCh38
        tuple datasetID, file("reformat_sumstat__desc_keep_a_GRCh38_reference_BA.txt") into ch_desc_keep_a_GRCh38_reference_BA

        script:
        """
        #prepare GRCh38 for downstream analysis
        awk -vFS="[[:space:]]" -vOFS="\t" '{print \$2,\$1,\$3,\$4,\$5}' $liftedandmapped > reformat_sumstat__gb_lifted_GRCh38

        #process before and after stats
        rowsBefore="\$(wc -l $liftedandmapped | awk '{print \$1}')"
        rowsAfter="\$(wc -l reformat_sumstat__gb_lifted_GRCh38 | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tSplit off a version of GRCh38 as coordinate reference" > reformat_sumstat__desc_keep_a_GRCh38_reference_BA.txt
        """
    }


    process split_multiallelics_resort_rowindex {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, build, mfile, liftgrs from ch_mapped_GRCh38

        output:
        tuple datasetID, build, mfile, file("split_multiallelics_resort_rowindex__gb_multialleles_splittorows") into ch_allele_correction
        tuple datasetID, file("split_multiallelics_resort_rowindex__desc_split_multi_allelics_and_sort_on_rowindex_BA.txt") into ch_desc_split_multi_allelics_and_sort_on_rowindex_BA
        file("split_multiallelics_resort_rowindex__gb_splitted_multiallelics")

        script:
        """
        split_multiallelics_to_rows.sh $liftgrs > split_multiallelics_resort_rowindex__gb_splitted_multiallelics
        echo -e "0\tCHRPOS\tRSID\tA1\tA2" > split_multiallelics_resort_rowindex__gb_multialleles_splittorows
        LC_ALL=C sort -k1,1 split_multiallelics_resort_rowindex__gb_splitted_multiallelics >> split_multiallelics_resort_rowindex__gb_multialleles_splittorows

        #process before and after stats (rows is -1 because of header)
        rowsBefore="\$(wc -l $liftgrs | awk '{print \$1}')"
        rowsAfter="\$(wc -l split_multiallelics_resort_rowindex__gb_multialleles_splittorows | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tSplit multi-allelics to multiple rows and sort on original rowindex " > split_multiallelics_resort_rowindex__desc_split_multi_allelics_and_sort_on_rowindex_BA.txt

        """
    }

    ch_allele_correction_combine=ch_allele_correction.combine(ch_sfile_on_stream2, by: 0)
    ch_allele_correction_combine.into{ ch_allele_correction_combine1; ch_allele_correction_combine2 }

    process does_exist_A2 {

        input:
        tuple datasetID, mfile from ch_mfile_ok2

        output:
        tuple datasetID, env(A2exists) into ch_present_A2

        script:
        """
        A2exists=\$(doesA2exist.sh $mfile)
        """
    }

    //Create filter for when A2 exists or not
    ch_present_A2_br=ch_present_A2.branch { key, value ->
                    A2exists: value == "true"
                    A2missing: value == "false"
                    }

    //split the channels based on filter
    ch_present_A2_br2=ch_present_A2_br.A2exists
    ch_present_A2_br3=ch_present_A2_br.A2missing

    //combine each channel with the matching datasetID
    ch_A2_exists=ch_allele_correction_combine1.combine(ch_present_A2_br2, by: 0)
    ch_A2_missing=ch_allele_correction_combine2.combine(ch_present_A2_br3, by: 0)

    process allele_correction_A1_A2 {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, build, mfile, mapped, sfile, A2exists from ch_A2_exists

        output:
        tuple datasetID, build, mfile, file("allele_correction_A1_A2__acorrected") into ch_A2_exists2
        tuple datasetID, file("allele_correction_A1_A2__removed_allele_filter_ix") into ch_removed_by_allele_filter_ix1
        tuple datasetID, file("allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA

        script:
        """
        echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > allele_correction_A1_A2__acorrected

        #init some the files collecting variants removed because of allele composition
        touch removed_notGCTA
        touch removed_indel
        touch removed_hom
        touch removed_palin
        touch removed_notPossPair
        touch removed_notExpA2

        colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
        colA2=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "altallele")
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}|\${colA2}" -n"0,A1,A2" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 1.3 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${mapped} | tail -n+2 | sstools-eallele correction -f - >> allele_correction_A1_A2__acorrected

        #only keep the index to prepare for the file with all removed lines
        touch allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notGCTA"}' removed_notGCTA >> allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"indel"}' removed_indel >> allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"hom"}' removed_hom >> allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"palin"}' removed_palin >> allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notPossPair"}' removed_notPossPair >> allele_correction_A1_A2__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notExpA2"}' removed_notExpA2 >> allele_correction_A1_A2__removed_allele_filter_ix

        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes. Usually a substantial amount." >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l allele_correction_A1_A2__acorrected | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tAllele corretion sanity check that final filtered file before and after file have same row count" >> allele_correction_A1_A2__desc_filtered_allele-pairs_with_dbsnp_as_reference
        """
    }

    process allele_correction_A1 {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
        publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

        input:
        tuple datasetID, build, mfile, mapped, sfile, A2missing from ch_A2_missing

        output:
        tuple datasetID, build, mfile, file("allele_correction_A1__acorrected") into ch_A2_missing2
        tuple datasetID, file("allele_correction_A1__removed_allele_filter_ix") into ch_removed_by_allele_filter_ix2
        tuple datasetID, file("allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference") into ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA
        file("allele_correction_A1__mapped2")

        script:
        """

        #NOTE to use A1 allele only complicates the filtering on possible pairs etc, so we always need a multiallelic filter in how the filter works right now.
        # This is something we should try to accomodate to, so that it is not required.
        multiallelic_filter.sh $mapped > allele_correction_A1__mapped2
        echo -e "0\tA1\tA2\tCHRPOS\tRSID\tEffectAllele\tOtherAllele\tEMOD" > allele_correction_A1__acorrected

        #init some the files collecting variants removed because of allele composition
        touch removed_notGCTA
        touch removed_indel
        touch removed_hom
        touch removed_palin
        touch removed_notPossPair
        touch removed_notExpA2

        colA1=\$(map_to_adhoc_function.sh ${ch_regexp_lexicon} ${mfile} ${sfile} "effallele")
        cat ${sfile} | sstools-utils ad-hoc-do -k "0|\${colA1}" -n"0,A1" | LC_ALL=C join -t "\$(printf '\t')" -o 1.1 1.2 2.2 2.3 2.4 2.5 -1 1 -2 1 - ${build}_mapped2 | tail -n+2 | sstools-eallele correction -f - -a >> allele_correction_A1__acorrected

        #only keep the index to prepare for the file with all removed lines
        touch allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notGCTA"}' removed_notGCTA >> allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"indel"}' removed_indel >> allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"hom"}' removed_hom >> allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"palin"}' removed_palin >> allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notPossPair"}' removed_notPossPair >> allele_correction_A1__removed_allele_filter_ix
        awk -vOFS="\t" '{print \$1,"notExpA2"}' removed_notExpA2 >> allele_correction_A1__removed_allele_filter_ix

        #process before and after stats (create one for each discarded filter, the original before after concept where all output files are directly tested is a bit violated here as we have to count down from input file)
        rowsBefore="\$(wc -l ${mapped} | awk '{print \$1-1}')"
        rowsAfter="\$(wc -l removed_notGCTA | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on nonGTAC characters" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_indel | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on indels. All indels in the dbsnp reference are already filtered out" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_hom | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on homozygotes. Should be rare." >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_palin | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on palindromes" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notPossPair | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not possible pair combinations comparing with reference db. Many multi-allelic sites are filtered out here" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l removed_notExpA2 | awk -vrb=\${rowsBefore} '{ra=rb-\$1; print ra}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFiltered rows on not expected otherAllele in reference db" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        rowsBefore="\${rowsAfter}"
        rowsAfter="\$(wc -l allele_correction_A1__acorrected | awk '{print \$1-1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tsanity sanity check that final filtered file before and after file have same row count" >> allele_correction_A1__desc_filtered_allele-pairs_with_dbsnp_as_reference

        """
    }

    //put the two branches into the same channel (as only one will be used per file, there will be no duplicates)
    ch_removed_by_allele_filter_ix1
      .mix(ch_removed_by_allele_filter_ix2)
      .set{ ch_removed_by_allele_filter_ix }


    ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1A2_BA
      .mix(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_A1_BA)
      .set{ ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA }


    //mix the A1_A2_both and A1_solo channels
    ch_A2_exists2
      .mix(ch_A2_missing2)
      .set{ ch_allele_corrected_mix_X }

    process rm_dup_chrpos_rows_after_acor {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, build, mfile, acorrected from ch_allele_corrected_mix_X

        output:
        tuple datasetID, build, mfile, file("rm_dup_chrpos_rows_after_acor__ac_unique_rows_sorted") into ch_allele_corrected_mix_Y
        tuple datasetID, file("rm_dup_chrpos_rows_after_acor__desc_removed_duplicated_rows") into ch_desc_removed_duplicated_chr_pos_rows_BA
        //file("ac_*")
        //file("afterAlleleCorrection_executionorder")
        //file("removed_*")

        script:
        """

        #Can be used as a sanitycheck-filter to discover potential misbehaviour
        # Added white space after "afterAlleleCorrectionFilter" as if it is empty, then the third argument will go one step forward
        filter_after_allele_correction.sh ${acorrected} "${afterAlleleCorrectionFilter} " "rm_dup_chrpos_rows_after_acor__"

        """
    }
    ch_allele_corrected_mix_Y
      .into{ ch_allele_corrected_mix1; ch_allele_corrected_mix2; ch_allele_corrected_mix3 }

    process numeric_filter_stats {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
      publishDir "${params.outdir}/${datasetID}/intermediates/removed_lines", mode: 'rellink', overwrite: true, pattern: 'removed_*', enabled: params.dev

      input:
      tuple datasetID, mfile, sfile from ch_stats_inference

      output:
      tuple datasetID, mfile, file("numeric_filter_stats__st_filtered_remains") into ch_stats_filtered_remain00
      tuple datasetID, file("numeric_filter_stats__removed_stat_non_numeric_in_awk_ix") into ch_stats_filtered_removed_ix
      tuple datasetID, file("numeric_filter_stats__desc_filtered_stat_rows_with_non_numbers_BA.txt") into ch_desc_filtered_stat_rows_with_non_numbers_BA
      file("numeric_filter_stats__removed_stat_non_numeric_in_awk")

      script:
      def metadata = session.get_metadata(datasetID)
      Map stat_fields = metadata.resolve_stat_fields()
      int se_column_id = -1

      stat_fields.eachWithIndex { entry, i ->
        if (entry.key == "SE") {
          se_column_id = i
        }
      }

      """
      if [[ \$(wc -l $sfile | awk '{print \$1}') == "1" ]]
      then
        echo "[ERROR] The inputted file sfile did not have any data"
        exit 1
      fi

      touch numeric_filter_stats__removed_stat_non_numeric_in_awk
      touch numeric_filter_stats__removed_stat_non_numeric_in_awk_ix

      sstools-utils ad-hoc-do -f $sfile \
        -k "0|${stat_fields.values().join("|")}" \
        -n "0,${stat_fields.values().join(",")}" | \
        filter_stat_values_awk.sh -vzeroSE="${se_column_id}" \
          > numeric_filter_stats__st_filtered_remains 2> numeric_filter_stats__removed_stat_non_numeric_in_awk

      awk -vOFS="\t" '{print \$1,"stat_non_numeric_in_awk"}' numeric_filter_stats__removed_stat_non_numeric_in_awk > numeric_filter_stats__removed_stat_non_numeric_in_awk_ix

      if [[ \$(wc -l numeric_filter_stats__st_filtered_remains | awk '{print \$1}') == "1" ]]
      then
        echo "[ERROR] The outputted file st_filtered_remains did not have any data"
        exit 1
      fi

      #process before and after stats
      rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
      rowsAfter="\$(wc -l numeric_filter_stats__st_filtered_remains | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFiltered out rows with stats impossible to do calculations from" > numeric_filter_stats__desc_filtered_stat_rows_with_non_numbers_BA.txt
      """
    }

    process force_eaf {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, mfile, sfile from ch_stats_filtered_remain00

        output:
        tuple datasetID, file("force_eaf__st_forced_eaf") into ch_stats_filtered_remain
        tuple datasetID, file("force_eaf__desc_forced_eaf_BA.txt") into ch_desc_forced_eaf_BA

        script:
        """
        if [[ \$(wc -l $sfile | awk '{print \$1}') == "1" ]]
        then
          echo "[ERROR] The inputted file sfile did not have any data"
          exit 1
        fi

        force_effect_allele_frequency.sh $mfile $sfile > force_eaf__st_forced_eaf

        if [[ \$(wc -l st_forced_eaf | awk '{print \$1}') == "1" ]]
        then
          echo "[ERROR] The outputted file st_forced_eaf did not have any data"
          exit 1
        fi

        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
        rowsAfter="\$(wc -l force_eaf__st_forced_eaf | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tForced Effect Allele Frequency" > force_eaf__desc_forced_eaf_BA.txt
        """
    }

    ch_stats_filtered_remain
      .join(ch_mfile_ok5, by: 0)
      .set{ ch_stats_filtered_remain3 }




    process prep_af_stats {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, build, mfile, sfile from ch_allele_corrected_mix3

        output:
        tuple datasetID, env(avail), file("prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx") into ch_prep_ref_allele_frequency
        file("prep_af_stats__st_1kg_af_ref_sorted")
        file("prep_af_stats__st_1kg_af_ref_sorted_joined")

        script:
        """
        # Check if the ancestry is one of the ones we have frequencies for (EAS, EUR, AFR, AMR, SAS)
        Ax="\$(grep "^study_Ancestry:" $mfile)"
        A="\$(echo "\${Ax#*: }")"

        avail="false"
        count=0
        # Important that this order is the same as in the allele frequency file
        for anc in EAS EUR AFR AMR SAS; do
          if [ "\${anc}" == "\${A}" ]; then
            avail="true"
          fi
          count=\$((count+1))
        done

        # If we have an available ancestry reference frequency
        if [ \${avail} == "true" ]; then
          # Join with AF table using chrpos column (keep only rowindex and allele frequency, merge later)
          awk -vFS="\t" -vOFS="\t" '{print \$4"-"\$2"-"\$3, \$1}' ${sfile} | C_ALL=C sort -t "\$(printf '\t')" -k 1,1 > prep_af_stats__st_1kg_af_ref_sorted
          awk -vFS="\t" -vOFS="\t" -vcount=\${count} '{print \$1"-"\$2"-"\$3,\$count}' ${ch_kg1000AFGRCh38} | LC_ALL=C join -1 1 -2 1 -t "\$(printf '\t')" -o 2.2 1.2 - prep_af_stats__st_1kg_af_ref_sorted > prep_af_stats__st_1kg_af_ref_sorted_joined
          echo -e "0\tAF_1KG_CS" > prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
          LC_ALL=C sort -k 1,1 prep_af_stats__st_1kg_af_ref_sorted_joined >> prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
        else
          touch prep_af_stats__st_1kg_af_ref_sorted
          touch prep_af_stats__st_1kg_af_ref_sorted_joined
          touch prep_af_stats__st_1kg_af_ref_sorted_joined_sorted_on_inx
        fi

        """
    }

    ch_stats_filtered_remain3
    .join(ch_prep_ref_allele_frequency, by: 0)
    .set { ch_add_ref_freq }


    //if ancestry code (eg EUR)is available, add allele_frequency
    process add_af_stats {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, st_filtered, mfile, availAF, afFreqs from ch_add_ref_freq

        output:
        tuple datasetID, val("g1kaf_stats_branch"), file("add_af_stats__st_added_1kg_ref"), mfile into ch_added_ref_allele_frequency_kg
        tuple datasetID, val("default_stats_branch"), file("add_af_stats__st_added_1kg_ref"), mfile into ch_added_ref_allele_frequency_default

        script:
        """
        if [[ \$(wc -l $st_filtered | awk '{print \$1}') == "1" ]]
        then
          echo "[ERROR] The inputted file st_filtered did not have any data"
          exit 1
        fi

        # If we have an available ancestry reference frequency
        if [ "${availAF}" == "true" ]; then
          # Join with AF table using chrpos column add NA for missing fields
         LC_ALL=C join -e "NA" -t "\$(printf '\t')" -a 1 -1 1 -2 1 -o auto ${st_filtered} ${afFreqs} > add_af_stats__st_added_1kg_ref
        else
          head -n1 ${st_filtered} > add_af_stats__st_added_1kg_ref
        fi
        """
    }
    //re-merge these stats in the select_stats process
    ch_added_ref_allele_frequency_default.into{ ch_added_ref_allele_frequency_default1; ch_added_ref_allele_frequency_default2 }

    //mix and run inference for 1kg-AF version and for a version without
    ch_added_ref_allele_frequency_kg
      .mix(ch_added_ref_allele_frequency_default1)
      .set{ ch_stats_to_infer }

    process infer_stats {

        publishDir "${params.outdir}/${datasetID}/intermediates/${af_branch}", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, af_branch, st_filtered, mfile from ch_stats_to_infer

        output:
        tuple datasetID, af_branch, mfile, file("infer_stats__st_inferred_stats") into ch_stats_selection
        tuple datasetID, file("infer_stats__desc_inferred_stats_if_inferred_BA.txt") into ch_desc_inferred_stats_if_inferred_BA
       // file("st_which_to_infer")
       // file("colfields")
       // file("colnames")
       // file("colpositions")

        script:
        """
        if [[ \$(wc -l $st_filtered | awk '{print \$1}') == "1" ]]
        then
          echo "[ERROR] The inputted file st_filtered did not have any data"
          exit 1
        fi

        check_stat_inference_functionfile.sh ${mfile} $af_branch > st_which_to_infer
        check_stat_inference_avail.sh $mfile colfields colnames colpositions $af_branch

        cf="\$(cat colfields)"
        cn="\$(cat colnames)"
        cp="\$(cat colpositions)"

        if [ -s st_which_to_infer ]; then

        thisdir="\$(pwd)"

        cat $st_filtered | sstools-utils ad-hoc-do -f - -k "\${cf}" -n"\${cn}" | r-stats-c-streamer --functionfile st_which_to_infer --skiplines 1 \${cp} --statmodel lin --allelefreqswitch > infer_stats__st_inferred_stats

        else
          touch infer_stats__st_inferred_stats
        fi

        #process before and after stats
        rowsBefore="\$(wc -l ${st_filtered} | awk '{print \$1}')"
        rowsAfter="\$(wc -l infer_stats__st_inferred_stats | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tInferred stats, if stats are inferred" > infer_stats__desc_inferred_stats_if_inferred_BA.txt
        """
    }


    //branch the stats_genome_build
    ch_stats_selection_filter=ch_stats_selection.branch { key, value, file1, file2 ->
                    g1kaf_stats_branch: value == "g1kaf_stats_branch"
                    default_stats_branch: value == "default_stats_branch"
                    }
    g1kaf_stats_branch=ch_stats_selection_filter.g1kaf_stats_branch
    default_stats_branch=ch_stats_selection_filter.default_stats_branch

    //combine the 1kg af branch and default branch for inferred information
    g1kaf_stats_branch
      .join(default_stats_branch, by: 0)
      .map { key, val, mfile, file, val2, mfile2, file2 -> tuple(key, mfile, file, file2) }
      .set{ ch_inferred_stats_combined }

    process merge_inferred_data {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, mfile, kgversion, defaultversion from ch_inferred_stats_combined

      output:
      tuple datasetID, file("merge_inferred_data__st_combined_set_of_inferred_data") into ch_combined_set_of_inferred_data
      file("merge_inferred_data__st_added_suffix")

      script:
      """
      # Add _1KG to all 1KG inferred variables
      add_suffix_to_colnames.sh $kgversion "_1KG" > merge_inferred_data__st_added_suffix

      # Merge the data add NA for missing fields
      LC_ALL=C join -e "NA" -t "\$(printf '\t')" -a 1 -1 1 -2 1 -o auto $defaultversion merge_inferred_data__st_added_suffix > merge_inferred_data__st_combined_set_of_inferred_data
      """

    }


    //ch_stats_selection_only_contains_inferred_variables
    ch_combined_set_of_inferred_data
      .join(ch_added_ref_allele_frequency_default2, by: 0)
      .set{ ch_stats_selection2 }

    process select_stats {

        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, inferred, stats_branch, sfile, mfile from ch_stats_selection2

        output:
        tuple datasetID, file("select_stats__st_stats_for_output") into ch_stats_for_output
        tuple datasetID, file("select_stats__desc_from_inferred_to_joined_selection_BA.txt") into ch_desc_from_inferred_to_joined_selection_BA
        tuple datasetID, file("select_stats__desc_from_sumstats_to_joined_selection_BA.txt") into ch_desc_from_sumstats_to_joined_selection_BA

        script:
        """
        select_stats_for_output.sh $mfile $sfile $inferred > select_stats__st_stats_for_output

        #process before and after stats
        rowsBefore="\$(wc -l ${inferred} | awk '{print \$1}')"
        rowsAfter="\$(wc -l select_stats__st_stats_for_output | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFrom inferred to joined selection of stats" > select_stats__desc_from_inferred_to_joined_selection_BA.txt

        #process before and after stats
        rowsBefore="\$(wc -l ${sfile} | awk '{print \$1}')"
        rowsAfter="\$(wc -l select_stats__st_stats_for_output | awk '{print \$1}')"
        echo -e "\$rowsBefore\t\$rowsAfter\tFrom raw sumstat to joined selection of stats" > select_stats__desc_from_sumstats_to_joined_selection_BA.txt
        """
    }



    ch_allele_corrected_mix1
      .combine(ch_stats_for_output, by: 0)
      .set{ ch_allele_corrected_and_outstats }

  process final_assembly {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, build, mfile, acorrected, stats from ch_allele_corrected_and_outstats

      output:
      tuple datasetID, file("final_assembly__cleaned2"), file("final_assembly__header") into ch_cleaned_file_1
      tuple datasetID, file("final_assembly__desc_final_merge_BA.txt") into ch_desc_final_merge_BA

      script:
      """
      apply_modifier_on_stats.sh $acorrected $stats > final_assembly__cleaned

      #sort on chrpos (which will make header not on top, so lift that out, and prepare order for next process)
      head -n1 final_assembly__cleaned | awk -vFS="\t" -vOFS="\t" '{printf "%s%s%s%s%s%s", \$2, OFS, \$3, OFS, \$1, OFS; for(i=4; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' > final_assembly__header
      awk -vFS="\t" -vOFS="\t" 'NR>1{printf "%s%s%s%s", \$2":"\$3, OFS, \$1, OFS; for(i=4; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' final_assembly__cleaned | LC_ALL=C sort -k1,1 > final_assembly__cleaned2

      # process before and after stats
      rowsBefore="\$(wc -l $acorrected | awk '{print \$1}')"
      rowsAfter="\$(wc -l final_assembly__cleaned2 | awk '{print \$1}')"
      echo -e "\$rowsBefore\t\$rowsAfter\tFrom dbsnp mapped to merged selection of stats, final step" > final_assembly__desc_final_merge_BA.txt
      """
  }

  process prep_GRCh37_coord {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, cleaned_chrpos_sorted, header from ch_cleaned_file_1

      output:
      tuple datasetID, file("prep_GRCh37_coord__cleaned_chrpos_sorted_header"), file("prep_GRCh37_coord__cleaned_GRCh37") into ch_cleaned_file
      //file("cleaned_chrpos_sorted")
      //file("inx_chrpos_GRCh37")

      script:

      """
      echo -e "CHR\tPOS\tRSID" > prep_GRCh37_coord__cleaned_GRCh37
      LC_ALL=C join -e "NA" -a1 -1 1 -2 1 -o 2.1 2.2 2.3 ${cleaned_chrpos_sorted} ${ch_dbsnp_38_37} | awk -vOFS="\t" '{split(\$2,out,":"); print out[1], out[2],\$3 }' >> prep_GRCh37_coord__cleaned_GRCh37
      cat $header > prep_GRCh37_coord__cleaned_chrpos_sorted_header
      awk -vFS="\t" -vOFS="\t" '{split(\$1,out,":");printf "%s%s%s%s", out[1], OFS, out[2], OFS; for(i=2; i<=NF-1; i++){printf "%s%s", \$i, OFS}; print \$NF}' $cleaned_chrpos_sorted >> prep_GRCh37_coord__cleaned_chrpos_sorted_header
      """

  }

    //Collect and place in corresponding stepwise order
    ch_removed_not_possible_to_lift_over_for_combined_set_ix
     .join(ch_removed_by_allele_filter_ix, by: 0)
     .join(ch_stats_filtered_removed_ix, by: 0)
     .set{ ch_collected_removed_lines }

    process collect_rmd_lines {
        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, step1, step2, step3 from ch_collected_removed_lines

        output:
        tuple datasetID, file("collect_rmd_lines__removed_lines_collected.txt") into ch_collected_removed_lines2

        script:
        """
        echo -e "RowIndex\tExclusionReason" > collect_rmd_lines__removed_lines_collected.txt
        cat ${step1} ${step2} ${step3} >> collect_rmd_lines__removed_lines_collected.txt
        """
    }

    ch_collected_removed_lines2
      .into { ch_collected_removed_lines3; ch_collected_removed_lines4 }

    process desc_rmd_lines_as_table {

      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, filtered_stats_removed from ch_collected_removed_lines3

        output:
        tuple datasetID, file("desc_rmd_lines_as_table__desc_removed_lines_table.txt") into ch_removed_lines_table

        script:
        """
        # prepare process specific descriptive statistics
        echo -e "NrExcludedRows\tExclusionReason" > desc_rmd_lines_as_table__desc_removed_lines_table.txt
        cat $filtered_stats_removed | tail -n+2 | awk -vOFS="\t" '{ seen[\$2] += 1 } END { for (i in seen) print seen[i],i }' >> desc_rmd_lines_as_table__desc_removed_lines_table.txt

        """
    }


    ch_cleaned_file
      .combine(ch_input_sfile2, by: 0)
      .combine(ch_sfile_on_stream5, by: 0)
      .combine(ch_collected_removed_lines4, by: 0)
      .set{ ch_to_write_to_filelibrary2 }

    process gzip_outfiles {
        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, sclean, scleanGRCh37, inputsfile, inputformatted, removedlines from ch_to_write_to_filelibrary2

        output:
        tuple datasetID, path("gzip_outfiles__sclean.gz"), path("gzip_outfiles__scleanGRCh37.gz"), path("gzip_outfiles__removed_lines.gz") into ch_to_write_to_filelibrary3
        tuple datasetID, path("gzip_outfiles__cleanedheader") into ch_cleaned_header
        tuple datasetID, inputsfile into ch_to_write_to_raw_library
        val datasetID into ch_check_avail

        script:
        """
        # Make a header file to use when deciding on what cols are present for the new meta file
        head -n1 ${sclean} > gzip_outfiles__cleanedheader

        # Store data in library
        gzip -c ${sclean} > gzip_outfiles__sclean.gz
        gzip -c ${scleanGRCh37} > gzip_outfiles__scleanGRCh37.gz
        gzip -c ${removedlines} > gzip_outfiles__removed_lines.gz
        """
    }

    ch_to_write_to_filelibrary3.into { ch_to_write_to_filelibrary3a; ch_to_write_to_filelibrary3b }

    process calculate_checksum_on_sumstat_cleaned {
        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, sclean, scleanGRCh37, removedlines from ch_to_write_to_filelibrary3a

        output:
        tuple datasetID, env(scleanchecksum), env(scleanGRCh37checksum), env(removedlineschecksum) into ch_cleaned_sumstat_checksums

        script:
        """
        scleanchecksum="\$(b3sum ${sclean} | awk '{print \$1}')"
        scleanGRCh37checksum="\$(b3sum ${scleanGRCh37} | awk '{print \$1}')"
        removedlineschecksum="\$(b3sum ${removedlines} | awk '{print \$1}')"
        """
    }

  ch_cleaned_sumstat_checksums.into { ch_cleaned_sumstat_checksums1; ch_cleaned_sumstat_checksums2 }


  //Do actual collection, placed in corresponding step order
  ch_desc_prep_force_tab_sep_BA
   .combine(ch_desc_prep_add_sorted_rowindex_BA, by: 0)
   .combine(ch_desc_combined_set_after_liftover, by: 0)
   .combine(ch_desc_removed_duplicates_after_liftover, by: 0)
   .combine(ch_desc_keep_a_GRCh38_reference_BA, by: 0)
   .combine(ch_desc_split_multi_allelics_and_sort_on_rowindex_BA, by: 0)
   .combine(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA, by: 0)
   .combine(ch_desc_removed_duplicated_chr_pos_rows_BA, by: 0)
   .combine(ch_desc_filtered_stat_rows_with_non_numbers_BA, by: 0)
   .combine(ch_desc_inferred_stats_if_inferred_BA, by: 0)
   .combine(ch_desc_from_inferred_to_joined_selection_BA, by: 0)
   .combine(ch_desc_from_sumstats_to_joined_selection_BA, by: 0)
   .combine(ch_desc_final_merge_BA, by: 0)
   .set{ ch_collected_workflow_stepwise_stats }

 //Some that now are part of the branched workflow. Unclear how to save the the stepwise branched workflow before after steps, but all info should be exported in channels.
 //  .combine(ch_desc_sex_chrom_formatting_BA, by: 0)
 //  .combine(ch_desc_prep_for_dbsnp_mapping_BA, by: 0)
 //  .combine(ch_removed_rows_before_liftover, by: 0)
 //  .combine(ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA, by: 0)

  process collect_and_prep_stepwise_readme {
      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

      input:
      tuple datasetID, step1, step2, step3, step4, step5, step6, step7, step8, step9, step10, step11, step12, step13 from ch_collected_workflow_stepwise_stats

      output:
      tuple datasetID, file("collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt") into ch_overview_workflow_steps

      script:
      """
      cat $step1 $step2 $step3 $step4 $step5 $step6 $step7 $step8 $step9 $step10 $step11 $step12 $step13 > all_removed_steps

      echo -e "Steps\tBefore\tAfter\tDescription" > collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt
      awk -vFS="\t" -vOFS="\t" '{print "Step"NR, \$1, \$2, \$3}' all_removed_steps >> collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt

      """
  }

    ch_mfile_ok4
      .combine(ch_usermeta_checksum, by: 0)
      .combine(ch_rawsumstat_checksum, by: 0)
      .combine(ch_cleaned_sumstat_checksums2, by: 0)
      .combine(ch_cleaned_header, by: 0)
      .set { ch_mfile_cleaned_x }

    process prepare_cleaned_metadata_file {
      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev

        input:
        tuple datasetID, mfile, usermetachecksum, rawsumstatchecksum, scleanchecksum, scleanGRCh37checksum, removedlineschecksum, cleanedheader from ch_mfile_cleaned_x

        output:
        tuple datasetID, path("prepare_cleaned_metadata_file__prepared_cleaned_metafile") into ch_mfile_cleaned_1

        script:
        """

        #Add cleaned output lines
        dateOfCreation="\$(date +%F-%H%M)"
        echo "cleansumstats_date: \${dateOfCreation}" > mfile_additions
        echo "cleansumstats_user: \$(id -u -n)" >> mfile_additions
        echo "cleansumstats_cleaned_GRCh38: sumstat_cleaned_GRCh38.gz" >> mfile_additions
        echo "cleansumstats_cleaned_GRCh38_checksum: ${scleanchecksum}" >> mfile_additions
        echo "cleansumstats_cleaned_GRCh37_coordinates: sumstat_cleaned_GRCh37.gz" >> mfile_additions
        echo "cleansumstats_cleaned_GRCh37_coordinates_checksum: ${scleanGRCh37checksum}" >> mfile_additions
        echo "cleansumstats_removed_lines: sumstat_removed_lines.gz" >> mfile_additions
        echo "cleansumstats_removed_lines_checksum: ${removedlineschecksum}" >> mfile_additions
        echo "cleansumstats_metafile_user_checksum: ${usermetachecksum}" >> mfile_additions
        echo "cleansumstats_sumstat_raw_checksum: ${rawsumstatchecksum}" >> mfile_additions

        #Calcualate effective N using meta data info
        try_infere_Neffective.sh ${mfile} >> mfile_additions

        # Apply additions to make the cleaned meta file ready
        create_output_meta_data_file_cleaned.sh mfile_additions ${cleanedheader} > prepare_cleaned_metadata_file__prepared_cleaned_metafile
        """
    }


    // Collect all metafiles in one channel
     ch_mfile_user_2
     .combine(ch_mfile_cleaned_1, by: 0)
     .set { ch_all_mfiles }

     ch_to_write_to_filelibrary3b
      .join(ch_input_readme, by: 0)
      .join(ch_overview_workflow_steps, by: 0)
      .join(ch_removed_lines_table, by: 0)
      .join(ch_gb_stats_combined, by: 0)
      .join(ch_all_mfiles, by: 0)
      .join(ch_to_write_to_raw_library, by: 0)
      .join(ch_input_pdf_stuff, by: 0)
      .set{ ch_to_write_to_filelibrary7 }


     //.combine(ch_collected_removed_lines2)

    process put_in_cleaned_library {

        publishDir "${params.outdir}/${datasetID}", mode: 'copy', overwrite: true

        input:
        tuple datasetID, sclean, scleanGRCh37, removedlines, readme, overviewworkflow, removedlinestable, gbdetectCHRPOS, gbdetectSNPCHRPOS, usermfile, cleanmfile, rawfile, pmid, pdfpath, pdfsuppdir from ch_to_write_to_filelibrary7

        output:
        path("*")

        script:

        """
        # Store data in library by copying (move is faster, but debug gets slower as input disappears)
        cp ${sclean} cleaned_GRCh38.gz
        cp ${scleanGRCh37} cleaned_GRCh37.gz

        # Make a folder with detailed data of the cleaning
        mkdir details
        cp $overviewworkflow details/stepwise_overview.txt
        cp ${removedlinestable} details/removed_lines_per_type_table.txt
        cp $gbdetectCHRPOS details/genome_build_map_count_table_chrpos.txt
        cp $gbdetectSNPCHRPOS details/genome_build_map_count_table_markername.txt
        cp ${removedlines} details/removed_lines.gz
        cp ${cleanmfile} cleaned_metadata.yaml

        # copy all raw stuff into raw
        mkdir raw
        cp ${rawfile} raw/.
        cp ${usermfile} raw/.

        #reference material
        mkdir raw/reference
        mkdir raw/reference/supplemental_material

        if [ "${pdfpath}" != "missing" ]
        then
          cp ${pdfpath} raw/reference/.
        fi

        cat ${pdfsuppdir} | while read -r supp; do
           if [ "\${supp}" != "missing" ]
           then
             cp \$supp raw/reference/supplemental_material/.
           else
             :
           fi
        done
        """
    }
  }
}

/*
 * Completion e-mail notification
 */
workflow.onComplete {

    // Set up the e-mail variables
    def subject = "[nf-core/cleansumstats] Successful: $workflow.runName"
    if (!workflow.success) {
      subject = "[nf-core/cleansumstats] FAILED: $workflow.runName"
    }
    def email_fields = [:]
    email_fields['version'] = workflow.manifest.version
    email_fields['runName'] = custom_runName ?: workflow.runName
    email_fields['success'] = workflow.success
    email_fields['dateComplete'] = workflow.complete
    email_fields['duration'] = workflow.duration
    email_fields['exitStatus'] = workflow.exitStatus
    email_fields['errorMessage'] = (workflow.errorMessage ?: 'None')
    email_fields['errorReport'] = (workflow.errorReport ?: 'None')
    email_fields['commandLine'] = workflow.commandLine
    email_fields['projectDir'] = workflow.projectDir
    email_fields['summary'] = summary
    email_fields['summary']['Date Started'] = workflow.start
    email_fields['summary']['Date Completed'] = workflow.complete
    email_fields['summary']['Pipeline script file path'] = workflow.scriptFile
    email_fields['summary']['Pipeline script hash ID'] = workflow.scriptId
    if (workflow.repository) email_fields['summary']['Pipeline repository Git URL'] = workflow.repository
    if (workflow.commitId) email_fields['summary']['Pipeline repository Git Commit'] = workflow.commitId
    if (workflow.revision) email_fields['summary']['Pipeline Git branch/tag'] = workflow.revision
    if (workflow.container) email_fields['summary']['Docker image'] = workflow.container
    email_fields['summary']['Nextflow Version'] = workflow.nextflow.version
    email_fields['summary']['Nextflow Build'] = workflow.nextflow.build
    email_fields['summary']['Nextflow Compile Timestamp'] = workflow.nextflow.timestamp

    // Check if we are only sending emails on failure
    email_address = params.email
    if (!params.email && params.email_on_fail && !workflow.success) {
        email_address = params.email_on_fail
    }

    // Render the TXT template
    def engine = new groovy.text.GStringTemplateEngine()
    def tf = new File("$baseDir/assets/email_template.txt")
    def txt_template = engine.createTemplate(tf).make(email_fields)
    def email_txt = txt_template.toString()

    // Render the HTML template
    def hf = new File("$baseDir/assets/email_template.html")
    def html_template = engine.createTemplate(hf).make(email_fields)
    def email_html = html_template.toString()

    // Render the sendmail template
    def smail_fields = [ email: email_address, subject: subject, email_txt: email_txt, email_html: email_html, baseDir: "$baseDir", mqcFile: mqc_report, mqcMaxSize: params.maxMultiqcEmailFileSize.toBytes() ]
    def sf = new File("$baseDir/assets/sendmail_template.txt")
    def sendmail_template = engine.createTemplate(sf).make(smail_fields)
    def sendmail_html = sendmail_template.toString()

    // Send the HTML e-mail
    if (email_address) {
        try {
          if ( params.plaintext_email ){ throw GroovyException('Send plaintext e-mail, not HTML') }
          // Try to send HTML e-mail using sendmail
          [ 'sendmail', '-t' ].execute() << sendmail_html
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (sendmail)"
        } catch (all) {
          // Catch failures and try with plaintext
          [ 'mail', '-s', subject, email_address ].execute() << email_txt
          log.info "[nf-core/cleansumstats] Sent summary e-mail to $email_address (mail)"
        }
    }

    // Write summary e-mail HTML to a file
    def output_d = new File( "${params.outdir}/pipeline_info/" )
    if (!output_d.exists()) {
      output_d.mkdirs()
    }
    def output_hf = new File( output_d, "pipeline_report.html" )
    output_hf.withWriter { w -> w << email_html }
    def output_tf = new File( output_d, "pipeline_report.txt" )
    output_tf.withWriter { w -> w << email_txt }

    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_red = params.monochrome_logs ? '' : "\033[0;31m";

    if (workflow.stats.ignoredCount > 0 && workflow.success) {
      log.info "${c_purple}Warning, pipeline completed, but with errored process(es) ${c_reset}"
      log.info "${c_red}Number of ignored errored process(es) : ${workflow.stats.ignoredCount} ${c_reset}"
      log.info "${c_green}Number of successfully ran process(es) : ${workflow.stats.succeedCount} ${c_reset}"
    }

    if (workflow.success) {
        log.info "${c_purple}[nf-core/cleansumstats]${c_green} Pipeline completed successfully${c_reset}"
    } else {
        checkHostname()
        log.info "${c_purple}[nf-core/cleansumstats]${c_red} Pipeline completed with errors${c_reset}"
    }

}


def nfcoreHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
    ${c_blue}        ___     __   __   __   ___     ${c_green}/,-._.--~\'${c_reset}
    ${c_blue}  |\\ | |__  __ /  ` /  \\ |__) |__         ${c_yellow}}  {${c_reset}
    ${c_blue}  | \\| |       \\__, \\__/ |  \\ |___     ${c_green}\\`-._,-`-,${c_reset}
                                            ${c_green}`._,._,\'${c_reset}
    ${c_purple}  nf-core/cleansumstats v${workflow.manifest.version}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
