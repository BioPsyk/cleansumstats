#!/usr/bin/env nextflow
// -*- mode: groovy; -*-
/*
vim: syntax=groovy

========================================================================================
                         cleansumstats
========================================================================================
cleansumstats Pipeline.
 #### Homepage / Documentation
 https://github.com/BioPsyk/cleansumstats
----------------------------------------------------------------------------------------
*/

nextflow.enable.dsl=2      

pipelineVersion = new File("$projectDir/VERSION").text.trim()

def helpMessage() {
    log.info nfcoreHeader()
    log.info"""

    Usage:

    The typical command for running the pipeline is as follows:

    nextflow run cleansumstats --input 'gwas_sumstats_meta_file.txt' -profile singularity

    Mandatory arguments:
      --input                       Path to metadata file in YAML format

    References:                     If not set here, it has to be specified in the configuration file
      --dbsnp_38                    Path to dbsnp GRCh38 reference
      --dbsnp_38_37                 Path to dbsnp GRCh38 to GRCh37 map reference
      --dbsnp_37_38                 Path to dbsnp GRCh37 to GRCh38 map reference
      --dbsnp_36_38                 Path to dbsnp GRCh36 to GRCh38 map reference
      --dbsnp_35_38                 Path to dbsnp GRCh35 to GRCh38 map reference
      --dbsnp_RSID_38               Path to dbsnp RSID to GRCh38 map reference

    Options:
      --placeholderOption           Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

    Filtering:
      --beforeLiftoverFilter        A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_keys
                                    Example(default): --beforeLiftoverFilter duplicated_keys

      --afterLiftoverFilter         A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_refalt_in_GRCh37
                                      duplicated_chrpos_refalt_in_GRCh38
                                      duplicated_chrpos_in_GRCh37
                                      duplicated_chrpos_in_GRCh38
                                      multiallelics_in_dbsnp
                                    Example(default): --afterLiftoverFilter duplicated_chrpos_refalt_in_GRCh37,duplicated_chrpos_refalt_in_GRCh38,duplicated_chrpos_in_GRCh37,duplicated_chrpos_in_GRCh38,multiallelics_in_dbsnp


      --afterAlleleCorrectionFilter A comma separated list ordered by filtering exclusion order including any of the following:
                                      duplicated_chrpos_in_GRCh37
                                    Example(default): --afterAlleleCorrectionFilter duplicated_chrpos_in_GRCh37

    Auxiliaries:
      --generateMetafile            Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline

      --generateDbSNPreference      Generates a meta file template, which is one of the required inputs to the cleansumstats pipeline
      --hg38ToHg19chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg18chain             chain file used for liftover (required for --generateDbSNPreference)
      --hg19ToHg17chain             chain file used for liftover (required for --generateDbSNPreference)

    Other options:
      --outdir                      The output directory where the results will be saved
      --email                       Set this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits
      --email_on_fail               Same as --email, except only send mail if the workflow is not successful
      --maxMultiqcEmailFileSize     Theshold size for MultiQC report to be attached in notification email. If file generated by pipeline exceeds the threshold, it will not be attached (Default: 25MB)
      -name                         Name for the pipeline run. If not specified, Nextflow will automatically generate a random mnemonic.

    Debug:
      --keepIntermediateFiles       Keeps intermediate files, useful for debugging

    """.stripIndent()

}

// Show help message
if (params.help) {
    helpMessage()
    exit 0
}

/*
 * SET UP CONFIGURATION VARIABLES
 */

// checker only
if(params.checkonly){
  doCompleteCleaningWorkflow = false
}else{
  doCompleteCleaningWorkflow = true
}

// check filter
beforeLiftoverFilter = params.beforeLiftoverFilter
afterLiftoverFilter = params.afterLiftoverFilter
afterAlleleCorrectionFilter = params.afterAlleleCorrectionFilter

// Set channels
if (params.generateDbSNPreference) {
  if (params.hg38ToHg19chain) { ch_hg38ToHg19chain = file(params.hg38ToHg19chain, checkIfExists: true) }
  if (params.hg19ToHg18chain) { ch_hg19ToHg18chain = file(params.hg19ToHg18chain, checkIfExists: true) }
  if (params.hg19ToHg17chain) { ch_hg19ToHg17chain = file(params.hg19ToHg17chain, checkIfExists: true) }

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38) }
}else if(params.generate1KgAfSNPreference){

  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }

}else {

  if (params.kg1000AFGRCh38) { ch_kg1000AFGRCh38 = file(params.kg1000AFGRCh38, checkIfExists: true) }
  if (params.dbsnp_38) { ch_dbsnp_38 = file(params.dbsnp_38, checkIfExists: true) }
  if (params.dbsnp_38_37) { ch_dbsnp_38_37 = file(params.dbsnp_38_37, checkIfExists: true) }
  if (params.dbsnp_37_38) { ch_dbsnp_37_38 = file(params.dbsnp_37_38, checkIfExists: true) }
  if (params.dbsnp_36_38) { ch_dbsnp_36_38 = file(params.dbsnp_36_38, checkIfExists: true) }
  if (params.dbsnp_35_38) { ch_dbsnp_35_38 = file(params.dbsnp_35_38, checkIfExists: true) }
  if (params.dbsnp_RSID_38) { ch_dbsnp_RSID_38 = file(params.dbsnp_RSID_38, checkIfExists: true) }
}

params.ch_regexp_lexicon = file("$baseDir/assets/map_regexp_and_adhocfunction.txt", checkIfExists: true)

// Has the run name been specified by the user?
//  this has the bonus effect of catching both -name and --name
custom_runName = params.name
if (!(workflow.runName ==~ /[a-z]+_[a-z]+/)) {
  custom_runName = workflow.runName
}



// Header log info
log.info cleansumstatsHeader()
def summary = [:]
if (workflow.revision) summary['Pipeline Release'] = workflow.revision
summary['Run Name']         = custom_runName ?: workflow.runName
summary['Input']            = params.input
//if (params.dbsnp38) summary['dbSNP38'] = params.dbsnp38
//if (params.dbsnp37) summary['dbSNP37'] = params.dbsnp37
//if (params.dbsnp36) summary['dbSNP36'] = params.dbsnp36
//if (params.dbsnp35) summary['dbSNP35'] = params.dbsnp35
//if (params.dbsnpRSID) summary['dbsnpRSID'] = params.dbsnpRSID

summary['Max Resources']    = "$params.max_memory memory, $params.max_cpus cpus, $params.max_time time per job"
if (workflow.containerEngine) summary['Container'] = "$workflow.containerEngine - $workflow.container"
summary['Output dir']       = params.outdir
summary['Launch dir']       = workflow.launchDir
summary['Working dir']      = workflow.workDir
summary['Script dir']       = workflow.projectDir
summary['User']             = workflow.userName
if (workflow.profile == 'awsbatch') {
  summary['AWS Region']     = params.awsregion
  summary['AWS Queue']      = params.awsqueue
}
summary['Config Profile'] = workflow.profile
if (params.config_profile_description) summary['Config Description'] = params.config_profile_description
if (params.config_profile_contact)     summary['Config Contact']     = params.config_profile_contact
if (params.config_profile_url)         summary['Config URL']         = params.config_profile_url
log.info summary.collect { k,v -> "${k.padRight(18)}: $v" }.join("\n")
log.info "-\033[2m--------------------------------------------------\033[0m-"

// Check the hostnames against configured profiles
//checkHostname()

def create_workflow_summary(summary) {
    def yaml_file = workDir.resolve('workflow_summary_mqc.yaml')
    yaml_file.text  = """
    id: 'cleansumstats-summary'
    description: " - this information is collected when the pipeline is started."
    section_name: 'nf-core/cleansumstats Workflow Summary'
    section_href: 'https://github.com/nf-core/cleansumstats'
    plot_type: 'html'
    data: |
        <dl class=\"dl-horizontal\">
${summary.collect { k,v -> "            <dt>$k</dt><dd><samp>${v ?: '<span style=\"color:#999999;\">N/A</a>'}</samp></dd>" }.join("\n")}
        </dl>
    """.stripIndent()

   return yaml_file
}


process get_software_versions {
    publishDir "${params.outdir}/pipeline_info", mode: 'copy', overwrite: true, pattern: '*.csv'

    output:
    file "software_versions" into ch_software_versions


    script:
    """
    echo $pipelineVersion > v_pipeline.txt
    echo $workflow.nextflow.version > v_nextflow.txt
    sstools-version > v_sumstattools.txt
    echo "placeholder" > software_versions
    """
}

import dk.biopsyk.PipelineSession

def sess = new PipelineSession<Metadata>(
  Metadata.class,
  baseDir,
  workflow.workDir,
  params.input
)

params.sess=sess


include { prepare_dbsnp_reference } from './modules/subworkflow/prepare_dbsnp.nf' 
include { prepare_1kgaf_reference } from './modules/subworkflow/prepare_1kgaf.nf' 

include { main_init_checks_crucial_paths } from './modules/subworkflow/main_init_checks_crucial_paths.nf' 
include {
  calculate_checksum_on_metafile_input
  make_metafile_unix_friendly
  calculate_checksum_on_sumstat_input
  check_sumstat_format
  add_sorted_rowindex_to_sumstat
} from './modules/process/main_init_checks.nf' 

include { map_to_dbsnp } from './modules/subworkflow/map_to_dbsnp.nf'
include { allele_correction } from './modules/subworkflow/allele_correction.nf'
include { update_stats } from './modules/subworkflow/update_stats.nf'
include { organize_output } from './modules/subworkflow/organize_output.nf'



workflow {
  main:

  if (params.generateMetafile){
    sess.metadata_paths.each {
      log.info "Writing metadata template"
  
      def metadata_id = it.getBaseName().toString()
  
      def template_file = new File("${params.outdir}/${metadata_id}.template.yaml")
      template_file.write(
        sess.metadata_schema.generate_metadata_template()
      )
  
      log.info "Metadata template written to ${params.outdir}/${metadata_id}.template.yaml"
    }
  }else if(params.generateMetaClass){
    log.info "Metadata class written to ${params.outdir}/Metadata.groovy"
  
    def class_file = new File("${params.outdir}/Metadata.groovy")
    class_file.write(
      sess.metadata_schema.generate_metadata_groovy_class()
    )
  
    log.info "Metadata class written to ${params.outdir}/Metadata.groovy"
  }else if(params.generateDbSNPreference){
    prepare_dbsnp_reference(
      "${params.input}", 
      ch_hg38ToHg19chain, 
      ch_hg19ToHg18chain, 
      ch_hg19ToHg17chain,
      ch_dbsnp_RSID_38,
      ch_dbsnp_38,
      ch_dbsnp_38_37,
      ch_dbsnp_37_38,
      ch_dbsnp_36_38,
      ch_dbsnp_35_38

    )
  }else if(params.generate1KgAfSNPreference){
    prepare_1kgaf_reference("${params.input}", ch_dbsnp_38)
    //#check for not agreeing ref alleles and alt alleles
    // awk '{if($2!=$10){print $0}}' 1kg_af_ref.sorted.joined | head
    // awk '{if($3!=$11){print $0}}' 1kg_af_ref.sorted.joined | head
  }else {
  
    //=================================================================================
    // Pre-execution validation
    //=================================================================================
  
    log.info("Reading metadata files")
  
    sess.read_metadata_files()
  
    log.info("All metadata files read")
    log.info("Validating pipeline parameters")
  
    ParametersValidator.validate_filters_allowed(
      "before",
      params.beforeLiftoverFilter,
      "${baseDir}/assets/allowed_names_beforeLiftoverFilter.txt"
    )
  
    ParametersValidator.validate_filters_allowed(
      "after",
      params.afterLiftoverFilter,
      "${baseDir}/assets/allowed_names_afterLiftoverFilter.txt"
    )
  
    log.info("All pipeline parameters validated")
  }

  //=================================================================================
  // Start of execution
  //=================================================================================

  Channel
    .fromPath("${params.input}", type: 'file')
    .map { file -> tuple(file.baseName, file) }
    .set { ch_mfile_checkX } 

  main_init_checks_crucial_paths(ch_mfile_checkX, sess)
  calculate_checksum_on_metafile_input(ch_mfile_checkX)
  calculate_checksum_on_sumstat_input(main_init_checks_crucial_paths.out.spath)
  make_metafile_unix_friendly(ch_mfile_checkX)
  check_sumstat_format(main_init_checks_crucial_paths.out.mfile_check_format)
  add_sorted_rowindex_to_sumstat(check_sumstat_format.out.sfile)

  if (doCompleteCleaningWorkflow){

    map_to_dbsnp(add_sorted_rowindex_to_sumstat.out.main)
    ch_allele_correction_combine=map_to_dbsnp.out.dbsnp_mapped.join(add_sorted_rowindex_to_sumstat.out.main, by: 0)
    allele_correction(ch_allele_correction_combine)
    update_stats(add_sorted_rowindex_to_sumstat.out.main, allele_correction.out.allele_corrected)
    organize_output(allele_correction.out.allele_corrected, update_stats.out.cleaned_stats)

  }
}

//
//    process collect_rmd_lines {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, step1, step2, step3 from ch_collected_removed_lines
//
//        output:
//        tuple datasetID, file("collect_rmd_lines__removed_lines_collected.txt") into ch_collected_removed_lines2
//
//        script:
//        """
//        echo -e "RowIndex\tExclusionReason" > collect_rmd_lines__removed_lines_collected.txt
//        cat ${step1} ${step2} ${step3} >> collect_rmd_lines__removed_lines_collected.txt
//        """
//    }
//
//    ch_collected_removed_lines2
//      .into { ch_collected_removed_lines3; ch_collected_removed_lines4 }
//
//    process desc_rmd_lines_as_table {
//
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, filtered_stats_removed from ch_collected_removed_lines3
//
//        output:
//        tuple datasetID, file("desc_rmd_lines_as_table__desc_removed_lines_table.txt") into ch_removed_lines_table
//
//        script:
//        """
//        # prepare process specific descriptive statistics
//        echo -e "NrExcludedRows\tExclusionReason" > desc_rmd_lines_as_table__desc_removed_lines_table.txt
//        cat $filtered_stats_removed | tail -n+2 | awk -vOFS="\t" '{ seen[\$2] += 1 } END { for (i in seen) print seen[i],i }' >> desc_rmd_lines_as_table__desc_removed_lines_table.txt
//
//        """
//    }
//
//
//    ch_cleaned_file
//      .join(ch_input_sfile2, by: 0)
//      .join(ch_sfile_on_stream5, by: 0)
//      .join(ch_collected_removed_lines4, by: 0)
//      .set{ ch_to_write_to_filelibrary2 }
//
//    process gzip_outfiles {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, inputsfile, inputformatted, removedlines from ch_to_write_to_filelibrary2
//
//        output:
//        tuple datasetID, path("gzip_outfiles__sclean.gz"), path("gzip_outfiles__scleanGRCh37.gz"), path("gzip_outfiles__removed_lines.gz") into ch_to_write_to_filelibrary3
//        tuple datasetID, path("gzip_outfiles__cleanedheader") into ch_cleaned_header
//        tuple datasetID, inputsfile into ch_to_write_to_raw_library
//        val datasetID into ch_check_avail
//
//        script:
//        """
//        # Make a header file to use when deciding on what cols are present for the new meta file
//        head -n1 ${sclean} > gzip_outfiles__cleanedheader
//
//        # Store data in library
//        gzip -c ${sclean} > gzip_outfiles__sclean.gz
//        gzip -c ${scleanGRCh37} > gzip_outfiles__scleanGRCh37.gz
//        gzip -c ${removedlines} > gzip_outfiles__removed_lines.gz
//        """
//    }
//
//    ch_to_write_to_filelibrary3.into { ch_to_write_to_filelibrary3a; ch_to_write_to_filelibrary3b }
//
//    process calculate_checksum_on_sumstat_cleaned {
//        publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, removedlines from ch_to_write_to_filelibrary3a
//
//        output:
//        tuple datasetID, env(scleanchecksum), env(scleanGRCh37checksum), env(removedlineschecksum) into ch_cleaned_sumstat_checksums
//
//        script:
//        """
//        scleanchecksum="\$(b3sum ${sclean} | awk '{print \$1}')"
//        scleanGRCh37checksum="\$(b3sum ${scleanGRCh37} | awk '{print \$1}')"
//        removedlineschecksum="\$(b3sum ${removedlines} | awk '{print \$1}')"
//        """
//    }
//
//  ch_cleaned_sumstat_checksums.into { ch_cleaned_sumstat_checksums1; ch_cleaned_sumstat_checksums2 }
//
//
//  //Do actual collection, placed in corresponding step order
//  ch_desc_prep_force_tab_sep_BA
//   .join(ch_desc_prep_add_sorted_rowindex_BA, by: 0)
//   .join(ch_desc_combined_set_after_liftover, by: 0)
//   .join(ch_desc_removed_duplicates_after_liftover, by: 0)
//   .join(ch_desc_keep_a_GRCh38_reference_BA, by: 0)
//   .join(ch_desc_split_multi_allelics_and_sort_on_rowindex_BA, by: 0)
//   .join(ch_desc_filtered_allele_pairs_with_dbsnp_as_reference_BA, by: 0)
//   .join(ch_desc_removed_duplicated_chr_pos_rows_BA, by: 0)
//   .join(ch_desc_filtered_stat_rows_with_non_numbers_BA, by: 0)
//   .join(ch_desc_inferred_stats_if_inferred_BA, by: 0)
//   .join(ch_desc_from_inferred_to_joined_selection_BA, by: 0)
//   .join(ch_desc_from_sumstats_to_joined_selection_BA, by: 0)
//   .join(ch_desc_final_merge_BA, by: 0)
//   .set{ ch_collected_workflow_stepwise_stats }
//
// //Some that now are part of the branched workflow. Unclear how to save the the stepwise branched workflow before after steps, but all info should be exported in channels.
// //  .combine(ch_desc_sex_chrom_formatting_BA, by: 0)
// //  .combine(ch_desc_prep_for_dbsnp_mapping_BA, by: 0)
// //  .combine(ch_removed_rows_before_liftover, by: 0)
// //  .combine(ch_desc_liftover_to_GRCh38_and_map_to_dbsnp_BA, by: 0)
//
//  process collect_and_prep_stepwise_readme {
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//      input:
//      tuple datasetID, step1, step2, step3, step4, step5, step6, step7, step8, step9, step10, step11, step12, step13 from ch_collected_workflow_stepwise_stats
//
//      output:
//      tuple datasetID, file("collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt") into ch_overview_workflow_steps
//
//      script:
//      """
//      cat $step1 $step2 $step3 $step4 $step5 $step6 $step7 $step8 $step9 $step10 $step11 $step12 $step13 > all_removed_steps
//
//      echo -e "Steps\tBefore\tAfter\tDescription" > collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt
//      awk -vFS="\t" -vOFS="\t" '{print "Step"NR, \$1, \$2, \$3}' all_removed_steps >> collect_and_prep_stepwise_readme__desc_collected_workflow_stepwise_stats.txt
//
//      """
//  }
//
//    ch_mfile_ok4
//      .join(ch_usermeta_checksum, by: 0)
//      .join(ch_rawsumstat_checksum, by: 0)
//      .join(ch_cleaned_sumstat_checksums2, by: 0)
//      .join(ch_cleaned_header, by: 0)
//      .set { ch_mfile_cleaned_x }
//
//    process prepare_cleaned_metadata_file {
//      publishDir "${params.outdir}/${datasetID}/intermediates", mode: 'rellink', overwrite: true, enabled: params.dev
//
//        input:
//        tuple datasetID, mfile, usermetachecksum, rawsumstatchecksum, scleanchecksum, scleanGRCh37checksum, removedlineschecksum, cleanedheader from ch_mfile_cleaned_x
//
//        output:
//        tuple datasetID, path("prepare_cleaned_metadata_file__prepared_cleaned_metafile") into ch_mfile_cleaned_1
//
//        script:
//        """
//
//        #Add cleaned output lines
//        dateOfCreation="\$(date +%F-%H%M)"
//        echo "cleansumstats_date: \${dateOfCreation}" > mfile_additions
//        echo "cleansumstats_user: \$(id -u -n)" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh38: sumstat_cleaned_GRCh38.gz" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh38_checksum: ${scleanchecksum}" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh37_coordinates: sumstat_cleaned_GRCh37.gz" >> mfile_additions
//        echo "cleansumstats_cleaned_GRCh37_coordinates_checksum: ${scleanGRCh37checksum}" >> mfile_additions
//        echo "cleansumstats_removed_lines: sumstat_removed_lines.gz" >> mfile_additions
//        echo "cleansumstats_removed_lines_checksum: ${removedlineschecksum}" >> mfile_additions
//        echo "cleansumstats_metafile_user_checksum: ${usermetachecksum}" >> mfile_additions
//        echo "cleansumstats_sumstat_raw_checksum: ${rawsumstatchecksum}" >> mfile_additions
//
//        #Calcualate effective N using meta data info
//        try_infere_Neffective.sh ${mfile} >> mfile_additions
//
//        # Apply additions to make the cleaned meta file ready
//        create_output_meta_data_file_cleaned.sh mfile_additions ${cleanedheader} > prepare_cleaned_metadata_file__prepared_cleaned_metafile
//        """
//    }
//
//
//    // Collect all metafiles in one channel
//     ch_mfile_user_2
//     .join(ch_mfile_cleaned_1, by: 0)
//     .set { ch_all_mfiles }
//
//     ch_to_write_to_filelibrary3b
//      .join(ch_input_readme, by: 0)
//      .join(ch_overview_workflow_steps, by: 0)
//      .join(ch_removed_lines_table, by: 0)
//      .join(ch_gb_stats_combined, by: 0)
//      .join(ch_all_mfiles, by: 0)
//      .join(ch_to_write_to_raw_library, by: 0)
//      .join(ch_input_pdf_stuff, by: 0)
//      .join(ch_stats_for_output_selected_source, by: 0)
//      .set{ ch_to_write_to_filelibrary7 }
//
//     //.combine(ch_collected_removed_lines2)
//
//    process put_in_cleaned_library {
//
//        publishDir "${params.outdir}/${datasetID}", mode: 'copy', overwrite: true
//
//        input:
//        tuple datasetID, sclean, scleanGRCh37, removedlines, readme, overviewworkflow, removedlinestable, gbdetectCHRPOS, gbdetectSNPCHRPOS, usermfile, cleanmfile, rawfile, pmid, pdfpath, pdfsuppdir, selected_source from ch_to_write_to_filelibrary7
//
//        output:
//        path("*")
//
//        script:
//
//        """
//        # Store data in library by copying (move is faster, but debug gets slower as input disappears)
//        cp ${sclean} cleaned_GRCh38.gz
//        cp ${scleanGRCh37} cleaned_GRCh37.gz
//        cp ${cleanmfile} cleaned_metadata.yaml
//
//        # Make a folder with detailed data of the cleaning
//        mkdir details
//        cp $overviewworkflow details/stepwise_overview.txt
//        cp ${removedlinestable} details/removed_lines_per_type_table.txt
//        cp $gbdetectCHRPOS details/genome_build_map_count_table_chrpos.txt
//        cp $gbdetectSNPCHRPOS details/genome_build_map_count_table_markername.txt
//        cp ${removedlines} details/removed_lines.gz
//        cp ${selected_source} details/selected_source_stats.txt
//
//
//        # copy all raw stuff into raw
//        mkdir raw
//        cp ${rawfile} raw/.
//        cp ${usermfile} raw/.
//
//        #reference material
//        mkdir raw/reference
//        mkdir raw/reference/supplemental_material
//
//        if [ "${pdfpath}" != "missing" ]
//        then
//          cp ${pdfpath} raw/reference/.
//        fi
//
//        cat ${pdfsuppdir} | while read -r supp; do
//           if [ "\${supp}" != "missing" ]
//           then
//             cp \$supp raw/reference/supplemental_material/.
//           else
//             :
//           fi
//        done
//        """
//    }
//  }
//}

def cleansumstatsHeader(){
    // Log colors ANSI codes
    c_reset = params.monochrome_logs ? '' : "\033[0m";
    c_dim = params.monochrome_logs ? '' : "\033[2m";
    c_black = params.monochrome_logs ? '' : "\033[0;30m";
    c_green = params.monochrome_logs ? '' : "\033[0;32m";
    c_yellow = params.monochrome_logs ? '' : "\033[0;33m";
    c_blue = params.monochrome_logs ? '' : "\033[0;34m";
    c_purple = params.monochrome_logs ? '' : "\033[0;35m";
    c_cyan = params.monochrome_logs ? '' : "\033[0;36m";
    c_white = params.monochrome_logs ? '' : "\033[0;37m";

    return """    -${c_dim}--------------------------------------------------${c_reset}-
                                            ${c_green},--.${c_black}/${c_green},-.${c_reset}
                                            ${c_cyan}`._,._,\'${c_reset}
    ${c_purple} cleansumstats v${pipelineVersion}${c_reset}
    -${c_dim}--------------------------------------------------${c_reset}-
    """.stripIndent()
}

def checkHostname(){
    def c_reset = params.monochrome_logs ? '' : "\033[0m"
    def c_white = params.monochrome_logs ? '' : "\033[0;37m"
    def c_red = params.monochrome_logs ? '' : "\033[1;91m"
    def c_yellow_bold = params.monochrome_logs ? '' : "\033[1;93m"
    if (params.hostnames) {
        def hostname = "hostname".execute().text.trim()
        params.hostnames.each { prof, hnames ->
            hnames.each { hname ->
                if (hostname.contains(hname) && !workflow.profile.contains(prof)) {
                    log.error "====================================================\n" +
                            "  ${c_red}WARNING!${c_reset} You are running with `-profile $workflow.profile`\n" +
                            "  but your machine hostname is ${c_white}'$hostname'${c_reset}\n" +
                            "  ${c_yellow_bold}It's highly recommended that you use `-profile $prof${c_reset}`\n" +
                            "============================================================"
                }
            }
        }
    }
}
